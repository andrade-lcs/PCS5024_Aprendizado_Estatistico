{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python382jvsc74a57bd019dd5068fd4c406359a86f27249fc517a25f1ed5fa0eb2341423f3354c0b7639",
      "display_name": "Python 3.8.2 32-bit"
    },
    "colab": {
      "name": "UCI_adult_predict.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "thsFFBMxfMHz"
      },
      "source": [
        "#Bibliotécas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiAfKFXzfMH_"
      },
      "source": [
        "#DataFrame\n",
        "headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "\n",
        "df = pd.read_csv('adult.data.csv', names=headers)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUsKO6ZRz8_e"
      },
      "source": [
        "#Tratamento de dados faltantes para o mais representatívo\n",
        "columns = df.columns\n",
        "for i in columns:\n",
        "    missing = df[i].isin([' ?']).sum()\n",
        "    df[i] = df[i].replace(' ?', np.NaN)\n",
        "df = df.apply(lambda x:x.fillna(x.value_counts().index[0]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzLUMIkrfMIA"
      },
      "source": [
        "#Divisão em Parâmetro e Classe\n",
        "X_df = df.iloc[:, 0:14].values\n",
        "y_df = df.iloc[:, 14].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8__DG0IfMIB"
      },
      "source": [
        "#LabelEncoder\n",
        "def labelencoder(pd_serie):\n",
        "    labelencoder = LabelEncoder()\n",
        "    pd_serie = labelencoder.fit_transform(pd_serie)\n",
        "    return pd_serie\n",
        "\n",
        "X_df[:, 1] = labelencoder(X_df[:, 1])\n",
        "X_df[:, 3] = labelencoder(X_df[:, 3])\n",
        "X_df[:, 5] = labelencoder(X_df[:, 5])\n",
        "X_df[:, 6] = labelencoder(X_df[:, 6])\n",
        "X_df[:, 7] = labelencoder(X_df[:, 7])\n",
        "X_df[:, 8] = labelencoder(X_df[:, 8])\n",
        "X_df[:, 9] = labelencoder(X_df[:, 9])\n",
        "X_df[:, 13] = labelencoder(X_df[:, 13])\n",
        "y_df = labelencoder(y_df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "type(X_df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYQn9S19fMIC"
      },
      "source": [
        "#One Hot Encoder\n",
        "onehotencorder = ColumnTransformer(transformers=[(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder='passthrough')\n",
        "X_df = onehotencorder.fit_transform(X_df).toarray()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twXAB1iKfMIC"
      },
      "source": [
        "#Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "X_df = scaler.fit_transform(X_df)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "312HacmbfMID"
      },
      "source": [
        "#Divisão df de treinamento e teste 15%\n",
        "X_df_train, X_df_test, y_df_train, y_df_test =  train_test_split(X_df, y_df, test_size=0.15, random_state=0)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8E9ZX2KfMID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d6900b-03eb-4fcc-d365-61e2bc72fe4f"
      },
      "source": [
        "print(X_df_train.shape, X_df_test.shape, y_df_train.shape, y_df_test.shape)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27676, 108) (4885, 108) (27676,) (4885,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cp03L4BfMIF"
      },
      "source": [
        "#kNN\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSiD4blLfMIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9037a88-9952-4862-feae-6ed8d50013a8"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_df_train, y_df_train)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sul2sYy8fMIG"
      },
      "source": [
        "predict_knn = knn.predict(X_df_test)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vovqlq6egJt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb9b419-1385-4b18-c995-e0cedcfe1606"
      },
      "source": [
        "accuracy_score(y_df_test, predict_knn)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8290685772773797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiWW-LhtgqYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe73e79-15da-4b69-a722-6221b8d3bd02"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_knn)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3439,  254],\n",
              "       [ 582,  610]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAWrWQMGg8S6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697beb78-2293-4e60-b955-21cb0b6081c9"
      },
      "source": [
        "print(classification_report(y_df_test, predict_knn))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89      3693\n",
            "           1       0.71      0.51      0.59      1192\n",
            "\n",
            "    accuracy                           0.83      4885\n",
            "   macro avg       0.78      0.72      0.74      4885\n",
            "weighted avg       0.82      0.83      0.82      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TPpC38rfMIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b21844-c42a-420a-fc7b-0ed59ce10327"
      },
      "source": [
        "#cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "score_knn = cross_val_score(knn, X_df, y_df, cv = kfold)\n",
        "score_knn.mean()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8318845951905516"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYvd9aSofMIH"
      },
      "source": [
        "#random forest\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFY_Lg6mHBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3838f029-e451-48eb-e292-63991c91506a"
      },
      "source": [
        "random_forest = RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_leaf=1, min_samples_split=5, random_state=0)\n",
        "random_forest.fit(X_df_train, y_df_train)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=5,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfSNGu-7mlJ9"
      },
      "source": [
        "predict_rf = random_forest.predict(X_df_test)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmHSc6sXtpFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1efc176-ed98-453a-9649-1383b3a9c17d"
      },
      "source": [
        "accuracy_score(y_df_test, predict_rf)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8562947799385875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOz-SI0DtpqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3704f32c-8d8c-4730-d744-7c15942d3f03"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_rf)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3430,  263],\n",
              "       [ 453,  739]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqmaUQ2ItsGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e36884-6a65-479e-d2fe-2448d95adbca"
      },
      "source": [
        "print(classification_report(y_df_test, predict_rf))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.91      3693\n",
            "           1       0.74      0.62      0.67      1192\n",
            "\n",
            "    accuracy                           0.85      4885\n",
            "   macro avg       0.81      0.77      0.79      4885\n",
            "weighted avg       0.85      0.85      0.85      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBlGlJqdG07E",
        "outputId": "7283492a-08c8-4c2f-8a5f-0656f2eee17e"
      },
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "score_random_forest = cross_val_score(random_forest, X_df, y_df, cv = kfold)\n",
        "score_random_forest.mean()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8585733694729702"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk7yxJ3FtyNB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "99067d40-8138-4f0d-c5b4-c85ab27aafb2"
      },
      "source": [
        "'''#MLP\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#MLP\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34mRORBC0up4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1ad3cd77-450f-49e4-da0f-674937345aab"
      },
      "source": [
        "'''mlp_keras = Sequential()\n",
        "mlp_keras.add(Dense(units=55, activation='relu', input_dim=108))\n",
        "mlp_keras.add(Dense(units=55, activation='relu'))\n",
        "mlp_keras.add(Dense(units=1, activation='sigmoid'))\n",
        "mlp_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])'''"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"mlp_keras = Sequential()\\nmlp_keras.add(Dense(units=55, activation='relu', input_dim=108))\\nmlp_keras.add(Dense(units=55, activation='relu'))\\nmlp_keras.add(Dense(units=1, activation='sigmoid'))\\nmlp_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "redL5hNs0wqQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d14d4df0-bc3e-449f-cb5e-222a57ad69a7"
      },
      "source": [
        "'''mlp_keras.fit(X_df_train, y_df_train, batch_size=10, epochs=100)'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mlp_keras.fit(X_df_train, y_df_train, batch_size=10, epochs=100)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSSLWtIg04HP"
      },
      "source": [
        "#predict_mlp_keras = mlp_keras.predict(X_df_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_80233Kd46qr"
      },
      "source": [
        "#predict_mlp_keras = (predict_mlp_keras > 0.5)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RsY_UwO4mVx"
      },
      "source": [
        "#accuracy_score(y_df_test, predict_mlp_keras)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQhFZaj24wXe"
      },
      "source": [
        "#confusion_matrix(y_df_test, predict_mlp_keras)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5lzhM7b5Hth"
      },
      "source": [
        "#print(classification_report(y_df_test, predict_mlp_keras ))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxd-waKZ5UFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef564ad-776a-45af-ecb5-bc923748d8ad"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(verbose=True, max_iter=1000, tol=0.000010, hidden_layer_sizes=(100, 100))\n",
        "mlp.fit(X_df_train, y_df_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.37329235\n",
            "Iteration 2, loss = 0.32174075\n",
            "Iteration 3, loss = 0.30973010\n",
            "Iteration 4, loss = 0.30444757\n",
            "Iteration 5, loss = 0.29889699\n",
            "Iteration 6, loss = 0.29460231\n",
            "Iteration 7, loss = 0.29120298\n",
            "Iteration 8, loss = 0.28857842\n",
            "Iteration 9, loss = 0.28566642\n",
            "Iteration 10, loss = 0.28177534\n",
            "Iteration 11, loss = 0.27965710\n",
            "Iteration 12, loss = 0.27857901\n",
            "Iteration 13, loss = 0.27740046\n",
            "Iteration 14, loss = 0.27328547\n",
            "Iteration 15, loss = 0.27158923\n",
            "Iteration 16, loss = 0.26791909\n",
            "Iteration 17, loss = 0.26601119\n",
            "Iteration 18, loss = 0.26488735\n",
            "Iteration 19, loss = 0.26182426\n",
            "Iteration 20, loss = 0.26057747\n",
            "Iteration 21, loss = 0.25725705\n",
            "Iteration 22, loss = 0.25455669\n",
            "Iteration 23, loss = 0.25324816\n",
            "Iteration 24, loss = 0.25320115\n",
            "Iteration 25, loss = 0.24832299\n",
            "Iteration 26, loss = 0.24832843\n",
            "Iteration 27, loss = 0.24599736\n",
            "Iteration 28, loss = 0.24404126\n",
            "Iteration 29, loss = 0.24037129\n",
            "Iteration 30, loss = 0.23889062\n",
            "Iteration 31, loss = 0.23707797\n",
            "Iteration 32, loss = 0.23484563\n",
            "Iteration 33, loss = 0.23201470\n",
            "Iteration 34, loss = 0.23093350\n",
            "Iteration 35, loss = 0.22815136\n",
            "Iteration 36, loss = 0.22687649\n",
            "Iteration 37, loss = 0.22547577\n",
            "Iteration 38, loss = 0.22319816\n",
            "Iteration 39, loss = 0.22112342\n",
            "Iteration 40, loss = 0.21912553\n",
            "Iteration 41, loss = 0.21737158\n",
            "Iteration 42, loss = 0.21611910\n",
            "Iteration 43, loss = 0.21512354\n",
            "Iteration 44, loss = 0.21231467\n",
            "Iteration 45, loss = 0.21157333\n",
            "Iteration 46, loss = 0.20871880\n",
            "Iteration 47, loss = 0.20868667\n",
            "Iteration 48, loss = 0.20672338\n",
            "Iteration 49, loss = 0.20576749\n",
            "Iteration 50, loss = 0.20392275\n",
            "Iteration 51, loss = 0.20421498\n",
            "Iteration 52, loss = 0.20170871\n",
            "Iteration 53, loss = 0.20077165\n",
            "Iteration 54, loss = 0.20087179\n",
            "Iteration 55, loss = 0.19796839\n",
            "Iteration 56, loss = 0.19459633\n",
            "Iteration 57, loss = 0.19415857\n",
            "Iteration 58, loss = 0.19475060\n",
            "Iteration 59, loss = 0.19295695\n",
            "Iteration 60, loss = 0.19165612\n",
            "Iteration 61, loss = 0.19037114\n",
            "Iteration 62, loss = 0.18878860\n",
            "Iteration 63, loss = 0.18779816\n",
            "Iteration 64, loss = 0.18882689\n",
            "Iteration 65, loss = 0.18590218\n",
            "Iteration 66, loss = 0.18464863\n",
            "Iteration 67, loss = 0.18407509\n",
            "Iteration 68, loss = 0.18371767\n",
            "Iteration 69, loss = 0.17978058\n",
            "Iteration 70, loss = 0.18091451\n",
            "Iteration 71, loss = 0.17913903\n",
            "Iteration 72, loss = 0.17764565\n",
            "Iteration 73, loss = 0.17740478\n",
            "Iteration 74, loss = 0.17704649\n",
            "Iteration 75, loss = 0.17788673\n",
            "Iteration 76, loss = 0.17460180\n",
            "Iteration 77, loss = 0.17485005\n",
            "Iteration 78, loss = 0.17254669\n",
            "Iteration 79, loss = 0.17337264\n",
            "Iteration 80, loss = 0.17192252\n",
            "Iteration 81, loss = 0.17135155\n",
            "Iteration 82, loss = 0.17119367\n",
            "Iteration 83, loss = 0.16941809\n",
            "Iteration 84, loss = 0.16731936\n",
            "Iteration 85, loss = 0.17000774\n",
            "Iteration 86, loss = 0.16829171\n",
            "Iteration 87, loss = 0.16640512\n",
            "Iteration 88, loss = 0.16397328\n",
            "Iteration 89, loss = 0.16495232\n",
            "Iteration 90, loss = 0.16579919\n",
            "Iteration 91, loss = 0.16454362\n",
            "Iteration 92, loss = 0.16233111\n",
            "Iteration 93, loss = 0.16320306\n",
            "Iteration 94, loss = 0.16123473\n",
            "Iteration 95, loss = 0.15869241\n",
            "Iteration 96, loss = 0.16021735\n",
            "Iteration 97, loss = 0.16014194\n",
            "Iteration 98, loss = 0.15822615\n",
            "Iteration 99, loss = 0.15660889\n",
            "Iteration 100, loss = 0.15752036\n",
            "Iteration 101, loss = 0.15899824\n",
            "Iteration 102, loss = 0.15468943\n",
            "Iteration 103, loss = 0.15468026\n",
            "Iteration 104, loss = 0.15424124\n",
            "Iteration 105, loss = 0.15412855\n",
            "Iteration 106, loss = 0.15299149\n",
            "Iteration 107, loss = 0.15505681\n",
            "Iteration 108, loss = 0.15142988\n",
            "Iteration 109, loss = 0.15030866\n",
            "Iteration 110, loss = 0.14963772\n",
            "Iteration 111, loss = 0.15134310\n",
            "Iteration 112, loss = 0.14931530\n",
            "Iteration 113, loss = 0.14807148\n",
            "Iteration 114, loss = 0.15041668\n",
            "Iteration 115, loss = 0.14823118\n",
            "Iteration 116, loss = 0.14759056\n",
            "Iteration 117, loss = 0.14763282\n",
            "Iteration 118, loss = 0.14691192\n",
            "Iteration 119, loss = 0.14581410\n",
            "Iteration 120, loss = 0.14655841\n",
            "Iteration 121, loss = 0.14572349\n",
            "Iteration 122, loss = 0.14672169\n",
            "Iteration 123, loss = 0.14483687\n",
            "Iteration 124, loss = 0.14692352\n",
            "Iteration 125, loss = 0.14287189\n",
            "Iteration 126, loss = 0.14230409\n",
            "Iteration 127, loss = 0.14345515\n",
            "Iteration 128, loss = 0.14236672\n",
            "Iteration 129, loss = 0.14307180\n",
            "Iteration 130, loss = 0.14355805\n",
            "Iteration 131, loss = 0.14004055\n",
            "Iteration 132, loss = 0.13923930\n",
            "Iteration 133, loss = 0.13996762\n",
            "Iteration 134, loss = 0.13743863\n",
            "Iteration 135, loss = 0.13723936\n",
            "Iteration 136, loss = 0.13751078\n",
            "Iteration 137, loss = 0.13927954\n",
            "Iteration 138, loss = 0.14115314\n",
            "Iteration 139, loss = 0.13713901\n",
            "Iteration 140, loss = 0.13580108\n",
            "Iteration 141, loss = 0.13683439\n",
            "Iteration 142, loss = 0.13549703\n",
            "Iteration 143, loss = 0.13437913\n",
            "Iteration 144, loss = 0.13685094\n",
            "Iteration 145, loss = 0.13373349\n",
            "Iteration 146, loss = 0.13372329\n",
            "Iteration 147, loss = 0.13325323\n",
            "Iteration 148, loss = 0.13401895\n",
            "Iteration 149, loss = 0.13247947\n",
            "Iteration 150, loss = 0.13363437\n",
            "Iteration 151, loss = 0.13211315\n",
            "Iteration 152, loss = 0.13036894\n",
            "Iteration 153, loss = 0.13107083\n",
            "Iteration 154, loss = 0.12993609\n",
            "Iteration 155, loss = 0.12992128\n",
            "Iteration 156, loss = 0.12860620\n",
            "Iteration 157, loss = 0.12910213\n",
            "Iteration 158, loss = 0.12994698\n",
            "Iteration 159, loss = 0.12760759\n",
            "Iteration 160, loss = 0.12953813\n",
            "Iteration 161, loss = 0.12936912\n",
            "Iteration 162, loss = 0.12869321\n",
            "Iteration 163, loss = 0.12961955\n",
            "Iteration 164, loss = 0.12841564\n",
            "Iteration 165, loss = 0.12963945\n",
            "Iteration 166, loss = 0.12636980\n",
            "Iteration 167, loss = 0.12975899\n",
            "Iteration 168, loss = 0.12778484\n",
            "Iteration 169, loss = 0.12439122\n",
            "Iteration 170, loss = 0.12583172\n",
            "Iteration 171, loss = 0.12328495\n",
            "Iteration 172, loss = 0.12480546\n",
            "Iteration 173, loss = 0.12429390\n",
            "Iteration 174, loss = 0.12430858\n",
            "Iteration 175, loss = 0.12282654\n",
            "Iteration 176, loss = 0.12498791\n",
            "Iteration 177, loss = 0.12272306\n",
            "Iteration 178, loss = 0.12248036\n",
            "Iteration 179, loss = 0.12174454\n",
            "Iteration 180, loss = 0.12239725\n",
            "Iteration 181, loss = 0.12102990\n",
            "Iteration 182, loss = 0.11970724\n",
            "Iteration 183, loss = 0.12207304\n",
            "Iteration 184, loss = 0.12307871\n",
            "Iteration 185, loss = 0.12122350\n",
            "Iteration 186, loss = 0.12198011\n",
            "Iteration 187, loss = 0.12129393\n",
            "Iteration 188, loss = 0.11985977\n",
            "Iteration 189, loss = 0.12089612\n",
            "Iteration 190, loss = 0.12113394\n",
            "Iteration 191, loss = 0.12342157\n",
            "Iteration 192, loss = 0.11983770\n",
            "Iteration 193, loss = 0.11758477\n",
            "Iteration 194, loss = 0.11847612\n",
            "Iteration 195, loss = 0.11786957\n",
            "Iteration 196, loss = 0.11826921\n",
            "Iteration 197, loss = 0.11875059\n",
            "Iteration 198, loss = 0.11591500\n",
            "Iteration 199, loss = 0.11796097\n",
            "Iteration 200, loss = 0.11681384\n",
            "Iteration 201, loss = 0.11845108\n",
            "Iteration 202, loss = 0.11773690\n",
            "Iteration 203, loss = 0.11999511\n",
            "Iteration 204, loss = 0.11764150\n",
            "Iteration 205, loss = 0.11489423\n",
            "Iteration 206, loss = 0.11552211\n",
            "Iteration 207, loss = 0.11741059\n",
            "Iteration 208, loss = 0.11475410\n",
            "Iteration 209, loss = 0.11550190\n",
            "Iteration 210, loss = 0.11586849\n",
            "Iteration 211, loss = 0.11476361\n",
            "Iteration 212, loss = 0.11410432\n",
            "Iteration 213, loss = 0.11351600\n",
            "Iteration 214, loss = 0.11309363\n",
            "Iteration 215, loss = 0.11487112\n",
            "Iteration 216, loss = 0.11468528\n",
            "Iteration 217, loss = 0.11321941\n",
            "Iteration 218, loss = 0.11822821\n",
            "Iteration 219, loss = 0.11240515\n",
            "Iteration 220, loss = inf\n",
            "Iteration 221, loss = 0.11388855\n",
            "Iteration 222, loss = 0.11219865\n",
            "Iteration 223, loss = 0.11148598\n",
            "Iteration 224, loss = 0.11404735\n",
            "Iteration 225, loss = 0.11110706\n",
            "Iteration 226, loss = 0.11276594\n",
            "Iteration 227, loss = 0.11194068\n",
            "Iteration 228, loss = 0.10924161\n",
            "Iteration 229, loss = 0.10967720\n",
            "Iteration 230, loss = 0.10989858\n",
            "Iteration 231, loss = 0.10971760\n",
            "Iteration 232, loss = 0.10914345\n",
            "Iteration 233, loss = 0.10830597\n",
            "Iteration 234, loss = 0.10959379\n",
            "Iteration 235, loss = 0.10918927\n",
            "Iteration 236, loss = 0.11369724\n",
            "Iteration 237, loss = 0.10913649\n",
            "Iteration 238, loss = 0.10923926\n",
            "Iteration 239, loss = 0.10695734\n",
            "Iteration 240, loss = 0.11030454\n",
            "Iteration 241, loss = 0.10802837\n",
            "Iteration 242, loss = 0.11081714\n",
            "Iteration 243, loss = 0.10999958\n",
            "Iteration 244, loss = 0.10835311\n",
            "Iteration 245, loss = 0.10855490\n",
            "Iteration 246, loss = 0.10869089\n",
            "Iteration 247, loss = 0.11015081\n",
            "Iteration 248, loss = 0.10680351\n",
            "Iteration 249, loss = 0.10699961\n",
            "Iteration 250, loss = 0.11090234\n",
            "Iteration 251, loss = 0.10534923\n",
            "Iteration 252, loss = 0.10561295\n",
            "Iteration 253, loss = 0.10572955\n",
            "Iteration 254, loss = 0.10599728\n",
            "Iteration 255, loss = 0.10674473\n",
            "Iteration 256, loss = 0.10494187\n",
            "Iteration 257, loss = 0.10615900\n",
            "Iteration 258, loss = 0.10346536\n",
            "Iteration 259, loss = 0.10639237\n",
            "Iteration 260, loss = 0.10536498\n",
            "Iteration 261, loss = 0.10526302\n",
            "Iteration 262, loss = 0.10627952\n",
            "Iteration 263, loss = 0.10603197\n",
            "Iteration 264, loss = 0.10578676\n",
            "Iteration 265, loss = 0.10451776\n",
            "Iteration 266, loss = 0.10262231\n",
            "Iteration 267, loss = 0.10276524\n",
            "Iteration 268, loss = 0.10510037\n",
            "Iteration 269, loss = 0.10215736\n",
            "Iteration 270, loss = 0.10461990\n",
            "Iteration 271, loss = 0.10452305\n",
            "Iteration 272, loss = 0.10567765\n",
            "Iteration 273, loss = 0.10256420\n",
            "Iteration 274, loss = 0.10575026\n",
            "Iteration 275, loss = 0.10472902\n",
            "Iteration 276, loss = 0.10257724\n",
            "Iteration 277, loss = 0.10351360\n",
            "Iteration 278, loss = 0.10371298\n",
            "Iteration 279, loss = 0.10433938\n",
            "Iteration 280, loss = 0.10527392\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECLqYWNY5w3s"
      },
      "source": [
        "predict_mlp = mlp.predict(X_df_test)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kpdAtSk6doS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baef2502-5ced-4f4c-e06d-5f4c5f51f8d7"
      },
      "source": [
        "accuracy_score(y_df_test, predict_mlp)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8141248720573183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAWSlA476mF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18e7f5b-2b52-4e86-8eca-01a090f3a3fb"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_mlp)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3231,  462],\n",
              "       [ 446,  746]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG4t-kps6q4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84aee61a-3e6c-43e0-814e-d81702a350a0"
      },
      "source": [
        "print(classification_report(y_df_test, predict_mlp))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.88      3693\n",
            "           1       0.62      0.63      0.62      1192\n",
            "\n",
            "    accuracy                           0.81      4885\n",
            "   macro avg       0.75      0.75      0.75      4885\n",
            "weighted avg       0.81      0.81      0.81      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85RO4hLp6tTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a27b14-6256-431f-d379-b9be25e1c610"
      },
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "score_mlp = cross_val_score(mlp, X_df, y_df, cv = kfold)\n",
        "score_mlp.mean()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.38069935\n",
            "Iteration 2, loss = 0.32203151\n",
            "Iteration 3, loss = 0.31041717\n",
            "Iteration 4, loss = 0.30389939\n",
            "Iteration 5, loss = 0.29940480\n",
            "Iteration 6, loss = 0.29529191\n",
            "Iteration 7, loss = 0.29179509\n",
            "Iteration 8, loss = 0.28919896\n",
            "Iteration 9, loss = 0.28692640\n",
            "Iteration 10, loss = 0.28402556\n",
            "Iteration 11, loss = 0.28177090\n",
            "Iteration 12, loss = 0.27812212\n",
            "Iteration 13, loss = 0.27613598\n",
            "Iteration 14, loss = 0.27396192\n",
            "Iteration 15, loss = 0.27159101\n",
            "Iteration 16, loss = 0.26911965\n",
            "Iteration 17, loss = 0.26603403\n",
            "Iteration 18, loss = 0.26414038\n",
            "Iteration 19, loss = 0.26185851\n",
            "Iteration 20, loss = 0.25953232\n",
            "Iteration 21, loss = 0.25701451\n",
            "Iteration 22, loss = 0.25455861\n",
            "Iteration 23, loss = 0.25334595\n",
            "Iteration 24, loss = 0.24972588\n",
            "Iteration 25, loss = 0.24761444\n",
            "Iteration 26, loss = 0.24452325\n",
            "Iteration 27, loss = 0.24250754\n",
            "Iteration 28, loss = 0.24003567\n",
            "Iteration 29, loss = 0.23812053\n",
            "Iteration 30, loss = 0.23602140\n",
            "Iteration 31, loss = 0.23431194\n",
            "Iteration 32, loss = 0.23057984\n",
            "Iteration 33, loss = 0.22971597\n",
            "Iteration 34, loss = 0.22706378\n",
            "Iteration 35, loss = 0.22417987\n",
            "Iteration 36, loss = 0.22268618\n",
            "Iteration 37, loss = 0.22053592\n",
            "Iteration 38, loss = 0.21903149\n",
            "Iteration 39, loss = 0.21699249\n",
            "Iteration 40, loss = 0.21607821\n",
            "Iteration 41, loss = 0.21449236\n",
            "Iteration 42, loss = 0.21307148\n",
            "Iteration 43, loss = 0.20930200\n",
            "Iteration 44, loss = 0.20790334\n",
            "Iteration 45, loss = 0.20649292\n",
            "Iteration 46, loss = 0.20411213\n",
            "Iteration 47, loss = 0.20317766\n",
            "Iteration 48, loss = 0.20171889\n",
            "Iteration 49, loss = 0.20152546\n",
            "Iteration 50, loss = 0.19918674\n",
            "Iteration 51, loss = 0.19834970\n",
            "Iteration 52, loss = 0.19553891\n",
            "Iteration 53, loss = 0.19423952\n",
            "Iteration 54, loss = 0.19421622\n",
            "Iteration 55, loss = 0.19150565\n",
            "Iteration 56, loss = 0.19066625\n",
            "Iteration 57, loss = 0.19161269\n",
            "Iteration 58, loss = 0.19011231\n",
            "Iteration 59, loss = 0.18833241\n",
            "Iteration 60, loss = 0.18603610\n",
            "Iteration 61, loss = 0.18422167\n",
            "Iteration 62, loss = 0.18293181\n",
            "Iteration 63, loss = 0.18254234\n",
            "Iteration 64, loss = 0.18197377\n",
            "Iteration 65, loss = 0.18210816\n",
            "Iteration 66, loss = 0.17963201\n",
            "Iteration 67, loss = 0.17960769\n",
            "Iteration 68, loss = 0.17862643\n",
            "Iteration 69, loss = 0.17796480\n",
            "Iteration 70, loss = 0.17593641\n",
            "Iteration 71, loss = 0.17380312\n",
            "Iteration 72, loss = 0.17322181\n",
            "Iteration 73, loss = 0.17359035\n",
            "Iteration 74, loss = 0.17113837\n",
            "Iteration 75, loss = 0.17268974\n",
            "Iteration 76, loss = 0.17209745\n",
            "Iteration 77, loss = 0.17000475\n",
            "Iteration 78, loss = 0.16762664\n",
            "Iteration 79, loss = 0.16736834\n",
            "Iteration 80, loss = 0.16567900\n",
            "Iteration 81, loss = 0.16516759\n",
            "Iteration 82, loss = 0.16585234\n",
            "Iteration 83, loss = 0.16412406\n",
            "Iteration 84, loss = 0.16385129\n",
            "Iteration 85, loss = 0.16284492\n",
            "Iteration 86, loss = 0.16346129\n",
            "Iteration 87, loss = 0.16279018\n",
            "Iteration 88, loss = 0.16162301\n",
            "Iteration 89, loss = 0.15851399\n",
            "Iteration 90, loss = 0.15874345\n",
            "Iteration 91, loss = 0.15926449\n",
            "Iteration 92, loss = 0.15788455\n",
            "Iteration 93, loss = 0.15787777\n",
            "Iteration 94, loss = 0.15738474\n",
            "Iteration 95, loss = 0.15588329\n",
            "Iteration 96, loss = 0.15627948\n",
            "Iteration 97, loss = 0.15437711\n",
            "Iteration 98, loss = 0.15363848\n",
            "Iteration 99, loss = 0.15348773\n",
            "Iteration 100, loss = 0.15478866\n",
            "Iteration 101, loss = 0.15309784\n",
            "Iteration 102, loss = 0.15276907\n",
            "Iteration 103, loss = 0.15021671\n",
            "Iteration 104, loss = 0.14960821\n",
            "Iteration 105, loss = 0.14978960\n",
            "Iteration 106, loss = 0.14951313\n",
            "Iteration 107, loss = 0.14939534\n",
            "Iteration 108, loss = 0.14773918\n",
            "Iteration 109, loss = 0.14857350\n",
            "Iteration 110, loss = 0.14673027\n",
            "Iteration 111, loss = 0.14758837\n",
            "Iteration 112, loss = 0.14653906\n",
            "Iteration 113, loss = 0.14789349\n",
            "Iteration 114, loss = 0.14715619\n",
            "Iteration 115, loss = 0.14540980\n",
            "Iteration 116, loss = 0.14345337\n",
            "Iteration 117, loss = 0.14373881\n",
            "Iteration 118, loss = 0.14306771\n",
            "Iteration 119, loss = 0.14348396\n",
            "Iteration 120, loss = 0.14319888\n",
            "Iteration 121, loss = 0.14134048\n",
            "Iteration 122, loss = 0.13929315\n",
            "Iteration 123, loss = 0.14150243\n",
            "Iteration 124, loss = 0.14126068\n",
            "Iteration 125, loss = 0.13995649\n",
            "Iteration 126, loss = 0.13986781\n",
            "Iteration 127, loss = 0.13975734\n",
            "Iteration 128, loss = 0.13988833\n",
            "Iteration 129, loss = 0.13574037\n",
            "Iteration 130, loss = 0.13604363\n",
            "Iteration 131, loss = 0.13602110\n",
            "Iteration 132, loss = 0.13547610\n",
            "Iteration 133, loss = 0.13595136\n",
            "Iteration 134, loss = 0.13714113\n",
            "Iteration 135, loss = 0.13590679\n",
            "Iteration 136, loss = 0.13517387\n",
            "Iteration 137, loss = 0.13618126\n",
            "Iteration 138, loss = 0.13451978\n",
            "Iteration 139, loss = 0.13679321\n",
            "Iteration 140, loss = 0.13409733\n",
            "Iteration 141, loss = 0.13251080\n",
            "Iteration 142, loss = 0.13363763\n",
            "Iteration 143, loss = 0.13356803\n",
            "Iteration 144, loss = 0.13243123\n",
            "Iteration 145, loss = 0.13252132\n",
            "Iteration 146, loss = 0.12987379\n",
            "Iteration 147, loss = 0.13146325\n",
            "Iteration 148, loss = 0.13332023\n",
            "Iteration 149, loss = 0.13021849\n",
            "Iteration 150, loss = 0.12901614\n",
            "Iteration 151, loss = 0.12902130\n",
            "Iteration 152, loss = 0.12853961\n",
            "Iteration 153, loss = 0.12966536\n",
            "Iteration 154, loss = 0.12928652\n",
            "Iteration 155, loss = 0.13029777\n",
            "Iteration 156, loss = 0.12612294\n",
            "Iteration 157, loss = 0.13045216\n",
            "Iteration 158, loss = 0.12704392\n",
            "Iteration 159, loss = 0.12681885\n",
            "Iteration 160, loss = 0.12537452\n",
            "Iteration 161, loss = 0.12559724\n",
            "Iteration 162, loss = 0.12531440\n",
            "Iteration 163, loss = 0.12548698\n",
            "Iteration 164, loss = 0.12485306\n",
            "Iteration 165, loss = 0.12558309\n",
            "Iteration 166, loss = 0.12315476\n",
            "Iteration 167, loss = 0.12264730\n",
            "Iteration 168, loss = 0.12294794\n",
            "Iteration 169, loss = 0.12411863\n",
            "Iteration 170, loss = 0.12515913\n",
            "Iteration 171, loss = 0.12508476\n",
            "Iteration 172, loss = 0.12348096\n",
            "Iteration 173, loss = 0.12419576\n",
            "Iteration 174, loss = 0.12198386\n",
            "Iteration 175, loss = 0.11967388\n",
            "Iteration 176, loss = 0.12112275\n",
            "Iteration 177, loss = 0.12165546\n",
            "Iteration 178, loss = 0.12134795\n",
            "Iteration 179, loss = 0.12285041\n",
            "Iteration 180, loss = 0.12060011\n",
            "Iteration 181, loss = 0.11934042\n",
            "Iteration 182, loss = 0.12123360\n",
            "Iteration 183, loss = 0.11832168\n",
            "Iteration 184, loss = 0.11983099\n",
            "Iteration 185, loss = 0.11868538\n",
            "Iteration 186, loss = 0.11836113\n",
            "Iteration 187, loss = 0.11765378\n",
            "Iteration 188, loss = 0.11946292\n",
            "Iteration 189, loss = 0.11747887\n",
            "Iteration 190, loss = 0.11863406\n",
            "Iteration 191, loss = 0.11790894\n",
            "Iteration 192, loss = 0.11594011\n",
            "Iteration 193, loss = 0.11524562\n",
            "Iteration 194, loss = 0.11883004\n",
            "Iteration 195, loss = 0.11832216\n",
            "Iteration 196, loss = 0.11718514\n",
            "Iteration 197, loss = 0.11985739\n",
            "Iteration 198, loss = 0.11747472\n",
            "Iteration 199, loss = 0.11561802\n",
            "Iteration 200, loss = 0.11694331\n",
            "Iteration 201, loss = 0.11373286\n",
            "Iteration 202, loss = 0.11422242\n",
            "Iteration 203, loss = 0.11581096\n",
            "Iteration 204, loss = 0.11478735\n",
            "Iteration 205, loss = 0.11543974\n",
            "Iteration 206, loss = 0.11646852\n",
            "Iteration 207, loss = inf\n",
            "Iteration 208, loss = 0.11363433\n",
            "Iteration 209, loss = 0.11344220\n",
            "Iteration 210, loss = 0.11118251\n",
            "Iteration 211, loss = 0.11252987\n",
            "Iteration 212, loss = 0.11178114\n",
            "Iteration 213, loss = 0.11187166\n",
            "Iteration 214, loss = 0.11142762\n",
            "Iteration 215, loss = 0.11219953\n",
            "Iteration 216, loss = 0.11183364\n",
            "Iteration 217, loss = 0.11147546\n",
            "Iteration 218, loss = 0.11051788\n",
            "Iteration 219, loss = 0.10772013\n",
            "Iteration 220, loss = 0.11003303\n",
            "Iteration 221, loss = 0.10977725\n",
            "Iteration 222, loss = 0.11202785\n",
            "Iteration 223, loss = 0.11158985\n",
            "Iteration 224, loss = 0.11046968\n",
            "Iteration 225, loss = 0.11423763\n",
            "Iteration 226, loss = 0.11182822\n",
            "Iteration 227, loss = 0.11264217\n",
            "Iteration 228, loss = 0.10919768\n",
            "Iteration 229, loss = 0.11020302\n",
            "Iteration 230, loss = 0.11001707\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.39359467\n",
            "Iteration 2, loss = 0.32089163\n",
            "Iteration 3, loss = 0.31028261\n",
            "Iteration 4, loss = 0.30336828\n",
            "Iteration 5, loss = 0.29818579\n",
            "Iteration 6, loss = 0.29516591\n",
            "Iteration 7, loss = 0.29193332\n",
            "Iteration 8, loss = 0.28984892\n",
            "Iteration 9, loss = 0.28565781\n",
            "Iteration 10, loss = 0.28339752\n",
            "Iteration 11, loss = 0.28030881\n",
            "Iteration 12, loss = 0.27893837\n",
            "Iteration 13, loss = 0.27582237\n",
            "Iteration 14, loss = 0.27369118\n",
            "Iteration 15, loss = 0.27185578\n",
            "Iteration 16, loss = 0.26915654\n",
            "Iteration 17, loss = 0.26753837\n",
            "Iteration 18, loss = 0.26428763\n",
            "Iteration 19, loss = 0.26198008\n",
            "Iteration 20, loss = 0.26030523\n",
            "Iteration 21, loss = 0.25830249\n",
            "Iteration 22, loss = 0.25485933\n",
            "Iteration 23, loss = 0.25395475\n",
            "Iteration 24, loss = 0.25272981\n",
            "Iteration 25, loss = 0.24957573\n",
            "Iteration 26, loss = 0.24741885\n",
            "Iteration 27, loss = 0.24475223\n",
            "Iteration 28, loss = 0.24207959\n",
            "Iteration 29, loss = 0.24142650\n",
            "Iteration 30, loss = 0.23995788\n",
            "Iteration 31, loss = 0.23780341\n",
            "Iteration 32, loss = 0.23600450\n",
            "Iteration 33, loss = 0.23355889\n",
            "Iteration 34, loss = 0.23118339\n",
            "Iteration 35, loss = 0.22970956\n",
            "Iteration 36, loss = 0.22762334\n",
            "Iteration 37, loss = 0.22566774\n",
            "Iteration 38, loss = 0.22327160\n",
            "Iteration 39, loss = 0.22369450\n",
            "Iteration 40, loss = 0.22062781\n",
            "Iteration 41, loss = 0.21800171\n",
            "Iteration 42, loss = 0.21689266\n",
            "Iteration 43, loss = 0.21611405\n",
            "Iteration 44, loss = 0.21488773\n",
            "Iteration 45, loss = 0.21255385\n",
            "Iteration 46, loss = 0.21213621\n",
            "Iteration 47, loss = 0.21221985\n",
            "Iteration 48, loss = 0.20790327\n",
            "Iteration 49, loss = 0.20642223\n",
            "Iteration 50, loss = 0.20478598\n",
            "Iteration 51, loss = 0.20416486\n",
            "Iteration 52, loss = 0.20256979\n",
            "Iteration 53, loss = 0.20179442\n",
            "Iteration 54, loss = 0.20059289\n",
            "Iteration 55, loss = 0.19930975\n",
            "Iteration 56, loss = 0.20191513\n",
            "Iteration 57, loss = 0.19794858\n",
            "Iteration 58, loss = 0.19531735\n",
            "Iteration 59, loss = 0.19361227\n",
            "Iteration 60, loss = 0.19228992\n",
            "Iteration 61, loss = 0.19146583\n",
            "Iteration 62, loss = 0.19237467\n",
            "Iteration 63, loss = 0.18920816\n",
            "Iteration 64, loss = 0.18706231\n",
            "Iteration 65, loss = 0.18669282\n",
            "Iteration 66, loss = 0.18524795\n",
            "Iteration 67, loss = 0.18499463\n",
            "Iteration 68, loss = 0.18423996\n",
            "Iteration 69, loss = 0.18235167\n",
            "Iteration 70, loss = 0.18474215\n",
            "Iteration 71, loss = 0.18404105\n",
            "Iteration 72, loss = 0.17987109\n",
            "Iteration 73, loss = 0.17886711\n",
            "Iteration 74, loss = 0.17863724\n",
            "Iteration 75, loss = 0.17592537\n",
            "Iteration 76, loss = 0.17698921\n",
            "Iteration 77, loss = 0.17598162\n",
            "Iteration 78, loss = 0.17458701\n",
            "Iteration 79, loss = 0.17282173\n",
            "Iteration 80, loss = 0.17191697\n",
            "Iteration 81, loss = 0.17323700\n",
            "Iteration 82, loss = 0.17116075\n",
            "Iteration 83, loss = 0.16947586\n",
            "Iteration 84, loss = 0.17024287\n",
            "Iteration 85, loss = 0.17087730\n",
            "Iteration 86, loss = 0.16829435\n",
            "Iteration 87, loss = 0.16858051\n",
            "Iteration 88, loss = 0.16655256\n",
            "Iteration 89, loss = 0.16610071\n",
            "Iteration 90, loss = 0.16528690\n",
            "Iteration 91, loss = 0.16652787\n",
            "Iteration 92, loss = 0.16312538\n",
            "Iteration 93, loss = 0.16339384\n",
            "Iteration 94, loss = 0.16203448\n",
            "Iteration 95, loss = 0.16085620\n",
            "Iteration 96, loss = 0.16035556\n",
            "Iteration 97, loss = 0.15985885\n",
            "Iteration 98, loss = 0.15946029\n",
            "Iteration 99, loss = 0.15850765\n",
            "Iteration 100, loss = 0.15880724\n",
            "Iteration 101, loss = 0.15869586\n",
            "Iteration 102, loss = 0.15778713\n",
            "Iteration 103, loss = 0.15638212\n",
            "Iteration 104, loss = 0.15508401\n",
            "Iteration 105, loss = 0.15427814\n",
            "Iteration 106, loss = 0.15409338\n",
            "Iteration 107, loss = 0.15304869\n",
            "Iteration 108, loss = 0.15341570\n",
            "Iteration 109, loss = 0.15352575\n",
            "Iteration 110, loss = 0.15295862\n",
            "Iteration 111, loss = 0.15062282\n",
            "Iteration 112, loss = 0.15155736\n",
            "Iteration 113, loss = 0.15326227\n",
            "Iteration 114, loss = 0.15066705\n",
            "Iteration 115, loss = 0.15338630\n",
            "Iteration 116, loss = 0.15262383\n",
            "Iteration 117, loss = 0.15120050\n",
            "Iteration 118, loss = 0.15021620\n",
            "Iteration 119, loss = 0.15137334\n",
            "Iteration 120, loss = 0.14981015\n",
            "Iteration 121, loss = 0.14801403\n",
            "Iteration 122, loss = 0.14548898\n",
            "Iteration 123, loss = 0.14489849\n",
            "Iteration 124, loss = 0.14456549\n",
            "Iteration 125, loss = 0.14300695\n",
            "Iteration 126, loss = 0.14625288\n",
            "Iteration 127, loss = 0.14504204\n",
            "Iteration 128, loss = 0.14407401\n",
            "Iteration 129, loss = 0.14277047\n",
            "Iteration 130, loss = 0.14239125\n",
            "Iteration 131, loss = 0.14142478\n",
            "Iteration 132, loss = 0.14357111\n",
            "Iteration 133, loss = 0.13992456\n",
            "Iteration 134, loss = 0.14048722\n",
            "Iteration 135, loss = 0.13974879\n",
            "Iteration 136, loss = 0.14016718\n",
            "Iteration 137, loss = 0.13943102\n",
            "Iteration 138, loss = 0.13894490\n",
            "Iteration 139, loss = 0.13978289\n",
            "Iteration 140, loss = 0.13888568\n",
            "Iteration 141, loss = 0.14004253\n",
            "Iteration 142, loss = 0.13810939\n",
            "Iteration 143, loss = 0.13646193\n",
            "Iteration 144, loss = 0.13600785\n",
            "Iteration 145, loss = 0.13643172\n",
            "Iteration 146, loss = 0.13596537\n",
            "Iteration 147, loss = 0.13516151\n",
            "Iteration 148, loss = 0.13400576\n",
            "Iteration 149, loss = 0.13494504\n",
            "Iteration 150, loss = 0.13633352\n",
            "Iteration 151, loss = 0.13364149\n",
            "Iteration 152, loss = 0.13396586\n",
            "Iteration 153, loss = 0.13290648\n",
            "Iteration 154, loss = 0.13306899\n",
            "Iteration 155, loss = 0.13451800\n",
            "Iteration 156, loss = 0.13369136\n",
            "Iteration 157, loss = 0.13159515\n",
            "Iteration 158, loss = 0.13283619\n",
            "Iteration 159, loss = 0.13240480\n",
            "Iteration 160, loss = 0.13095854\n",
            "Iteration 161, loss = 0.13045228\n",
            "Iteration 162, loss = 0.13195725\n",
            "Iteration 163, loss = 0.13231190\n",
            "Iteration 164, loss = 0.13173312\n",
            "Iteration 165, loss = 0.12846755\n",
            "Iteration 166, loss = 0.12843749\n",
            "Iteration 167, loss = 0.13090782\n",
            "Iteration 168, loss = 0.12693145\n",
            "Iteration 169, loss = 0.12852931\n",
            "Iteration 170, loss = 0.12691361\n",
            "Iteration 171, loss = 0.12570527\n",
            "Iteration 172, loss = 0.12551998\n",
            "Iteration 173, loss = 0.12580174\n",
            "Iteration 174, loss = 0.12728654\n",
            "Iteration 175, loss = 0.12516161\n",
            "Iteration 176, loss = 0.12762576\n",
            "Iteration 177, loss = 0.12584682\n",
            "Iteration 178, loss = 0.12676773\n",
            "Iteration 179, loss = 0.12451458\n",
            "Iteration 180, loss = 0.12447609\n",
            "Iteration 181, loss = 0.12606978\n",
            "Iteration 182, loss = 0.12371900\n",
            "Iteration 183, loss = 0.12590827\n",
            "Iteration 184, loss = 0.12642767\n",
            "Iteration 185, loss = 0.12421544\n",
            "Iteration 186, loss = 0.12363533\n",
            "Iteration 187, loss = 0.12110370\n",
            "Iteration 188, loss = 0.12163562\n",
            "Iteration 189, loss = 0.12381458\n",
            "Iteration 190, loss = 0.11928332\n",
            "Iteration 191, loss = 0.12318438\n",
            "Iteration 192, loss = 0.12214147\n",
            "Iteration 193, loss = 0.12207586\n",
            "Iteration 194, loss = 0.12125435\n",
            "Iteration 195, loss = 0.12123710\n",
            "Iteration 196, loss = 0.12117024\n",
            "Iteration 197, loss = 0.12103898\n",
            "Iteration 198, loss = 0.11930951\n",
            "Iteration 199, loss = 0.12061394\n",
            "Iteration 200, loss = 0.12025734\n",
            "Iteration 201, loss = 0.11913252\n",
            "Iteration 202, loss = 0.11857633\n",
            "Iteration 203, loss = 0.12181104\n",
            "Iteration 204, loss = 0.11870648\n",
            "Iteration 205, loss = 0.11678157\n",
            "Iteration 206, loss = 0.11720369\n",
            "Iteration 207, loss = 0.11732852\n",
            "Iteration 208, loss = 0.11709724\n",
            "Iteration 209, loss = 0.11988168\n",
            "Iteration 210, loss = 0.11667091\n",
            "Iteration 211, loss = 0.11399406\n",
            "Iteration 212, loss = 0.11677161\n",
            "Iteration 213, loss = 0.11572974\n",
            "Iteration 214, loss = 0.11500251\n",
            "Iteration 215, loss = 0.11701999\n",
            "Iteration 216, loss = 0.11890761\n",
            "Iteration 217, loss = 0.11798888\n",
            "Iteration 218, loss = 0.11696802\n",
            "Iteration 219, loss = inf\n",
            "Iteration 220, loss = 0.11805864\n",
            "Iteration 221, loss = 0.11859006\n",
            "Iteration 222, loss = 0.11728671\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.36309071\n",
            "Iteration 2, loss = 0.31989410\n",
            "Iteration 3, loss = 0.30966954\n",
            "Iteration 4, loss = 0.30228189\n",
            "Iteration 5, loss = 0.29724511\n",
            "Iteration 6, loss = 0.29488758\n",
            "Iteration 7, loss = 0.29008256\n",
            "Iteration 8, loss = 0.28901001\n",
            "Iteration 9, loss = 0.28515484\n",
            "Iteration 10, loss = 0.28214933\n",
            "Iteration 11, loss = 0.27962262\n",
            "Iteration 12, loss = 0.27752573\n",
            "Iteration 13, loss = 0.27493393\n",
            "Iteration 14, loss = 0.27240700\n",
            "Iteration 15, loss = 0.26992947\n",
            "Iteration 16, loss = 0.26705212\n",
            "Iteration 17, loss = 0.26466926\n",
            "Iteration 18, loss = 0.26416911\n",
            "Iteration 19, loss = 0.26040802\n",
            "Iteration 20, loss = 0.25759019\n",
            "Iteration 21, loss = 0.25539713\n",
            "Iteration 22, loss = 0.25228125\n",
            "Iteration 23, loss = 0.25138921\n",
            "Iteration 24, loss = 0.24846836\n",
            "Iteration 25, loss = 0.24703048\n",
            "Iteration 26, loss = 0.24633011\n",
            "Iteration 27, loss = 0.24263230\n",
            "Iteration 28, loss = 0.23913039\n",
            "Iteration 29, loss = 0.23769921\n",
            "Iteration 30, loss = 0.23458211\n",
            "Iteration 31, loss = 0.23285804\n",
            "Iteration 32, loss = 0.23297918\n",
            "Iteration 33, loss = 0.23151051\n",
            "Iteration 34, loss = 0.22978749\n",
            "Iteration 35, loss = 0.22715551\n",
            "Iteration 36, loss = 0.22445279\n",
            "Iteration 37, loss = 0.22361957\n",
            "Iteration 38, loss = 0.22243561\n",
            "Iteration 39, loss = 0.21971049\n",
            "Iteration 40, loss = 0.21866332\n",
            "Iteration 41, loss = 0.21755759\n",
            "Iteration 42, loss = 0.21547754\n",
            "Iteration 43, loss = 0.21191925\n",
            "Iteration 44, loss = 0.21258790\n",
            "Iteration 45, loss = 0.20971767\n",
            "Iteration 46, loss = 0.20858204\n",
            "Iteration 47, loss = 0.20829915\n",
            "Iteration 48, loss = 0.20607546\n",
            "Iteration 49, loss = 0.20325233\n",
            "Iteration 50, loss = 0.20323805\n",
            "Iteration 51, loss = 0.20173561\n",
            "Iteration 52, loss = 0.20133575\n",
            "Iteration 53, loss = 0.20055729\n",
            "Iteration 54, loss = 0.19800050\n",
            "Iteration 55, loss = 0.19508570\n",
            "Iteration 56, loss = 0.19486213\n",
            "Iteration 57, loss = 0.19444910\n",
            "Iteration 58, loss = 0.19383716\n",
            "Iteration 59, loss = 0.19417699\n",
            "Iteration 60, loss = 0.19201195\n",
            "Iteration 61, loss = 0.18975687\n",
            "Iteration 62, loss = 0.18863797\n",
            "Iteration 63, loss = 0.18464810\n",
            "Iteration 64, loss = 0.18487993\n",
            "Iteration 65, loss = 0.18468361\n",
            "Iteration 66, loss = 0.18339215\n",
            "Iteration 67, loss = 0.18415345\n",
            "Iteration 68, loss = 0.18136681\n",
            "Iteration 69, loss = 0.18063303\n",
            "Iteration 70, loss = 0.17836862\n",
            "Iteration 71, loss = 0.18025669\n",
            "Iteration 72, loss = 0.17662510\n",
            "Iteration 73, loss = 0.17799168\n",
            "Iteration 74, loss = 0.17660929\n",
            "Iteration 75, loss = 0.17579484\n",
            "Iteration 76, loss = 0.17648850\n",
            "Iteration 77, loss = 0.17374830\n",
            "Iteration 78, loss = 0.17127588\n",
            "Iteration 79, loss = 0.17096774\n",
            "Iteration 80, loss = 0.17129828\n",
            "Iteration 81, loss = 0.17238581\n",
            "Iteration 82, loss = 0.17092484\n",
            "Iteration 83, loss = 0.16830380\n",
            "Iteration 84, loss = 0.16861259\n",
            "Iteration 85, loss = 0.16888425\n",
            "Iteration 86, loss = 0.17022185\n",
            "Iteration 87, loss = 0.16552968\n",
            "Iteration 88, loss = 0.16613219\n",
            "Iteration 89, loss = 0.16624883\n",
            "Iteration 90, loss = 0.16420155\n",
            "Iteration 91, loss = 0.16533611\n",
            "Iteration 92, loss = 0.16167160\n",
            "Iteration 93, loss = 0.16061363\n",
            "Iteration 94, loss = 0.16212106\n",
            "Iteration 95, loss = 0.16077863\n",
            "Iteration 96, loss = 0.15994469\n",
            "Iteration 97, loss = 0.15847960\n",
            "Iteration 98, loss = 0.15766031\n",
            "Iteration 99, loss = 0.15749914\n",
            "Iteration 100, loss = 0.15731910\n",
            "Iteration 101, loss = 0.15756997\n",
            "Iteration 102, loss = 0.15541547\n",
            "Iteration 103, loss = 0.15444092\n",
            "Iteration 104, loss = 0.15311956\n",
            "Iteration 105, loss = 0.15646460\n",
            "Iteration 106, loss = 0.15587943\n",
            "Iteration 107, loss = 0.15386048\n",
            "Iteration 108, loss = 0.15142131\n",
            "Iteration 109, loss = 0.15430438\n",
            "Iteration 110, loss = 0.15184254\n",
            "Iteration 111, loss = 0.15089794\n",
            "Iteration 112, loss = 0.15148160\n",
            "Iteration 113, loss = 0.15059802\n",
            "Iteration 114, loss = 0.14856008\n",
            "Iteration 115, loss = 0.14680105\n",
            "Iteration 116, loss = 0.14713226\n",
            "Iteration 117, loss = 0.14941163\n",
            "Iteration 118, loss = 0.14612109\n",
            "Iteration 119, loss = 0.14838190\n",
            "Iteration 120, loss = 0.14677822\n",
            "Iteration 121, loss = 0.14615658\n",
            "Iteration 122, loss = 0.14848557\n",
            "Iteration 123, loss = 0.14929989\n",
            "Iteration 124, loss = 0.14372301\n",
            "Iteration 125, loss = 0.14404796\n",
            "Iteration 126, loss = 0.14437402\n",
            "Iteration 127, loss = 0.14215400\n",
            "Iteration 128, loss = 0.14316622\n",
            "Iteration 129, loss = 0.14131121\n",
            "Iteration 130, loss = 0.14200233\n",
            "Iteration 131, loss = 0.14272806\n",
            "Iteration 132, loss = 0.14198904\n",
            "Iteration 133, loss = 0.14104950\n",
            "Iteration 134, loss = 0.14051286\n",
            "Iteration 135, loss = 0.13897482\n",
            "Iteration 136, loss = 0.13896994\n",
            "Iteration 137, loss = 0.14039527\n",
            "Iteration 138, loss = 0.13785971\n",
            "Iteration 139, loss = 0.13735042\n",
            "Iteration 140, loss = 0.13735446\n",
            "Iteration 141, loss = 0.13875291\n",
            "Iteration 142, loss = 0.13818728\n",
            "Iteration 143, loss = 0.13782062\n",
            "Iteration 144, loss = 0.14042135\n",
            "Iteration 145, loss = 0.14301917\n",
            "Iteration 146, loss = 0.13470836\n",
            "Iteration 147, loss = 0.13648837\n",
            "Iteration 148, loss = 0.13534954\n",
            "Iteration 149, loss = 0.13277563\n",
            "Iteration 150, loss = 0.13301731\n",
            "Iteration 151, loss = 0.13478271\n",
            "Iteration 152, loss = 0.13370759\n",
            "Iteration 153, loss = 0.13254769\n",
            "Iteration 154, loss = 0.13300716\n",
            "Iteration 155, loss = 0.13143970\n",
            "Iteration 156, loss = 0.13354123\n",
            "Iteration 157, loss = 0.13222689\n",
            "Iteration 158, loss = 0.13350053\n",
            "Iteration 159, loss = 0.13434489\n",
            "Iteration 160, loss = 0.13052418\n",
            "Iteration 161, loss = 0.13235354\n",
            "Iteration 162, loss = 0.13085187\n",
            "Iteration 163, loss = 0.13096116\n",
            "Iteration 164, loss = 0.13148199\n",
            "Iteration 165, loss = 0.12938574\n",
            "Iteration 166, loss = 0.13094865\n",
            "Iteration 167, loss = 0.12928952\n",
            "Iteration 168, loss = 0.13033256\n",
            "Iteration 169, loss = 0.12686821\n",
            "Iteration 170, loss = 0.12784107\n",
            "Iteration 171, loss = 0.12542104\n",
            "Iteration 172, loss = 0.12585138\n",
            "Iteration 173, loss = 0.12687381\n",
            "Iteration 174, loss = 0.12714681\n",
            "Iteration 175, loss = 0.12510041\n",
            "Iteration 176, loss = 0.12578525\n",
            "Iteration 177, loss = 0.12602347\n",
            "Iteration 178, loss = 0.12809014\n",
            "Iteration 179, loss = 0.12685308\n",
            "Iteration 180, loss = inf\n",
            "Iteration 181, loss = 0.12840177\n",
            "Iteration 182, loss = 0.12387909\n",
            "Iteration 183, loss = 0.12552309\n",
            "Iteration 184, loss = 0.12574544\n",
            "Iteration 185, loss = 0.12385567\n",
            "Iteration 186, loss = 0.12362662\n",
            "Iteration 187, loss = 0.12272256\n",
            "Iteration 188, loss = 0.12383264\n",
            "Iteration 189, loss = 0.12183005\n",
            "Iteration 190, loss = 0.12245276\n",
            "Iteration 191, loss = 0.12332066\n",
            "Iteration 192, loss = 0.12051289\n",
            "Iteration 193, loss = 0.12126521\n",
            "Iteration 194, loss = 0.12326096\n",
            "Iteration 195, loss = 0.12073645\n",
            "Iteration 196, loss = 0.12177095\n",
            "Iteration 197, loss = 0.12553774\n",
            "Iteration 198, loss = 0.12140014\n",
            "Iteration 199, loss = 0.12147083\n",
            "Iteration 200, loss = 0.12066829\n",
            "Iteration 201, loss = 0.11911832\n",
            "Iteration 202, loss = 0.12051770\n",
            "Iteration 203, loss = 0.11931735\n",
            "Iteration 204, loss = 0.11899756\n",
            "Iteration 205, loss = 0.12026649\n",
            "Iteration 206, loss = 0.11900142\n",
            "Iteration 207, loss = 0.11786947\n",
            "Iteration 208, loss = 0.11767544\n",
            "Iteration 209, loss = 0.11864176\n",
            "Iteration 210, loss = 0.11836463\n",
            "Iteration 211, loss = 0.11684678\n",
            "Iteration 212, loss = 0.11990029\n",
            "Iteration 213, loss = 0.11917973\n",
            "Iteration 214, loss = 0.11685351\n",
            "Iteration 215, loss = 0.11976950\n",
            "Iteration 216, loss = 0.11657645\n",
            "Iteration 217, loss = 0.11754635\n",
            "Iteration 218, loss = 0.11618452\n",
            "Iteration 219, loss = 0.11544461\n",
            "Iteration 220, loss = 0.11686321\n",
            "Iteration 221, loss = 0.11704457\n",
            "Iteration 222, loss = 0.11451014\n",
            "Iteration 223, loss = 0.11739189\n",
            "Iteration 224, loss = 0.11444480\n",
            "Iteration 225, loss = 0.11374253\n",
            "Iteration 226, loss = 0.11510065\n",
            "Iteration 227, loss = 0.11483938\n",
            "Iteration 228, loss = 0.11507106\n",
            "Iteration 229, loss = 0.11546868\n",
            "Iteration 230, loss = 0.11777189\n",
            "Iteration 231, loss = 0.11476541\n",
            "Iteration 232, loss = 0.11572405\n",
            "Iteration 233, loss = 0.11309266\n",
            "Iteration 234, loss = 0.11251679\n",
            "Iteration 235, loss = 0.11343624\n",
            "Iteration 236, loss = 0.11182029\n",
            "Iteration 237, loss = 0.11363399\n",
            "Iteration 238, loss = 0.11131437\n",
            "Iteration 239, loss = 0.11275134\n",
            "Iteration 240, loss = 0.10949125\n",
            "Iteration 241, loss = 0.11170480\n",
            "Iteration 242, loss = 0.11523223\n",
            "Iteration 243, loss = 0.11796070\n",
            "Iteration 244, loss = 0.11090642\n",
            "Iteration 245, loss = 0.11056675\n",
            "Iteration 246, loss = 0.10992560\n",
            "Iteration 247, loss = 0.11287592\n",
            "Iteration 248, loss = 0.10991356\n",
            "Iteration 249, loss = 0.11213341\n",
            "Iteration 250, loss = 0.11135899\n",
            "Iteration 251, loss = 0.10928408\n",
            "Iteration 252, loss = 0.10972276\n",
            "Iteration 253, loss = 0.11042881\n",
            "Iteration 254, loss = 0.10912469\n",
            "Iteration 255, loss = 0.10728607\n",
            "Iteration 256, loss = 0.10682765\n",
            "Iteration 257, loss = 0.10913829\n",
            "Iteration 258, loss = 0.10887926\n",
            "Iteration 259, loss = 0.10961990\n",
            "Iteration 260, loss = 0.10733962\n",
            "Iteration 261, loss = 0.10622159\n",
            "Iteration 262, loss = 0.10927095\n",
            "Iteration 263, loss = 0.11050506\n",
            "Iteration 264, loss = 0.10903221\n",
            "Iteration 265, loss = 0.10679782\n",
            "Iteration 266, loss = 0.10676009\n",
            "Iteration 267, loss = 0.10679573\n",
            "Iteration 268, loss = 0.10796257\n",
            "Iteration 269, loss = 0.10617257\n",
            "Iteration 270, loss = 0.10652293\n",
            "Iteration 271, loss = 0.10878224\n",
            "Iteration 272, loss = 0.10832381\n",
            "Iteration 273, loss = 0.10666107\n",
            "Iteration 274, loss = 0.10501715\n",
            "Iteration 275, loss = 0.10827244\n",
            "Iteration 276, loss = 0.10721305\n",
            "Iteration 277, loss = 0.10475025\n",
            "Iteration 278, loss = 0.10507376\n",
            "Iteration 279, loss = 0.10665836\n",
            "Iteration 280, loss = 0.10609892\n",
            "Iteration 281, loss = 0.10822970\n",
            "Iteration 282, loss = 0.11155262\n",
            "Iteration 283, loss = 0.10451470\n",
            "Iteration 284, loss = 0.10593021\n",
            "Iteration 285, loss = 0.10390713\n",
            "Iteration 286, loss = 0.10707551\n",
            "Iteration 287, loss = 0.10706832\n",
            "Iteration 288, loss = 0.10286104\n",
            "Iteration 289, loss = 0.10159792\n",
            "Iteration 290, loss = 0.10716080\n",
            "Iteration 291, loss = 0.10474114\n",
            "Iteration 292, loss = 0.10399110\n",
            "Iteration 293, loss = 0.10399130\n",
            "Iteration 294, loss = 0.10239604\n",
            "Iteration 295, loss = 0.10466194\n",
            "Iteration 296, loss = 0.10285347\n",
            "Iteration 297, loss = 0.10097659\n",
            "Iteration 298, loss = 0.10344061\n",
            "Iteration 299, loss = 0.10271075\n",
            "Iteration 300, loss = 0.10378815\n",
            "Iteration 301, loss = 0.10226059\n",
            "Iteration 302, loss = 0.10087728\n",
            "Iteration 303, loss = 0.10362441\n",
            "Iteration 304, loss = 0.10281316\n",
            "Iteration 305, loss = 0.10652186\n",
            "Iteration 306, loss = 0.10198004\n",
            "Iteration 307, loss = 0.09961760\n",
            "Iteration 308, loss = 0.10038560\n",
            "Iteration 309, loss = 0.10209889\n",
            "Iteration 310, loss = 0.10347422\n",
            "Iteration 311, loss = 0.10125218\n",
            "Iteration 312, loss = 0.10178141\n",
            "Iteration 313, loss = 0.09901300\n",
            "Iteration 314, loss = 0.10112687\n",
            "Iteration 315, loss = 0.10208811\n",
            "Iteration 316, loss = 0.10032435\n",
            "Iteration 317, loss = 0.09990356\n",
            "Iteration 318, loss = 0.10080950\n",
            "Iteration 319, loss = 0.10184445\n",
            "Iteration 320, loss = 0.10117392\n",
            "Iteration 321, loss = 0.10258935\n",
            "Iteration 322, loss = 0.09795148\n",
            "Iteration 323, loss = 0.09929305\n",
            "Iteration 324, loss = 0.09800787\n",
            "Iteration 325, loss = 0.09781300\n",
            "Iteration 326, loss = 0.10157073\n",
            "Iteration 327, loss = 0.09944845\n",
            "Iteration 328, loss = 0.10006533\n",
            "Iteration 329, loss = 0.10115139\n",
            "Iteration 330, loss = 0.09951813\n",
            "Iteration 331, loss = 0.09909103\n",
            "Iteration 332, loss = 0.09688710\n",
            "Iteration 333, loss = 0.09812401\n",
            "Iteration 334, loss = 0.09831052\n",
            "Iteration 335, loss = 0.09635976\n",
            "Iteration 336, loss = 0.09805219\n",
            "Iteration 337, loss = 0.10050664\n",
            "Iteration 338, loss = 0.09649002\n",
            "Iteration 339, loss = 0.09678070\n",
            "Iteration 340, loss = 0.09753063\n",
            "Iteration 341, loss = 0.11259789\n",
            "Iteration 342, loss = 0.10028027\n",
            "Iteration 343, loss = 0.09526621\n",
            "Iteration 344, loss = 0.09484590\n",
            "Iteration 345, loss = 0.10184411\n",
            "Iteration 346, loss = 0.10054909\n",
            "Iteration 347, loss = 0.09936390\n",
            "Iteration 348, loss = 0.09701911\n",
            "Iteration 349, loss = 0.09725772\n",
            "Iteration 350, loss = 0.09734946\n",
            "Iteration 351, loss = 0.09919957\n",
            "Iteration 352, loss = 0.09778030\n",
            "Iteration 353, loss = 0.09647843\n",
            "Iteration 354, loss = 0.09532192\n",
            "Iteration 355, loss = 0.09501418\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.37057490\n",
            "Iteration 2, loss = 0.32167025\n",
            "Iteration 3, loss = 0.30974285\n",
            "Iteration 4, loss = 0.30279340\n",
            "Iteration 5, loss = 0.29762544\n",
            "Iteration 6, loss = 0.29373852\n",
            "Iteration 7, loss = 0.29114369\n",
            "Iteration 8, loss = 0.28682368\n",
            "Iteration 9, loss = 0.28365375\n",
            "Iteration 10, loss = 0.28132614\n",
            "Iteration 11, loss = 0.27862330\n",
            "Iteration 12, loss = 0.27733406\n",
            "Iteration 13, loss = 0.27450777\n",
            "Iteration 14, loss = 0.27199553\n",
            "Iteration 15, loss = 0.26924109\n",
            "Iteration 16, loss = 0.26827275\n",
            "Iteration 17, loss = 0.26522040\n",
            "Iteration 18, loss = 0.26280066\n",
            "Iteration 19, loss = 0.26063755\n",
            "Iteration 20, loss = 0.25854509\n",
            "Iteration 21, loss = 0.25644182\n",
            "Iteration 22, loss = 0.25442255\n",
            "Iteration 23, loss = 0.25146811\n",
            "Iteration 24, loss = 0.24916103\n",
            "Iteration 25, loss = 0.24794584\n",
            "Iteration 26, loss = 0.24592265\n",
            "Iteration 27, loss = 0.24419689\n",
            "Iteration 28, loss = 0.24094884\n",
            "Iteration 29, loss = 0.24039970\n",
            "Iteration 30, loss = 0.23901858\n",
            "Iteration 31, loss = 0.23547158\n",
            "Iteration 32, loss = 0.23335824\n",
            "Iteration 33, loss = 0.23203109\n",
            "Iteration 34, loss = 0.23115861\n",
            "Iteration 35, loss = 0.22803333\n",
            "Iteration 36, loss = 0.22722659\n",
            "Iteration 37, loss = 0.22604990\n",
            "Iteration 38, loss = 0.22301198\n",
            "Iteration 39, loss = 0.22171956\n",
            "Iteration 40, loss = 0.21970615\n",
            "Iteration 41, loss = 0.21875277\n",
            "Iteration 42, loss = 0.21674314\n",
            "Iteration 43, loss = 0.21603204\n",
            "Iteration 44, loss = 0.21365619\n",
            "Iteration 45, loss = 0.21163654\n",
            "Iteration 46, loss = 0.20970590\n",
            "Iteration 47, loss = 0.20928151\n",
            "Iteration 48, loss = 0.20812129\n",
            "Iteration 49, loss = 0.20690002\n",
            "Iteration 50, loss = 0.20596638\n",
            "Iteration 51, loss = 0.20353670\n",
            "Iteration 52, loss = 0.20144236\n",
            "Iteration 53, loss = 0.20173414\n",
            "Iteration 54, loss = 0.19876529\n",
            "Iteration 55, loss = 0.19867534\n",
            "Iteration 56, loss = 0.19887454\n",
            "Iteration 57, loss = 0.19554098\n",
            "Iteration 58, loss = 0.19518564\n",
            "Iteration 59, loss = 0.19232259\n",
            "Iteration 60, loss = 0.19372766\n",
            "Iteration 61, loss = 0.19124834\n",
            "Iteration 62, loss = 0.19001331\n",
            "Iteration 63, loss = 0.18913575\n",
            "Iteration 64, loss = 0.18884508\n",
            "Iteration 65, loss = 0.18716637\n",
            "Iteration 66, loss = 0.18537943\n",
            "Iteration 67, loss = 0.18396254\n",
            "Iteration 68, loss = 0.18411302\n",
            "Iteration 69, loss = 0.18158294\n",
            "Iteration 70, loss = 0.18144885\n",
            "Iteration 71, loss = 0.18037758\n",
            "Iteration 72, loss = 0.17996514\n",
            "Iteration 73, loss = 0.17864834\n",
            "Iteration 74, loss = 0.17749492\n",
            "Iteration 75, loss = 0.17651638\n",
            "Iteration 76, loss = 0.17625656\n",
            "Iteration 77, loss = 0.17462963\n",
            "Iteration 78, loss = 0.17583376\n",
            "Iteration 79, loss = 0.17352165\n",
            "Iteration 80, loss = 0.17283130\n",
            "Iteration 81, loss = 0.17447631\n",
            "Iteration 82, loss = 0.17262570\n",
            "Iteration 83, loss = 0.17224999\n",
            "Iteration 84, loss = 0.16969423\n",
            "Iteration 85, loss = 0.16833409\n",
            "Iteration 86, loss = 0.16734947\n",
            "Iteration 87, loss = 0.16658060\n",
            "Iteration 88, loss = 0.16652818\n",
            "Iteration 89, loss = 0.16490687\n",
            "Iteration 90, loss = 0.16420890\n",
            "Iteration 91, loss = 0.16431298\n",
            "Iteration 92, loss = 0.16291443\n",
            "Iteration 93, loss = 0.16196823\n",
            "Iteration 94, loss = 0.16294732\n",
            "Iteration 95, loss = 0.16157958\n",
            "Iteration 96, loss = 0.16123032\n",
            "Iteration 97, loss = 0.15991347\n",
            "Iteration 98, loss = 0.16007075\n",
            "Iteration 99, loss = 0.15738208\n",
            "Iteration 100, loss = 0.15820007\n",
            "Iteration 101, loss = 0.15764705\n",
            "Iteration 102, loss = 0.15618624\n",
            "Iteration 103, loss = 0.15578902\n",
            "Iteration 104, loss = 0.15464776\n",
            "Iteration 105, loss = 0.15415789\n",
            "Iteration 106, loss = 0.15502959\n",
            "Iteration 107, loss = 0.15259632\n",
            "Iteration 108, loss = 0.15192696\n",
            "Iteration 109, loss = 0.15241041\n",
            "Iteration 110, loss = 0.15478970\n",
            "Iteration 111, loss = 0.15161420\n",
            "Iteration 112, loss = 0.15033447\n",
            "Iteration 113, loss = 0.15051799\n",
            "Iteration 114, loss = 0.15122028\n",
            "Iteration 115, loss = 0.14786599\n",
            "Iteration 116, loss = 0.14757004\n",
            "Iteration 117, loss = 0.14693560\n",
            "Iteration 118, loss = 0.14627358\n",
            "Iteration 119, loss = 0.14770719\n",
            "Iteration 120, loss = 0.14750737\n",
            "Iteration 121, loss = 0.14555303\n",
            "Iteration 122, loss = 0.14551650\n",
            "Iteration 123, loss = inf\n",
            "Iteration 124, loss = 0.14403107\n",
            "Iteration 125, loss = 0.14454528\n",
            "Iteration 126, loss = 0.14212092\n",
            "Iteration 127, loss = 0.14256186\n",
            "Iteration 128, loss = 0.14139045\n",
            "Iteration 129, loss = 0.14252396\n",
            "Iteration 130, loss = 0.14171109\n",
            "Iteration 131, loss = 0.13996784\n",
            "Iteration 132, loss = 0.14023432\n",
            "Iteration 133, loss = 0.14094789\n",
            "Iteration 134, loss = 0.14037620\n",
            "Iteration 135, loss = 0.14040382\n",
            "Iteration 136, loss = 0.13645859\n",
            "Iteration 137, loss = 0.13789638\n",
            "Iteration 138, loss = 0.13879075\n",
            "Iteration 139, loss = 0.13603552\n",
            "Iteration 140, loss = 0.13806670\n",
            "Iteration 141, loss = 0.13660958\n",
            "Iteration 142, loss = 0.13707627\n",
            "Iteration 143, loss = 0.13621062\n",
            "Iteration 144, loss = 0.13679667\n",
            "Iteration 145, loss = 0.13559010\n",
            "Iteration 146, loss = 0.13649300\n",
            "Iteration 147, loss = 0.13393011\n",
            "Iteration 148, loss = 0.13412289\n",
            "Iteration 149, loss = 0.13454289\n",
            "Iteration 150, loss = 0.13412265\n",
            "Iteration 151, loss = 0.13234230\n",
            "Iteration 152, loss = 0.13326897\n",
            "Iteration 153, loss = 0.13344818\n",
            "Iteration 154, loss = 0.13055491\n",
            "Iteration 155, loss = 0.13277480\n",
            "Iteration 156, loss = 0.13124968\n",
            "Iteration 157, loss = 0.12985084\n",
            "Iteration 158, loss = 0.13139559\n",
            "Iteration 159, loss = 0.13229502\n",
            "Iteration 160, loss = 0.13042928\n",
            "Iteration 161, loss = 0.12899121\n",
            "Iteration 162, loss = 0.12873573\n",
            "Iteration 163, loss = 0.12829331\n",
            "Iteration 164, loss = 0.12741921\n",
            "Iteration 165, loss = 0.12662199\n",
            "Iteration 166, loss = 0.12761632\n",
            "Iteration 167, loss = 0.12677351\n",
            "Iteration 168, loss = 0.12959518\n",
            "Iteration 169, loss = 0.12560454\n",
            "Iteration 170, loss = 0.12590888\n",
            "Iteration 171, loss = 0.12732116\n",
            "Iteration 172, loss = 0.12764198\n",
            "Iteration 173, loss = 0.12657511\n",
            "Iteration 174, loss = 0.12678665\n",
            "Iteration 175, loss = 0.12674634\n",
            "Iteration 176, loss = 0.12314866\n",
            "Iteration 177, loss = 0.12153387\n",
            "Iteration 178, loss = 0.12492790\n",
            "Iteration 179, loss = 0.12504402\n",
            "Iteration 180, loss = 0.12463802\n",
            "Iteration 181, loss = 0.12432923\n",
            "Iteration 182, loss = 0.12252470\n",
            "Iteration 183, loss = 0.12201921\n",
            "Iteration 184, loss = 0.12370704\n",
            "Iteration 185, loss = 0.12120581\n",
            "Iteration 186, loss = 0.12414433\n",
            "Iteration 187, loss = 0.12050319\n",
            "Iteration 188, loss = 0.12072279\n",
            "Iteration 189, loss = 0.12047037\n",
            "Iteration 190, loss = 0.12153513\n",
            "Iteration 191, loss = 0.12068099\n",
            "Iteration 192, loss = 0.11927522\n",
            "Iteration 193, loss = 0.11907399\n",
            "Iteration 194, loss = 0.11779535\n",
            "Iteration 195, loss = 0.12062872\n",
            "Iteration 196, loss = 0.12010721\n",
            "Iteration 197, loss = 0.12189290\n",
            "Iteration 198, loss = 0.11768642\n",
            "Iteration 199, loss = 0.11952518\n",
            "Iteration 200, loss = 0.12018068\n",
            "Iteration 201, loss = 0.11694813\n",
            "Iteration 202, loss = 0.11742549\n",
            "Iteration 203, loss = 0.11793065\n",
            "Iteration 204, loss = 0.11789454\n",
            "Iteration 205, loss = 0.12083124\n",
            "Iteration 206, loss = 0.11770836\n",
            "Iteration 207, loss = 0.11650567\n",
            "Iteration 208, loss = 0.11720712\n",
            "Iteration 209, loss = 0.11873168\n",
            "Iteration 210, loss = 0.11436468\n",
            "Iteration 211, loss = 0.11543000\n",
            "Iteration 212, loss = 0.11312245\n",
            "Iteration 213, loss = 0.11564219\n",
            "Iteration 214, loss = 0.11371370\n",
            "Iteration 215, loss = 0.11714216\n",
            "Iteration 216, loss = 0.11520495\n",
            "Iteration 217, loss = 0.11425149\n",
            "Iteration 218, loss = 0.11297880\n",
            "Iteration 219, loss = 0.11337390\n",
            "Iteration 220, loss = 0.11418395\n",
            "Iteration 221, loss = 0.11163607\n",
            "Iteration 222, loss = 0.11350868\n",
            "Iteration 223, loss = 0.11265471\n",
            "Iteration 224, loss = 0.11207120\n",
            "Iteration 225, loss = 0.11226284\n",
            "Iteration 226, loss = 0.11107171\n",
            "Iteration 227, loss = 0.11150778\n",
            "Iteration 228, loss = 0.11089808\n",
            "Iteration 229, loss = 0.11488130\n",
            "Iteration 230, loss = 0.11366191\n",
            "Iteration 231, loss = 0.10991070\n",
            "Iteration 232, loss = 0.10855443\n",
            "Iteration 233, loss = 0.11265547\n",
            "Iteration 234, loss = 0.11166426\n",
            "Iteration 235, loss = 0.11276311\n",
            "Iteration 236, loss = 0.10970034\n",
            "Iteration 237, loss = 0.10995193\n",
            "Iteration 238, loss = 0.11052605\n",
            "Iteration 239, loss = 0.11170297\n",
            "Iteration 240, loss = 0.11136076\n",
            "Iteration 241, loss = 0.10795142\n",
            "Iteration 242, loss = 0.10595728\n",
            "Iteration 243, loss = 0.10836444\n",
            "Iteration 244, loss = 0.10834711\n",
            "Iteration 245, loss = 0.11043607\n",
            "Iteration 246, loss = 0.10917905\n",
            "Iteration 247, loss = 0.10913762\n",
            "Iteration 248, loss = 0.10829817\n",
            "Iteration 249, loss = 0.11050596\n",
            "Iteration 250, loss = 0.10892702\n",
            "Iteration 251, loss = 0.10931688\n",
            "Iteration 252, loss = 0.10818524\n",
            "Iteration 253, loss = 0.10728901\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.39272118\n",
            "Iteration 2, loss = 0.32706298\n",
            "Iteration 3, loss = 0.31458326\n",
            "Iteration 4, loss = 0.30844982\n",
            "Iteration 5, loss = 0.30223984\n",
            "Iteration 6, loss = 0.29822807\n",
            "Iteration 7, loss = 0.29476177\n",
            "Iteration 8, loss = 0.29174937\n",
            "Iteration 9, loss = 0.28794494\n",
            "Iteration 10, loss = 0.28521535\n",
            "Iteration 11, loss = 0.28292495\n",
            "Iteration 12, loss = 0.28070369\n",
            "Iteration 13, loss = 0.27897604\n",
            "Iteration 14, loss = 0.27591355\n",
            "Iteration 15, loss = 0.27394809\n",
            "Iteration 16, loss = 0.27092981\n",
            "Iteration 17, loss = 0.26886589\n",
            "Iteration 18, loss = 0.26638239\n",
            "Iteration 19, loss = 0.26351108\n",
            "Iteration 20, loss = 0.26069340\n",
            "Iteration 21, loss = 0.25926633\n",
            "Iteration 22, loss = 0.25669662\n",
            "Iteration 23, loss = 0.25312790\n",
            "Iteration 24, loss = 0.25201022\n",
            "Iteration 25, loss = 0.24908902\n",
            "Iteration 26, loss = 0.24623540\n",
            "Iteration 27, loss = 0.24380494\n",
            "Iteration 28, loss = 0.24229251\n",
            "Iteration 29, loss = 0.23920491\n",
            "Iteration 30, loss = 0.23675055\n",
            "Iteration 31, loss = 0.23449737\n",
            "Iteration 32, loss = 0.23334447\n",
            "Iteration 33, loss = 0.23100634\n",
            "Iteration 34, loss = 0.22864994\n",
            "Iteration 35, loss = 0.22720970\n",
            "Iteration 36, loss = 0.22609205\n",
            "Iteration 37, loss = 0.22454019\n",
            "Iteration 38, loss = 0.22202108\n",
            "Iteration 39, loss = 0.22053830\n",
            "Iteration 40, loss = 0.21884376\n",
            "Iteration 41, loss = 0.21626204\n",
            "Iteration 42, loss = 0.21450592\n",
            "Iteration 43, loss = 0.21306172\n",
            "Iteration 44, loss = 0.21091815\n",
            "Iteration 45, loss = 0.20913559\n",
            "Iteration 46, loss = 0.20727539\n",
            "Iteration 47, loss = 0.20587055\n",
            "Iteration 48, loss = 0.20606002\n",
            "Iteration 49, loss = 0.20294220\n",
            "Iteration 50, loss = 0.20139360\n",
            "Iteration 51, loss = 0.20088222\n",
            "Iteration 52, loss = 0.19867616\n",
            "Iteration 53, loss = 0.19815622\n",
            "Iteration 54, loss = 0.19677382\n",
            "Iteration 55, loss = 0.19543370\n",
            "Iteration 56, loss = 0.19367847\n",
            "Iteration 57, loss = 0.19234115\n",
            "Iteration 58, loss = 0.19189693\n",
            "Iteration 59, loss = 0.19024036\n",
            "Iteration 60, loss = 0.18935484\n",
            "Iteration 61, loss = 0.18722997\n",
            "Iteration 62, loss = 0.18690531\n",
            "Iteration 63, loss = 0.18473049\n",
            "Iteration 64, loss = 0.18392309\n",
            "Iteration 65, loss = 0.18425991\n",
            "Iteration 66, loss = 0.18164597\n",
            "Iteration 67, loss = 0.18020598\n",
            "Iteration 68, loss = 0.18369412\n",
            "Iteration 69, loss = 0.17928863\n",
            "Iteration 70, loss = 0.17730160\n",
            "Iteration 71, loss = 0.17711768\n",
            "Iteration 72, loss = 0.17600891\n",
            "Iteration 73, loss = 0.17691059\n",
            "Iteration 74, loss = 0.17488221\n",
            "Iteration 75, loss = 0.17314090\n",
            "Iteration 76, loss = 0.17252556\n",
            "Iteration 77, loss = 0.17062291\n",
            "Iteration 78, loss = 0.17093166\n",
            "Iteration 79, loss = 0.17032325\n",
            "Iteration 80, loss = 0.16957890\n",
            "Iteration 81, loss = 0.16917647\n",
            "Iteration 82, loss = 0.16784205\n",
            "Iteration 83, loss = 0.16696350\n",
            "Iteration 84, loss = 0.16691179\n",
            "Iteration 85, loss = 0.16451238\n",
            "Iteration 86, loss = 0.16294986\n",
            "Iteration 87, loss = 0.16457505\n",
            "Iteration 88, loss = 0.16427655\n",
            "Iteration 89, loss = 0.16429745\n",
            "Iteration 90, loss = 0.16166095\n",
            "Iteration 91, loss = 0.15968350\n",
            "Iteration 92, loss = 0.15865245\n",
            "Iteration 93, loss = 0.15816312\n",
            "Iteration 94, loss = 0.15802276\n",
            "Iteration 95, loss = 0.15761296\n",
            "Iteration 96, loss = 0.16065279\n",
            "Iteration 97, loss = 0.15622265\n",
            "Iteration 98, loss = 0.15679406\n",
            "Iteration 99, loss = 0.15582168\n",
            "Iteration 100, loss = 0.15317829\n",
            "Iteration 101, loss = 0.15487027\n",
            "Iteration 102, loss = 0.15371534\n",
            "Iteration 103, loss = 0.15172613\n",
            "Iteration 104, loss = 0.15209802\n",
            "Iteration 105, loss = 0.15186835\n",
            "Iteration 106, loss = 0.15077950\n",
            "Iteration 107, loss = 0.15151509\n",
            "Iteration 108, loss = 0.15110679\n",
            "Iteration 109, loss = 0.14908292\n",
            "Iteration 110, loss = 0.14815585\n",
            "Iteration 111, loss = 0.14614637\n",
            "Iteration 112, loss = 0.14696174\n",
            "Iteration 113, loss = 0.14613253\n",
            "Iteration 114, loss = 0.14563131\n",
            "Iteration 115, loss = 0.14719068\n",
            "Iteration 116, loss = 0.14567237\n",
            "Iteration 117, loss = 0.14610676\n",
            "Iteration 118, loss = 0.14573512\n",
            "Iteration 119, loss = 0.14583813\n",
            "Iteration 120, loss = 0.14445805\n",
            "Iteration 121, loss = 0.14366864\n",
            "Iteration 122, loss = 0.14280516\n",
            "Iteration 123, loss = 0.14386397\n",
            "Iteration 124, loss = 0.14308610\n",
            "Iteration 125, loss = 0.14156801\n",
            "Iteration 126, loss = 0.14301066\n",
            "Iteration 127, loss = 0.14000179\n",
            "Iteration 128, loss = 0.14024580\n",
            "Iteration 129, loss = 0.14091054\n",
            "Iteration 130, loss = 0.13786560\n",
            "Iteration 131, loss = 0.13795432\n",
            "Iteration 132, loss = 0.14001175\n",
            "Iteration 133, loss = 0.13854387\n",
            "Iteration 134, loss = 0.13689200\n",
            "Iteration 135, loss = 0.13783518\n",
            "Iteration 136, loss = 0.13891726\n",
            "Iteration 137, loss = 0.13755810\n",
            "Iteration 138, loss = 0.13776393\n",
            "Iteration 139, loss = 0.13642798\n",
            "Iteration 140, loss = 0.13571687\n",
            "Iteration 141, loss = 0.13553087\n",
            "Iteration 142, loss = 0.13380499\n",
            "Iteration 143, loss = 0.13556683\n",
            "Iteration 144, loss = 0.13249160\n",
            "Iteration 145, loss = 0.13258114\n",
            "Iteration 146, loss = 0.13435724\n",
            "Iteration 147, loss = 0.13402366\n",
            "Iteration 148, loss = 0.13167562\n",
            "Iteration 149, loss = 0.13297484\n",
            "Iteration 150, loss = 0.13330641\n",
            "Iteration 151, loss = 0.13320728\n",
            "Iteration 152, loss = 0.13203316\n",
            "Iteration 153, loss = 0.13262927\n",
            "Iteration 154, loss = 0.12993949\n",
            "Iteration 155, loss = 0.12990022\n",
            "Iteration 156, loss = 0.13014580\n",
            "Iteration 157, loss = 0.12920240\n",
            "Iteration 158, loss = 0.12979923\n",
            "Iteration 159, loss = 0.12934211\n",
            "Iteration 160, loss = 0.12981989\n",
            "Iteration 161, loss = 0.12743814\n",
            "Iteration 162, loss = 0.12874388\n",
            "Iteration 163, loss = 0.12848818\n",
            "Iteration 164, loss = 0.12613219\n",
            "Iteration 165, loss = 0.12911755\n",
            "Iteration 166, loss = 0.12743430\n",
            "Iteration 167, loss = 0.12654295\n",
            "Iteration 168, loss = 0.12623691\n",
            "Iteration 169, loss = 0.12674883\n",
            "Iteration 170, loss = 0.12770134\n",
            "Iteration 171, loss = 0.12669509\n",
            "Iteration 172, loss = 0.12754491\n",
            "Iteration 173, loss = 0.12490610\n",
            "Iteration 174, loss = 0.12370449\n",
            "Iteration 175, loss = 0.12299502\n",
            "Iteration 176, loss = 0.12349580\n",
            "Iteration 177, loss = 0.12398940\n",
            "Iteration 178, loss = 0.12182903\n",
            "Iteration 179, loss = 0.12344786\n",
            "Iteration 180, loss = 0.12430804\n",
            "Iteration 181, loss = 0.12448726\n",
            "Iteration 182, loss = 0.12305185\n",
            "Iteration 183, loss = 0.12406675\n",
            "Iteration 184, loss = 0.12263281\n",
            "Iteration 185, loss = 0.12329996\n",
            "Iteration 186, loss = 0.12057049\n",
            "Iteration 187, loss = 0.12091863\n",
            "Iteration 188, loss = 0.12023456\n",
            "Iteration 189, loss = 0.11937316\n",
            "Iteration 190, loss = 0.11852065\n",
            "Iteration 191, loss = 0.11986023\n",
            "Iteration 192, loss = 0.12104086\n",
            "Iteration 193, loss = 0.11853098\n",
            "Iteration 194, loss = 0.11961562\n",
            "Iteration 195, loss = 0.11883735\n",
            "Iteration 196, loss = 0.11952924\n",
            "Iteration 197, loss = 0.11831582\n",
            "Iteration 198, loss = 0.11802713\n",
            "Iteration 199, loss = 0.11894066\n",
            "Iteration 200, loss = 0.11801615\n",
            "Iteration 201, loss = 0.11894291\n",
            "Iteration 202, loss = 0.11719915\n",
            "Iteration 203, loss = 0.11938425\n",
            "Iteration 204, loss = 0.11656089\n",
            "Iteration 205, loss = 0.11674690\n",
            "Iteration 206, loss = 0.11414759\n",
            "Iteration 207, loss = 0.11921511\n",
            "Iteration 208, loss = 0.11557110\n",
            "Iteration 209, loss = 0.11780532\n",
            "Iteration 210, loss = 0.11735981\n",
            "Iteration 211, loss = 0.11552396\n",
            "Iteration 212, loss = 0.11680558\n",
            "Iteration 213, loss = 0.11487977\n",
            "Iteration 214, loss = 0.11393343\n",
            "Iteration 215, loss = 0.11441337\n",
            "Iteration 216, loss = 0.11381966\n",
            "Iteration 217, loss = 0.11482987\n",
            "Iteration 218, loss = 0.11375598\n",
            "Iteration 219, loss = 0.11349010\n",
            "Iteration 220, loss = 0.11228413\n",
            "Iteration 221, loss = 0.11609960\n",
            "Iteration 222, loss = 0.11363903\n",
            "Iteration 223, loss = 0.11306236\n",
            "Iteration 224, loss = 0.11139612\n",
            "Iteration 225, loss = 0.11178339\n",
            "Iteration 226, loss = 0.11216900\n",
            "Iteration 227, loss = 0.11153756\n",
            "Iteration 228, loss = 0.11149711\n",
            "Iteration 229, loss = 0.11246739\n",
            "Iteration 230, loss = 0.11190750\n",
            "Iteration 231, loss = 0.11210778\n",
            "Iteration 232, loss = 0.11231504\n",
            "Iteration 233, loss = 0.11065903\n",
            "Iteration 234, loss = 0.11109162\n",
            "Iteration 235, loss = 0.11202523\n",
            "Iteration 236, loss = 0.11033920\n",
            "Iteration 237, loss = 0.11013165\n",
            "Iteration 238, loss = 0.10912758\n",
            "Iteration 239, loss = 0.11008190\n",
            "Iteration 240, loss = 0.10913073\n",
            "Iteration 241, loss = 0.10931039\n",
            "Iteration 242, loss = 0.11219789\n",
            "Iteration 243, loss = 0.10964664\n",
            "Iteration 244, loss = 0.10789950\n",
            "Iteration 245, loss = 0.11051674\n",
            "Iteration 246, loss = 0.10891146\n",
            "Iteration 247, loss = 0.11002831\n",
            "Iteration 248, loss = 0.10818691\n",
            "Iteration 249, loss = 0.10664179\n",
            "Iteration 250, loss = 0.10837752\n",
            "Iteration 251, loss = 0.10722128\n",
            "Iteration 252, loss = 0.10695510\n",
            "Iteration 253, loss = 0.10909242\n",
            "Iteration 254, loss = 0.10767583\n",
            "Iteration 255, loss = 0.10708693\n",
            "Iteration 256, loss = 0.10768073\n",
            "Iteration 257, loss = 0.10878525\n",
            "Iteration 258, loss = 0.10804689\n",
            "Iteration 259, loss = 0.10585815\n",
            "Iteration 260, loss = 0.10780088\n",
            "Iteration 261, loss = 0.10553291\n",
            "Iteration 262, loss = 0.10651948\n",
            "Iteration 263, loss = 0.10668835\n",
            "Iteration 264, loss = 0.10357738\n",
            "Iteration 265, loss = 0.10851034\n",
            "Iteration 266, loss = 0.10916807\n",
            "Iteration 267, loss = 0.10569777\n",
            "Iteration 268, loss = 0.10562813\n",
            "Iteration 269, loss = 0.10391855\n",
            "Iteration 270, loss = 0.10330419\n",
            "Iteration 271, loss = 0.10344836\n",
            "Iteration 272, loss = 0.10605112\n",
            "Iteration 273, loss = 0.10596095\n",
            "Iteration 274, loss = 0.10838068\n",
            "Iteration 275, loss = 0.10809382\n",
            "Iteration 276, loss = 0.10423586\n",
            "Iteration 277, loss = 0.10389211\n",
            "Iteration 278, loss = 0.10357306\n",
            "Iteration 279, loss = 0.10415657\n",
            "Iteration 280, loss = 0.10439103\n",
            "Iteration 281, loss = 0.10256665\n",
            "Iteration 282, loss = 0.10215286\n",
            "Iteration 283, loss = 0.10716977\n",
            "Iteration 284, loss = 0.10330005\n",
            "Iteration 285, loss = 0.10223835\n",
            "Iteration 286, loss = 0.10227282\n",
            "Iteration 287, loss = 0.10015261\n",
            "Iteration 288, loss = 0.10192858\n",
            "Iteration 289, loss = 0.10390512\n",
            "Iteration 290, loss = 0.10199374\n",
            "Iteration 291, loss = 0.10149996\n",
            "Iteration 292, loss = 0.09998417\n",
            "Iteration 293, loss = 0.10126303\n",
            "Iteration 294, loss = 0.10335712\n",
            "Iteration 295, loss = 0.10315495\n",
            "Iteration 296, loss = 0.10022421\n",
            "Iteration 297, loss = 0.09882292\n",
            "Iteration 298, loss = 0.09944251\n",
            "Iteration 299, loss = 0.09933015\n",
            "Iteration 300, loss = 0.10157899\n",
            "Iteration 301, loss = 0.09976164\n",
            "Iteration 302, loss = 0.10007160\n",
            "Iteration 303, loss = 0.09740555\n",
            "Iteration 304, loss = 0.10185204\n",
            "Iteration 305, loss = 0.10124266\n",
            "Iteration 306, loss = 0.09856793\n",
            "Iteration 307, loss = inf\n",
            "Iteration 308, loss = 0.09944623\n",
            "Iteration 309, loss = 0.10230105\n",
            "Iteration 310, loss = 0.09795122\n",
            "Iteration 311, loss = 0.09918962\n",
            "Iteration 312, loss = 0.09939473\n",
            "Iteration 313, loss = 0.09791938\n",
            "Iteration 314, loss = 0.09966056\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.36743973\n",
            "Iteration 2, loss = 0.32210656\n",
            "Iteration 3, loss = 0.31121820\n",
            "Iteration 4, loss = 0.30465753\n",
            "Iteration 5, loss = 0.29903589\n",
            "Iteration 6, loss = 0.29520066\n",
            "Iteration 7, loss = 0.29192263\n",
            "Iteration 8, loss = 0.28859290\n",
            "Iteration 9, loss = 0.28606805\n",
            "Iteration 10, loss = 0.28491215\n",
            "Iteration 11, loss = 0.28128564\n",
            "Iteration 12, loss = 0.27902217\n",
            "Iteration 13, loss = 0.27698111\n",
            "Iteration 14, loss = 0.27500413\n",
            "Iteration 15, loss = 0.27317407\n",
            "Iteration 16, loss = 0.26897363\n",
            "Iteration 17, loss = 0.26858652\n",
            "Iteration 18, loss = 0.26632228\n",
            "Iteration 19, loss = 0.26429494\n",
            "Iteration 20, loss = 0.26026010\n",
            "Iteration 21, loss = 0.25841064\n",
            "Iteration 22, loss = 0.25648426\n",
            "Iteration 23, loss = 0.25450194\n",
            "Iteration 24, loss = 0.25188001\n",
            "Iteration 25, loss = 0.25097652\n",
            "Iteration 26, loss = 0.24665042\n",
            "Iteration 27, loss = 0.24594113\n",
            "Iteration 28, loss = 0.24430059\n",
            "Iteration 29, loss = 0.24132954\n",
            "Iteration 30, loss = 0.24001018\n",
            "Iteration 31, loss = 0.23812894\n",
            "Iteration 32, loss = 0.23566767\n",
            "Iteration 33, loss = 0.23297049\n",
            "Iteration 34, loss = 0.23277354\n",
            "Iteration 35, loss = 0.22877405\n",
            "Iteration 36, loss = 0.22735728\n",
            "Iteration 37, loss = 0.22690577\n",
            "Iteration 38, loss = 0.22403343\n",
            "Iteration 39, loss = 0.22269466\n",
            "Iteration 40, loss = 0.22192144\n",
            "Iteration 41, loss = 0.21930516\n",
            "Iteration 42, loss = 0.21842671\n",
            "Iteration 43, loss = 0.21592433\n",
            "Iteration 44, loss = 0.21691624\n",
            "Iteration 45, loss = 0.21183741\n",
            "Iteration 46, loss = 0.21110451\n",
            "Iteration 47, loss = 0.20872065\n",
            "Iteration 48, loss = 0.20878462\n",
            "Iteration 49, loss = 0.20746860\n",
            "Iteration 50, loss = 0.20580991\n",
            "Iteration 51, loss = 0.20365711\n",
            "Iteration 52, loss = 0.20371403\n",
            "Iteration 53, loss = 0.20231099\n",
            "Iteration 54, loss = 0.20012161\n",
            "Iteration 55, loss = 0.19858249\n",
            "Iteration 56, loss = 0.19694759\n",
            "Iteration 57, loss = 0.19679225\n",
            "Iteration 58, loss = 0.19479285\n",
            "Iteration 59, loss = 0.19202906\n",
            "Iteration 60, loss = 0.19320239\n",
            "Iteration 61, loss = 0.19252142\n",
            "Iteration 62, loss = 0.18907175\n",
            "Iteration 63, loss = 0.19008917\n",
            "Iteration 64, loss = 0.18783898\n",
            "Iteration 65, loss = 0.18701520\n",
            "Iteration 66, loss = 0.18646149\n",
            "Iteration 67, loss = 0.18439961\n",
            "Iteration 68, loss = 0.18394936\n",
            "Iteration 69, loss = 0.18572144\n",
            "Iteration 70, loss = 0.18105258\n",
            "Iteration 71, loss = 0.18097109\n",
            "Iteration 72, loss = 0.17972805\n",
            "Iteration 73, loss = 0.17881452\n",
            "Iteration 74, loss = 0.17820487\n",
            "Iteration 75, loss = 0.17542728\n",
            "Iteration 76, loss = 0.17697470\n",
            "Iteration 77, loss = 0.17780530\n",
            "Iteration 78, loss = 0.17536178\n",
            "Iteration 79, loss = 0.17520912\n",
            "Iteration 80, loss = 0.17172439\n",
            "Iteration 81, loss = 0.17127520\n",
            "Iteration 82, loss = 0.17200451\n",
            "Iteration 83, loss = 0.17065415\n",
            "Iteration 84, loss = 0.17000039\n",
            "Iteration 85, loss = 0.16895891\n",
            "Iteration 86, loss = 0.16996605\n",
            "Iteration 87, loss = 0.16887393\n",
            "Iteration 88, loss = 0.16592708\n",
            "Iteration 89, loss = 0.16488451\n",
            "Iteration 90, loss = 0.16675510\n",
            "Iteration 91, loss = 0.16486326\n",
            "Iteration 92, loss = 0.16267013\n",
            "Iteration 93, loss = 0.16233069\n",
            "Iteration 94, loss = 0.16149704\n",
            "Iteration 95, loss = 0.16336995\n",
            "Iteration 96, loss = 0.15950242\n",
            "Iteration 97, loss = 0.15983227\n",
            "Iteration 98, loss = 0.15983346\n",
            "Iteration 99, loss = 0.16054909\n",
            "Iteration 100, loss = 0.15757917\n",
            "Iteration 101, loss = 0.15627685\n",
            "Iteration 102, loss = 0.15804309\n",
            "Iteration 103, loss = 0.15559754\n",
            "Iteration 104, loss = 0.15350717\n",
            "Iteration 105, loss = 0.15643209\n",
            "Iteration 106, loss = 0.15540181\n",
            "Iteration 107, loss = 0.15376457\n",
            "Iteration 108, loss = 0.15468648\n",
            "Iteration 109, loss = 0.15095061\n",
            "Iteration 110, loss = 0.15279796\n",
            "Iteration 111, loss = 0.15152457\n",
            "Iteration 112, loss = 0.15385724\n",
            "Iteration 113, loss = 0.15093048\n",
            "Iteration 114, loss = 0.15066185\n",
            "Iteration 115, loss = 0.15047161\n",
            "Iteration 116, loss = 0.15067378\n",
            "Iteration 117, loss = 0.14941799\n",
            "Iteration 118, loss = 0.14710448\n",
            "Iteration 119, loss = 0.14639783\n",
            "Iteration 120, loss = 0.14602415\n",
            "Iteration 121, loss = 0.14655445\n",
            "Iteration 122, loss = 0.14663818\n",
            "Iteration 123, loss = 0.14822869\n",
            "Iteration 124, loss = 0.14434452\n",
            "Iteration 125, loss = 0.14375050\n",
            "Iteration 126, loss = 0.14531724\n",
            "Iteration 127, loss = 0.14300332\n",
            "Iteration 128, loss = 0.14361024\n",
            "Iteration 129, loss = 0.14255890\n",
            "Iteration 130, loss = 0.14129397\n",
            "Iteration 131, loss = 0.14353482\n",
            "Iteration 132, loss = 0.14721539\n",
            "Iteration 133, loss = 0.14231901\n",
            "Iteration 134, loss = 0.14121513\n",
            "Iteration 135, loss = 0.14083954\n",
            "Iteration 136, loss = 0.14116003\n",
            "Iteration 137, loss = 0.14216925\n",
            "Iteration 138, loss = 0.13856493\n",
            "Iteration 139, loss = 0.13867790\n",
            "Iteration 140, loss = 0.13652210\n",
            "Iteration 141, loss = 0.13562283\n",
            "Iteration 142, loss = 0.13561758\n",
            "Iteration 143, loss = 0.13854888\n",
            "Iteration 144, loss = 0.13519911\n",
            "Iteration 145, loss = 0.13637960\n",
            "Iteration 146, loss = 0.13487968\n",
            "Iteration 147, loss = 0.13410600\n",
            "Iteration 148, loss = 0.13510653\n",
            "Iteration 149, loss = 0.13627359\n",
            "Iteration 150, loss = 0.13310517\n",
            "Iteration 151, loss = 0.13174682\n",
            "Iteration 152, loss = 0.13234189\n",
            "Iteration 153, loss = 0.13216897\n",
            "Iteration 154, loss = 0.13496504\n",
            "Iteration 155, loss = 0.13143356\n",
            "Iteration 156, loss = 0.13405862\n",
            "Iteration 157, loss = 0.13023411\n",
            "Iteration 158, loss = 0.13046113\n",
            "Iteration 159, loss = 0.13139069\n",
            "Iteration 160, loss = 0.13022173\n",
            "Iteration 161, loss = 0.12952240\n",
            "Iteration 162, loss = 0.13194968\n",
            "Iteration 163, loss = 0.12795653\n",
            "Iteration 164, loss = 0.12920095\n",
            "Iteration 165, loss = 0.13045630\n",
            "Iteration 166, loss = 0.12803929\n",
            "Iteration 167, loss = 0.12967497\n",
            "Iteration 168, loss = 0.12793402\n",
            "Iteration 169, loss = 0.12716701\n",
            "Iteration 170, loss = 0.12831164\n",
            "Iteration 171, loss = 0.12749654\n",
            "Iteration 172, loss = 0.12906914\n",
            "Iteration 173, loss = 0.12532479\n",
            "Iteration 174, loss = 0.12570443\n",
            "Iteration 175, loss = 0.12531251\n",
            "Iteration 176, loss = 0.12280045\n",
            "Iteration 177, loss = 0.12347522\n",
            "Iteration 178, loss = 0.12558618\n",
            "Iteration 179, loss = 0.12552083\n",
            "Iteration 180, loss = 0.12325062\n",
            "Iteration 181, loss = 0.12433889\n",
            "Iteration 182, loss = 0.12540351\n",
            "Iteration 183, loss = 0.12179702\n",
            "Iteration 184, loss = 0.12418422\n",
            "Iteration 185, loss = 0.12431525\n",
            "Iteration 186, loss = 0.12420181\n",
            "Iteration 187, loss = 0.12073724\n",
            "Iteration 188, loss = 0.12203311\n",
            "Iteration 189, loss = 0.12060660\n",
            "Iteration 190, loss = 0.12134286\n",
            "Iteration 191, loss = 0.11878586\n",
            "Iteration 192, loss = 0.12110107\n",
            "Iteration 193, loss = 0.12120017\n",
            "Iteration 194, loss = 0.11888989\n",
            "Iteration 195, loss = 0.12068589\n",
            "Iteration 196, loss = 0.11984691\n",
            "Iteration 197, loss = 0.11766960\n",
            "Iteration 198, loss = 0.11877280\n",
            "Iteration 199, loss = 0.11818101\n",
            "Iteration 200, loss = 0.11981367\n",
            "Iteration 201, loss = 0.12115051\n",
            "Iteration 202, loss = 0.12104567\n",
            "Iteration 203, loss = 0.12072678\n",
            "Iteration 204, loss = 0.11715637\n",
            "Iteration 205, loss = 0.11795096\n",
            "Iteration 206, loss = 0.11716906\n",
            "Iteration 207, loss = 0.11651077\n",
            "Iteration 208, loss = 0.11700919\n",
            "Iteration 209, loss = 0.11872537\n",
            "Iteration 210, loss = 0.11486076\n",
            "Iteration 211, loss = 0.11455334\n",
            "Iteration 212, loss = 0.11852958\n",
            "Iteration 213, loss = 0.11460926\n",
            "Iteration 214, loss = 0.11419752\n",
            "Iteration 215, loss = 0.11329321\n",
            "Iteration 216, loss = 0.11430247\n",
            "Iteration 217, loss = 0.11358722\n",
            "Iteration 218, loss = 0.11451183\n",
            "Iteration 219, loss = 0.11296685\n",
            "Iteration 220, loss = 0.11401402\n",
            "Iteration 221, loss = 0.11382085\n",
            "Iteration 222, loss = 0.11481282\n",
            "Iteration 223, loss = 0.11210193\n",
            "Iteration 224, loss = 0.11216578\n",
            "Iteration 225, loss = 0.11162402\n",
            "Iteration 226, loss = 0.11315406\n",
            "Iteration 227, loss = 0.11254913\n",
            "Iteration 228, loss = 0.11246901\n",
            "Iteration 229, loss = 0.11120935\n",
            "Iteration 230, loss = 0.11174510\n",
            "Iteration 231, loss = 0.11421128\n",
            "Iteration 232, loss = 0.11295725\n",
            "Iteration 233, loss = 0.11135678\n",
            "Iteration 234, loss = 0.11154270\n",
            "Iteration 235, loss = 0.11009938\n",
            "Iteration 236, loss = 0.10952011\n",
            "Iteration 237, loss = 0.11030385\n",
            "Iteration 238, loss = 0.11025700\n",
            "Iteration 239, loss = 0.10851166\n",
            "Iteration 240, loss = 0.11157775\n",
            "Iteration 241, loss = 0.11020749\n",
            "Iteration 242, loss = 0.10824357\n",
            "Iteration 243, loss = 0.11038266\n",
            "Iteration 244, loss = 0.10820241\n",
            "Iteration 245, loss = 0.10854035\n",
            "Iteration 246, loss = 0.10832137\n",
            "Iteration 247, loss = 0.11012993\n",
            "Iteration 248, loss = 0.10767335\n",
            "Iteration 249, loss = 0.10906314\n",
            "Iteration 250, loss = 0.10617312\n",
            "Iteration 251, loss = 0.10780230\n",
            "Iteration 252, loss = 0.10740039\n",
            "Iteration 253, loss = 0.10663190\n",
            "Iteration 254, loss = 0.10467022\n",
            "Iteration 255, loss = 0.10659468\n",
            "Iteration 256, loss = 0.10533141\n",
            "Iteration 257, loss = 0.10891644\n",
            "Iteration 258, loss = 0.10626176\n",
            "Iteration 259, loss = 0.10626585\n",
            "Iteration 260, loss = 0.10762922\n",
            "Iteration 261, loss = 0.10469002\n",
            "Iteration 262, loss = 0.10451801\n",
            "Iteration 263, loss = 0.10700088\n",
            "Iteration 264, loss = 0.10392509\n",
            "Iteration 265, loss = 0.10475554\n",
            "Iteration 266, loss = 0.10334012\n",
            "Iteration 267, loss = 0.10471474\n",
            "Iteration 268, loss = 0.10537330\n",
            "Iteration 269, loss = 0.10259930\n",
            "Iteration 270, loss = 0.10415461\n",
            "Iteration 271, loss = 0.10575532\n",
            "Iteration 272, loss = 0.10238597\n",
            "Iteration 273, loss = 0.10439000\n",
            "Iteration 274, loss = 0.10605556\n",
            "Iteration 275, loss = 0.10518202\n",
            "Iteration 276, loss = 0.10342708\n",
            "Iteration 277, loss = 0.10344046\n",
            "Iteration 278, loss = 0.10271512\n",
            "Iteration 279, loss = 0.10450415\n",
            "Iteration 280, loss = 0.10416558\n",
            "Iteration 281, loss = 0.10343737\n",
            "Iteration 282, loss = 0.10181414\n",
            "Iteration 283, loss = 0.10192562\n",
            "Iteration 284, loss = 0.10002436\n",
            "Iteration 285, loss = 0.10068075\n",
            "Iteration 286, loss = 0.10714944\n",
            "Iteration 287, loss = 0.10863657\n",
            "Iteration 288, loss = 0.10430497\n",
            "Iteration 289, loss = 0.10375457\n",
            "Iteration 290, loss = 0.10343362\n",
            "Iteration 291, loss = 0.10080137\n",
            "Iteration 292, loss = 0.10385394\n",
            "Iteration 293, loss = 0.10110642\n",
            "Iteration 294, loss = 0.09922097\n",
            "Iteration 295, loss = 0.09865994\n",
            "Iteration 296, loss = 0.09913720\n",
            "Iteration 297, loss = 0.09885063\n",
            "Iteration 298, loss = 0.09694346\n",
            "Iteration 299, loss = 0.09893271\n",
            "Iteration 300, loss = 0.09824891\n",
            "Iteration 301, loss = 0.10103576\n",
            "Iteration 302, loss = 0.09757293\n",
            "Iteration 303, loss = 0.09957610\n",
            "Iteration 304, loss = 0.09926875\n",
            "Iteration 305, loss = 0.09787559\n",
            "Iteration 306, loss = 0.09861490\n",
            "Iteration 307, loss = 0.09956483\n",
            "Iteration 308, loss = 0.09895168\n",
            "Iteration 309, loss = 0.09791254\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.38452326\n",
            "Iteration 2, loss = 0.32201333\n",
            "Iteration 3, loss = 0.31134710\n",
            "Iteration 4, loss = 0.30441552\n",
            "Iteration 5, loss = 0.29935094\n",
            "Iteration 6, loss = 0.29562842\n",
            "Iteration 7, loss = 0.29193888\n",
            "Iteration 8, loss = 0.28977365\n",
            "Iteration 9, loss = 0.28722739\n",
            "Iteration 10, loss = 0.28359761\n",
            "Iteration 11, loss = 0.28194493\n",
            "Iteration 12, loss = 0.27777441\n",
            "Iteration 13, loss = 0.27616310\n",
            "Iteration 14, loss = 0.27305727\n",
            "Iteration 15, loss = 0.27272781\n",
            "Iteration 16, loss = 0.26917065\n",
            "Iteration 17, loss = 0.26648968\n",
            "Iteration 18, loss = 0.26412665\n",
            "Iteration 19, loss = 0.26145497\n",
            "Iteration 20, loss = 0.25865537\n",
            "Iteration 21, loss = 0.25594531\n",
            "Iteration 22, loss = 0.25510130\n",
            "Iteration 23, loss = 0.25269125\n",
            "Iteration 24, loss = 0.24966004\n",
            "Iteration 25, loss = 0.24761770\n",
            "Iteration 26, loss = 0.24563582\n",
            "Iteration 27, loss = 0.24344824\n",
            "Iteration 28, loss = 0.24027156\n",
            "Iteration 29, loss = 0.23826613\n",
            "Iteration 30, loss = 0.23507546\n",
            "Iteration 31, loss = 0.23247620\n",
            "Iteration 32, loss = 0.23206786\n",
            "Iteration 33, loss = 0.23018881\n",
            "Iteration 34, loss = 0.22838463\n",
            "Iteration 35, loss = 0.22716588\n",
            "Iteration 36, loss = 0.22278806\n",
            "Iteration 37, loss = 0.22352463\n",
            "Iteration 38, loss = 0.22002384\n",
            "Iteration 39, loss = 0.21671398\n",
            "Iteration 40, loss = 0.21473185\n",
            "Iteration 41, loss = 0.21339437\n",
            "Iteration 42, loss = 0.21278176\n",
            "Iteration 43, loss = 0.20992013\n",
            "Iteration 44, loss = 0.20920256\n",
            "Iteration 45, loss = 0.20778992\n",
            "Iteration 46, loss = 0.20532952\n",
            "Iteration 47, loss = 0.20312064\n",
            "Iteration 48, loss = 0.20324861\n",
            "Iteration 49, loss = 0.20185737\n",
            "Iteration 50, loss = 0.20131980\n",
            "Iteration 51, loss = 0.19976421\n",
            "Iteration 52, loss = 0.19707924\n",
            "Iteration 53, loss = 0.19579962\n",
            "Iteration 54, loss = 0.19472064\n",
            "Iteration 55, loss = 0.19505153\n",
            "Iteration 56, loss = 0.19285926\n",
            "Iteration 57, loss = 0.18996329\n",
            "Iteration 58, loss = 0.18917342\n",
            "Iteration 59, loss = 0.18918626\n",
            "Iteration 60, loss = 0.18817151\n",
            "Iteration 61, loss = 0.18685256\n",
            "Iteration 62, loss = 0.18762866\n",
            "Iteration 63, loss = 0.18441723\n",
            "Iteration 64, loss = 0.18087835\n",
            "Iteration 65, loss = 0.18238070\n",
            "Iteration 66, loss = 0.18200718\n",
            "Iteration 67, loss = 0.18013522\n",
            "Iteration 68, loss = 0.17954091\n",
            "Iteration 69, loss = 0.17861631\n",
            "Iteration 70, loss = 0.17694085\n",
            "Iteration 71, loss = 0.17617028\n",
            "Iteration 72, loss = 0.17513160\n",
            "Iteration 73, loss = 0.17413859\n",
            "Iteration 74, loss = 0.17430133\n",
            "Iteration 75, loss = 0.17237248\n",
            "Iteration 76, loss = 0.17062332\n",
            "Iteration 77, loss = 0.16975462\n",
            "Iteration 78, loss = 0.17032417\n",
            "Iteration 79, loss = 0.16825097\n",
            "Iteration 80, loss = 0.16815605\n",
            "Iteration 81, loss = 0.16803480\n",
            "Iteration 82, loss = 0.16643340\n",
            "Iteration 83, loss = 0.16551431\n",
            "Iteration 84, loss = 0.16346961\n",
            "Iteration 85, loss = 0.16475514\n",
            "Iteration 86, loss = 0.16573249\n",
            "Iteration 87, loss = 0.16468657\n",
            "Iteration 88, loss = 0.16297765\n",
            "Iteration 89, loss = 0.16117282\n",
            "Iteration 90, loss = 0.15999391\n",
            "Iteration 91, loss = 0.15938853\n",
            "Iteration 92, loss = 0.15757765\n",
            "Iteration 93, loss = 0.15789130\n",
            "Iteration 94, loss = 0.15738936\n",
            "Iteration 95, loss = 0.15450166\n",
            "Iteration 96, loss = 0.15569099\n",
            "Iteration 97, loss = 0.15553525\n",
            "Iteration 98, loss = 0.15655018\n",
            "Iteration 99, loss = 0.15511544\n",
            "Iteration 100, loss = 0.15360071\n",
            "Iteration 101, loss = 0.15539552\n",
            "Iteration 102, loss = 0.15304896\n",
            "Iteration 103, loss = 0.15140311\n",
            "Iteration 104, loss = 0.14884540\n",
            "Iteration 105, loss = 0.15088560\n",
            "Iteration 106, loss = 0.15038870\n",
            "Iteration 107, loss = 0.15210241\n",
            "Iteration 108, loss = 0.14977283\n",
            "Iteration 109, loss = 0.14850994\n",
            "Iteration 110, loss = 0.14865450\n",
            "Iteration 111, loss = 0.14734039\n",
            "Iteration 112, loss = 0.14598682\n",
            "Iteration 113, loss = 0.14635542\n",
            "Iteration 114, loss = 0.14653967\n",
            "Iteration 115, loss = 0.14445319\n",
            "Iteration 116, loss = 0.14607557\n",
            "Iteration 117, loss = 0.14498294\n",
            "Iteration 118, loss = 0.14536409\n",
            "Iteration 119, loss = 0.14378809\n",
            "Iteration 120, loss = 0.14365007\n",
            "Iteration 121, loss = 0.14321953\n",
            "Iteration 122, loss = 0.14235731\n",
            "Iteration 123, loss = 0.14197505\n",
            "Iteration 124, loss = 0.14198572\n",
            "Iteration 125, loss = 0.13961085\n",
            "Iteration 126, loss = 0.14157052\n",
            "Iteration 127, loss = 0.13937718\n",
            "Iteration 128, loss = 0.13848874\n",
            "Iteration 129, loss = 0.13699474\n",
            "Iteration 130, loss = 0.13804126\n",
            "Iteration 131, loss = 0.13650734\n",
            "Iteration 132, loss = 0.13533793\n",
            "Iteration 133, loss = 0.13734587\n",
            "Iteration 134, loss = 0.13605374\n",
            "Iteration 135, loss = 0.13741137\n",
            "Iteration 136, loss = 0.13628977\n",
            "Iteration 137, loss = 0.13463118\n",
            "Iteration 138, loss = 0.13379029\n",
            "Iteration 139, loss = 0.13638772\n",
            "Iteration 140, loss = 0.13523115\n",
            "Iteration 141, loss = 0.13416204\n",
            "Iteration 142, loss = 0.13548684\n",
            "Iteration 143, loss = 0.13342389\n",
            "Iteration 144, loss = 0.13244801\n",
            "Iteration 145, loss = 0.13408595\n",
            "Iteration 146, loss = 0.13186440\n",
            "Iteration 147, loss = 0.13058834\n",
            "Iteration 148, loss = 0.13239928\n",
            "Iteration 149, loss = 0.13104061\n",
            "Iteration 150, loss = 0.13214804\n",
            "Iteration 151, loss = 0.13293089\n",
            "Iteration 152, loss = 0.12986429\n",
            "Iteration 153, loss = 0.12829976\n",
            "Iteration 154, loss = 0.13219452\n",
            "Iteration 155, loss = 0.12936387\n",
            "Iteration 156, loss = 0.12879903\n",
            "Iteration 157, loss = 0.13167426\n",
            "Iteration 158, loss = 0.13152690\n",
            "Iteration 159, loss = 0.13001212\n",
            "Iteration 160, loss = 0.12714692\n",
            "Iteration 161, loss = 0.12761222\n",
            "Iteration 162, loss = 0.12756778\n",
            "Iteration 163, loss = 0.12919763\n",
            "Iteration 164, loss = 0.12535310\n",
            "Iteration 165, loss = 0.12718294\n",
            "Iteration 166, loss = 0.12372986\n",
            "Iteration 167, loss = 0.12420103\n",
            "Iteration 168, loss = 0.12407582\n",
            "Iteration 169, loss = 0.12507118\n",
            "Iteration 170, loss = 0.12556053\n",
            "Iteration 171, loss = 0.12391341\n",
            "Iteration 172, loss = 0.12335856\n",
            "Iteration 173, loss = 0.12190189\n",
            "Iteration 174, loss = 0.12482166\n",
            "Iteration 175, loss = 0.12081001\n",
            "Iteration 176, loss = 0.12219274\n",
            "Iteration 177, loss = 0.12406427\n",
            "Iteration 178, loss = 0.12041002\n",
            "Iteration 179, loss = 0.12429304\n",
            "Iteration 180, loss = 0.12326683\n",
            "Iteration 181, loss = 0.12289201\n",
            "Iteration 182, loss = 0.12015873\n",
            "Iteration 183, loss = 0.12082274\n",
            "Iteration 184, loss = 0.11943115\n",
            "Iteration 185, loss = 0.11828556\n",
            "Iteration 186, loss = 0.11816869\n",
            "Iteration 187, loss = 0.11975867\n",
            "Iteration 188, loss = 0.11808029\n",
            "Iteration 189, loss = 0.11983598\n",
            "Iteration 190, loss = 0.11788426\n",
            "Iteration 191, loss = 0.11752187\n",
            "Iteration 192, loss = 0.11737857\n",
            "Iteration 193, loss = 0.11912271\n",
            "Iteration 194, loss = 0.11634467\n",
            "Iteration 195, loss = 0.11584506\n",
            "Iteration 196, loss = 0.11717049\n",
            "Iteration 197, loss = 0.12066563\n",
            "Iteration 198, loss = 0.11765366\n",
            "Iteration 199, loss = 0.11765977\n",
            "Iteration 200, loss = 0.11909652\n",
            "Iteration 201, loss = 0.11678808\n",
            "Iteration 202, loss = 0.11523101\n",
            "Iteration 203, loss = 0.11544933\n",
            "Iteration 204, loss = 0.11545610\n",
            "Iteration 205, loss = 0.11416778\n",
            "Iteration 206, loss = 0.11582579\n",
            "Iteration 207, loss = 0.11513180\n",
            "Iteration 208, loss = 0.11336428\n",
            "Iteration 209, loss = 0.11671638\n",
            "Iteration 210, loss = 0.11197968\n",
            "Iteration 211, loss = 0.11383354\n",
            "Iteration 212, loss = 0.11631923\n",
            "Iteration 213, loss = 0.11588524\n",
            "Iteration 214, loss = 0.11436266\n",
            "Iteration 215, loss = 0.11460694\n",
            "Iteration 216, loss = 0.11266204\n",
            "Iteration 217, loss = 0.11495488\n",
            "Iteration 218, loss = 0.11282003\n",
            "Iteration 219, loss = 0.11348390\n",
            "Iteration 220, loss = 0.11131825\n",
            "Iteration 221, loss = 0.11267712\n",
            "Iteration 222, loss = 0.11030273\n",
            "Iteration 223, loss = 0.10990450\n",
            "Iteration 224, loss = 0.11298349\n",
            "Iteration 225, loss = 0.11146787\n",
            "Iteration 226, loss = 0.11023398\n",
            "Iteration 227, loss = 0.10870170\n",
            "Iteration 228, loss = 0.10830897\n",
            "Iteration 229, loss = 0.11087413\n",
            "Iteration 230, loss = 0.10963950\n",
            "Iteration 231, loss = 0.10920940\n",
            "Iteration 232, loss = 0.10916976\n",
            "Iteration 233, loss = 0.10920743\n",
            "Iteration 234, loss = 0.10982501\n",
            "Iteration 235, loss = 0.11116106\n",
            "Iteration 236, loss = 0.10789741\n",
            "Iteration 237, loss = 0.10946381\n",
            "Iteration 238, loss = 0.10887884\n",
            "Iteration 239, loss = 0.10947462\n",
            "Iteration 240, loss = 0.10797954\n",
            "Iteration 241, loss = 0.10758690\n",
            "Iteration 242, loss = 0.10560254\n",
            "Iteration 243, loss = 0.10767342\n",
            "Iteration 244, loss = 0.10508067\n",
            "Iteration 245, loss = 0.10711427\n",
            "Iteration 246, loss = 0.10746398\n",
            "Iteration 247, loss = 0.10842327\n",
            "Iteration 248, loss = 0.10809519\n",
            "Iteration 249, loss = 0.10770567\n",
            "Iteration 250, loss = 0.10387241\n",
            "Iteration 251, loss = 0.10794655\n",
            "Iteration 252, loss = 0.10586591\n",
            "Iteration 253, loss = 0.10502197\n",
            "Iteration 254, loss = 0.10670663\n",
            "Iteration 255, loss = 0.10890207\n",
            "Iteration 256, loss = 0.10624561\n",
            "Iteration 257, loss = 0.10471891\n",
            "Iteration 258, loss = 0.10397693\n",
            "Iteration 259, loss = 0.10472735\n",
            "Iteration 260, loss = 0.10557900\n",
            "Iteration 261, loss = 0.10393530\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.39075582\n",
            "Iteration 2, loss = 0.32480569\n",
            "Iteration 3, loss = 0.31195778\n",
            "Iteration 4, loss = 0.30623980\n",
            "Iteration 5, loss = 0.30096398\n",
            "Iteration 6, loss = 0.29687342\n",
            "Iteration 7, loss = 0.29288524\n",
            "Iteration 8, loss = 0.29020812\n",
            "Iteration 9, loss = 0.28794224\n",
            "Iteration 10, loss = 0.28425769\n",
            "Iteration 11, loss = 0.28195895\n",
            "Iteration 12, loss = 0.28088873\n",
            "Iteration 13, loss = 0.27683513\n",
            "Iteration 14, loss = 0.27402665\n",
            "Iteration 15, loss = 0.27301631\n",
            "Iteration 16, loss = 0.27018598\n",
            "Iteration 17, loss = 0.26774914\n",
            "Iteration 18, loss = 0.26600029\n",
            "Iteration 19, loss = 0.26212076\n",
            "Iteration 20, loss = 0.25973479\n",
            "Iteration 21, loss = 0.25868893\n",
            "Iteration 22, loss = 0.25643228\n",
            "Iteration 23, loss = 0.25397822\n",
            "Iteration 24, loss = 0.25236904\n",
            "Iteration 25, loss = 0.24928272\n",
            "Iteration 26, loss = 0.24712578\n",
            "Iteration 27, loss = 0.24437690\n",
            "Iteration 28, loss = 0.24315261\n",
            "Iteration 29, loss = 0.24126846\n",
            "Iteration 30, loss = 0.23982367\n",
            "Iteration 31, loss = 0.23646646\n",
            "Iteration 32, loss = 0.23592614\n",
            "Iteration 33, loss = 0.23268900\n",
            "Iteration 34, loss = 0.23172217\n",
            "Iteration 35, loss = 0.22866406\n",
            "Iteration 36, loss = 0.22525036\n",
            "Iteration 37, loss = 0.22618717\n",
            "Iteration 38, loss = 0.22311654\n",
            "Iteration 39, loss = 0.22203760\n",
            "Iteration 40, loss = 0.22000458\n",
            "Iteration 41, loss = 0.21887926\n",
            "Iteration 42, loss = 0.21532311\n",
            "Iteration 43, loss = 0.21437807\n",
            "Iteration 44, loss = 0.21604254\n",
            "Iteration 45, loss = 0.21295669\n",
            "Iteration 46, loss = 0.20993096\n",
            "Iteration 47, loss = 0.20766626\n",
            "Iteration 48, loss = 0.20788644\n",
            "Iteration 49, loss = 0.20503514\n",
            "Iteration 50, loss = 0.20383003\n",
            "Iteration 51, loss = 0.20351655\n",
            "Iteration 52, loss = 0.20089816\n",
            "Iteration 53, loss = 0.20077334\n",
            "Iteration 54, loss = 0.19918525\n",
            "Iteration 55, loss = 0.19768002\n",
            "Iteration 56, loss = 0.19652213\n",
            "Iteration 57, loss = 0.19520666\n",
            "Iteration 58, loss = 0.19483564\n",
            "Iteration 59, loss = 0.19261937\n",
            "Iteration 60, loss = 0.19212214\n",
            "Iteration 61, loss = 0.19171385\n",
            "Iteration 62, loss = 0.19055919\n",
            "Iteration 63, loss = 0.18791419\n",
            "Iteration 64, loss = 0.18739641\n",
            "Iteration 65, loss = 0.18716745\n",
            "Iteration 66, loss = 0.18364437\n",
            "Iteration 67, loss = 0.18523210\n",
            "Iteration 68, loss = 0.18322073\n",
            "Iteration 69, loss = 0.18289822\n",
            "Iteration 70, loss = 0.18204733\n",
            "Iteration 71, loss = 0.17796973\n",
            "Iteration 72, loss = 0.17874870\n",
            "Iteration 73, loss = 0.17927066\n",
            "Iteration 74, loss = 0.17613659\n",
            "Iteration 75, loss = 0.17603777\n",
            "Iteration 76, loss = 0.17569157\n",
            "Iteration 77, loss = 0.17479209\n",
            "Iteration 78, loss = 0.17324121\n",
            "Iteration 79, loss = 0.17236723\n",
            "Iteration 80, loss = 0.17428854\n",
            "Iteration 81, loss = 0.17052458\n",
            "Iteration 82, loss = 0.16914949\n",
            "Iteration 83, loss = 0.16984636\n",
            "Iteration 84, loss = 0.17047350\n",
            "Iteration 85, loss = 0.16791940\n",
            "Iteration 86, loss = 0.16685283\n",
            "Iteration 87, loss = 0.16580344\n",
            "Iteration 88, loss = 0.16634852\n",
            "Iteration 89, loss = 0.16580477\n",
            "Iteration 90, loss = 0.16396655\n",
            "Iteration 91, loss = 0.16345995\n",
            "Iteration 92, loss = 0.16340015\n",
            "Iteration 93, loss = 0.16326448\n",
            "Iteration 94, loss = 0.16225835\n",
            "Iteration 95, loss = 0.16084727\n",
            "Iteration 96, loss = 0.16201431\n",
            "Iteration 97, loss = 0.16017561\n",
            "Iteration 98, loss = 0.16030585\n",
            "Iteration 99, loss = 0.15805234\n",
            "Iteration 100, loss = 0.15785703\n",
            "Iteration 101, loss = 0.15652391\n",
            "Iteration 102, loss = 0.15689285\n",
            "Iteration 103, loss = 0.15626872\n",
            "Iteration 104, loss = 0.15558638\n",
            "Iteration 105, loss = 0.15373612\n",
            "Iteration 106, loss = 0.15457424\n",
            "Iteration 107, loss = 0.15276132\n",
            "Iteration 108, loss = 0.15308842\n",
            "Iteration 109, loss = 0.15354312\n",
            "Iteration 110, loss = 0.15113405\n",
            "Iteration 111, loss = 0.15118059\n",
            "Iteration 112, loss = 0.15020245\n",
            "Iteration 113, loss = 0.14938376\n",
            "Iteration 114, loss = 0.15051605\n",
            "Iteration 115, loss = 0.14854088\n",
            "Iteration 116, loss = 0.14837405\n",
            "Iteration 117, loss = 0.14715557\n",
            "Iteration 118, loss = 0.14678777\n",
            "Iteration 119, loss = 0.14921584\n",
            "Iteration 120, loss = 0.14678517\n",
            "Iteration 121, loss = 0.14802857\n",
            "Iteration 122, loss = 0.14645002\n",
            "Iteration 123, loss = 0.14568407\n",
            "Iteration 124, loss = 0.14794311\n",
            "Iteration 125, loss = 0.14530386\n",
            "Iteration 126, loss = 0.14291466\n",
            "Iteration 127, loss = 0.14243119\n",
            "Iteration 128, loss = 0.14134581\n",
            "Iteration 129, loss = 0.14284079\n",
            "Iteration 130, loss = 0.14260899\n",
            "Iteration 131, loss = 0.14038604\n",
            "Iteration 132, loss = 0.14047985\n",
            "Iteration 133, loss = 0.14195403\n",
            "Iteration 134, loss = 0.13951722\n",
            "Iteration 135, loss = 0.14114069\n",
            "Iteration 136, loss = 0.14201280\n",
            "Iteration 137, loss = 0.13947002\n",
            "Iteration 138, loss = 0.13732163\n",
            "Iteration 139, loss = 0.13832151\n",
            "Iteration 140, loss = 0.13756889\n",
            "Iteration 141, loss = 0.13951985\n",
            "Iteration 142, loss = 0.13648811\n",
            "Iteration 143, loss = 0.13591795\n",
            "Iteration 144, loss = 0.13613995\n",
            "Iteration 145, loss = 0.13582655\n",
            "Iteration 146, loss = 0.13323045\n",
            "Iteration 147, loss = 0.13488074\n",
            "Iteration 148, loss = 0.13368463\n",
            "Iteration 149, loss = 0.13321889\n",
            "Iteration 150, loss = 0.13300608\n",
            "Iteration 151, loss = 0.13474246\n",
            "Iteration 152, loss = 0.13473805\n",
            "Iteration 153, loss = 0.13399491\n",
            "Iteration 154, loss = 0.13375267\n",
            "Iteration 155, loss = 0.13095608\n",
            "Iteration 156, loss = 0.13072827\n",
            "Iteration 157, loss = 0.13093014\n",
            "Iteration 158, loss = 0.13223454\n",
            "Iteration 159, loss = 0.12998106\n",
            "Iteration 160, loss = 0.13243921\n",
            "Iteration 161, loss = 0.13088262\n",
            "Iteration 162, loss = 0.12945465\n",
            "Iteration 163, loss = 0.13023917\n",
            "Iteration 164, loss = 0.12899893\n",
            "Iteration 165, loss = 0.12952331\n",
            "Iteration 166, loss = 0.12738244\n",
            "Iteration 167, loss = 0.13300271\n",
            "Iteration 168, loss = 0.13096911\n",
            "Iteration 169, loss = 0.13004701\n",
            "Iteration 170, loss = 0.12624331\n",
            "Iteration 171, loss = 0.12712799\n",
            "Iteration 172, loss = 0.12824962\n",
            "Iteration 173, loss = 0.12691172\n",
            "Iteration 174, loss = 0.12644264\n",
            "Iteration 175, loss = 0.12530957\n",
            "Iteration 176, loss = 0.12564017\n",
            "Iteration 177, loss = 0.12440756\n",
            "Iteration 178, loss = 0.12383056\n",
            "Iteration 179, loss = 0.12605390\n",
            "Iteration 180, loss = 0.12478413\n",
            "Iteration 181, loss = 0.12631529\n",
            "Iteration 182, loss = 0.12272550\n",
            "Iteration 183, loss = 0.12194575\n",
            "Iteration 184, loss = 0.12083617\n",
            "Iteration 185, loss = 0.12433970\n",
            "Iteration 186, loss = 0.12318185\n",
            "Iteration 187, loss = 0.12246229\n",
            "Iteration 188, loss = 0.12153028\n",
            "Iteration 189, loss = 0.12264450\n",
            "Iteration 190, loss = 0.12240924\n",
            "Iteration 191, loss = 0.12028013\n",
            "Iteration 192, loss = 0.11911540\n",
            "Iteration 193, loss = 0.12163262\n",
            "Iteration 194, loss = 0.12568914\n",
            "Iteration 195, loss = 0.12551581\n",
            "Iteration 196, loss = 0.12171261\n",
            "Iteration 197, loss = 0.12106718\n",
            "Iteration 198, loss = 0.11910660\n",
            "Iteration 199, loss = 0.11937611\n",
            "Iteration 200, loss = 0.11942193\n",
            "Iteration 201, loss = 0.11894991\n",
            "Iteration 202, loss = 0.11910971\n",
            "Iteration 203, loss = 0.11715844\n",
            "Iteration 204, loss = 0.11783372\n",
            "Iteration 205, loss = 0.11645341\n",
            "Iteration 206, loss = 0.11691313\n",
            "Iteration 207, loss = 0.11864882\n",
            "Iteration 208, loss = 0.11697223\n",
            "Iteration 209, loss = 0.12233861\n",
            "Iteration 210, loss = 0.11859562\n",
            "Iteration 211, loss = 0.12036908\n",
            "Iteration 212, loss = 0.11820376\n",
            "Iteration 213, loss = 0.11439521\n",
            "Iteration 214, loss = 0.11811696\n",
            "Iteration 215, loss = 0.11840381\n",
            "Iteration 216, loss = 0.11728985\n",
            "Iteration 217, loss = 0.11689759\n",
            "Iteration 218, loss = 0.11448255\n",
            "Iteration 219, loss = 0.11533293\n",
            "Iteration 220, loss = 0.11469587\n",
            "Iteration 221, loss = 0.11110225\n",
            "Iteration 222, loss = 0.11342287\n",
            "Iteration 223, loss = 0.11535540\n",
            "Iteration 224, loss = 0.11419812\n",
            "Iteration 225, loss = 0.11242773\n",
            "Iteration 226, loss = 0.11286733\n",
            "Iteration 227, loss = 0.11510193\n",
            "Iteration 228, loss = 0.11224039\n",
            "Iteration 229, loss = 0.11491505\n",
            "Iteration 230, loss = 0.11368248\n",
            "Iteration 231, loss = 0.11012770\n",
            "Iteration 232, loss = 0.11181045\n",
            "Iteration 233, loss = 0.11189505\n",
            "Iteration 234, loss = 0.10926559\n",
            "Iteration 235, loss = 0.11129559\n",
            "Iteration 236, loss = 0.11203762\n",
            "Iteration 237, loss = 0.11037991\n",
            "Iteration 238, loss = 0.11068768\n",
            "Iteration 239, loss = 0.10942877\n",
            "Iteration 240, loss = 0.11126029\n",
            "Iteration 241, loss = 0.11239654\n",
            "Iteration 242, loss = 0.11544663\n",
            "Iteration 243, loss = 0.11463601\n",
            "Iteration 244, loss = 0.11410016\n",
            "Iteration 245, loss = 0.10815016\n",
            "Iteration 246, loss = 0.10888429\n",
            "Iteration 247, loss = 0.10891967\n",
            "Iteration 248, loss = 0.10914961\n",
            "Iteration 249, loss = 0.10935657\n",
            "Iteration 250, loss = 0.10804772\n",
            "Iteration 251, loss = 0.10750075\n",
            "Iteration 252, loss = 0.10755717\n",
            "Iteration 253, loss = 0.10969643\n",
            "Iteration 254, loss = 0.10960039\n",
            "Iteration 255, loss = 0.11006268\n",
            "Iteration 256, loss = 0.11199451\n",
            "Iteration 257, loss = 0.10558509\n",
            "Iteration 258, loss = 0.10830216\n",
            "Iteration 259, loss = 0.10998686\n",
            "Iteration 260, loss = 0.10821082\n",
            "Iteration 261, loss = 0.10656097\n",
            "Iteration 262, loss = 0.10552516\n",
            "Iteration 263, loss = 0.10651799\n",
            "Iteration 264, loss = 0.10526623\n",
            "Iteration 265, loss = 0.10877534\n",
            "Iteration 266, loss = 0.10548062\n",
            "Iteration 267, loss = 0.10605038\n",
            "Iteration 268, loss = 0.10902277\n",
            "Iteration 269, loss = 0.10558440\n",
            "Iteration 270, loss = 0.10463873\n",
            "Iteration 271, loss = 0.10505629\n",
            "Iteration 272, loss = 0.10516187\n",
            "Iteration 273, loss = 0.10299992\n",
            "Iteration 274, loss = 0.10521862\n",
            "Iteration 275, loss = 0.10507227\n",
            "Iteration 276, loss = 0.10739163\n",
            "Iteration 277, loss = 0.10472134\n",
            "Iteration 278, loss = 0.10620480\n",
            "Iteration 279, loss = 0.10584098\n",
            "Iteration 280, loss = 0.10553021\n",
            "Iteration 281, loss = 0.10421044\n",
            "Iteration 282, loss = 0.10512907\n",
            "Iteration 283, loss = 0.10380562\n",
            "Iteration 284, loss = 0.10465683\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.37813189\n",
            "Iteration 2, loss = 0.32280625\n",
            "Iteration 3, loss = 0.31160549\n",
            "Iteration 4, loss = 0.30410138\n",
            "Iteration 5, loss = 0.29799094\n",
            "Iteration 6, loss = 0.29481646\n",
            "Iteration 7, loss = 0.29104190\n",
            "Iteration 8, loss = 0.28791450\n",
            "Iteration 9, loss = 0.28585725\n",
            "Iteration 10, loss = 0.28351105\n",
            "Iteration 11, loss = 0.28025586\n",
            "Iteration 12, loss = 0.27838278\n",
            "Iteration 13, loss = 0.27542785\n",
            "Iteration 14, loss = 0.27376775\n",
            "Iteration 15, loss = 0.27093907\n",
            "Iteration 16, loss = 0.26825818\n",
            "Iteration 17, loss = 0.26744278\n",
            "Iteration 18, loss = 0.26366851\n",
            "Iteration 19, loss = 0.26162427\n",
            "Iteration 20, loss = 0.25917015\n",
            "Iteration 21, loss = 0.25679045\n",
            "Iteration 22, loss = 0.25594802\n",
            "Iteration 23, loss = 0.25242272\n",
            "Iteration 24, loss = 0.25044719\n",
            "Iteration 25, loss = 0.24888987\n",
            "Iteration 26, loss = 0.24683027\n",
            "Iteration 27, loss = 0.24462526\n",
            "Iteration 28, loss = 0.24278601\n",
            "Iteration 29, loss = 0.23931748\n",
            "Iteration 30, loss = 0.24020305\n",
            "Iteration 31, loss = 0.23691085\n",
            "Iteration 32, loss = 0.23483704\n",
            "Iteration 33, loss = 0.23178750\n",
            "Iteration 34, loss = 0.22990854\n",
            "Iteration 35, loss = 0.22907605\n",
            "Iteration 36, loss = 0.22731112\n",
            "Iteration 37, loss = 0.22655227\n",
            "Iteration 38, loss = 0.22471220\n",
            "Iteration 39, loss = 0.22361240\n",
            "Iteration 40, loss = 0.21948935\n",
            "Iteration 41, loss = 0.21813936\n",
            "Iteration 42, loss = 0.21767548\n",
            "Iteration 43, loss = 0.21632392\n",
            "Iteration 44, loss = 0.21453223\n",
            "Iteration 45, loss = 0.21213490\n",
            "Iteration 46, loss = 0.21132661\n",
            "Iteration 47, loss = 0.21011261\n",
            "Iteration 48, loss = 0.20638205\n",
            "Iteration 49, loss = 0.20606809\n",
            "Iteration 50, loss = 0.20515045\n",
            "Iteration 51, loss = 0.20352749\n",
            "Iteration 52, loss = 0.20237546\n",
            "Iteration 53, loss = 0.20081607\n",
            "Iteration 54, loss = 0.19881932\n",
            "Iteration 55, loss = 0.19953994\n",
            "Iteration 56, loss = 0.19732316\n",
            "Iteration 57, loss = 0.19609113\n",
            "Iteration 58, loss = 0.19496062\n",
            "Iteration 59, loss = 0.19417066\n",
            "Iteration 60, loss = 0.19233769\n",
            "Iteration 61, loss = 0.19094770\n",
            "Iteration 62, loss = 0.19100590\n",
            "Iteration 63, loss = 0.19104502\n",
            "Iteration 64, loss = 0.18933935\n",
            "Iteration 65, loss = 0.18735501\n",
            "Iteration 66, loss = 0.18710948\n",
            "Iteration 67, loss = 0.18614921\n",
            "Iteration 68, loss = 0.18318628\n",
            "Iteration 69, loss = 0.18395942\n",
            "Iteration 70, loss = 0.18292644\n",
            "Iteration 71, loss = 0.18017527\n",
            "Iteration 72, loss = 0.18008585\n",
            "Iteration 73, loss = 0.17860744\n",
            "Iteration 74, loss = 0.18017886\n",
            "Iteration 75, loss = 0.17810994\n",
            "Iteration 76, loss = 0.17758842\n",
            "Iteration 77, loss = 0.17763354\n",
            "Iteration 78, loss = 0.17626093\n",
            "Iteration 79, loss = 0.17453038\n",
            "Iteration 80, loss = 0.17375667\n",
            "Iteration 81, loss = 0.17167768\n",
            "Iteration 82, loss = 0.17063104\n",
            "Iteration 83, loss = 0.17171830\n",
            "Iteration 84, loss = 0.16965182\n",
            "Iteration 85, loss = 0.16833714\n",
            "Iteration 86, loss = 0.16892712\n",
            "Iteration 87, loss = 0.16858785\n",
            "Iteration 88, loss = 0.16669942\n",
            "Iteration 89, loss = 0.16670476\n",
            "Iteration 90, loss = 0.16582553\n",
            "Iteration 91, loss = 0.16397821\n",
            "Iteration 92, loss = 0.16461613\n",
            "Iteration 93, loss = 0.16277621\n",
            "Iteration 94, loss = 0.16147576\n",
            "Iteration 95, loss = 0.16181992\n",
            "Iteration 96, loss = 0.16084537\n",
            "Iteration 97, loss = 0.16106600\n",
            "Iteration 98, loss = 0.16035527\n",
            "Iteration 99, loss = 0.15836544\n",
            "Iteration 100, loss = 0.15774385\n",
            "Iteration 101, loss = 0.15840423\n",
            "Iteration 102, loss = 0.15841438\n",
            "Iteration 103, loss = 0.15675347\n",
            "Iteration 104, loss = 0.15876772\n",
            "Iteration 105, loss = 0.15711680\n",
            "Iteration 106, loss = 0.15530719\n",
            "Iteration 107, loss = 0.15337591\n",
            "Iteration 108, loss = 0.15301061\n",
            "Iteration 109, loss = 0.15349490\n",
            "Iteration 110, loss = 0.15226565\n",
            "Iteration 111, loss = 0.15261382\n",
            "Iteration 112, loss = 0.14973703\n",
            "Iteration 113, loss = 0.15053440\n",
            "Iteration 114, loss = 0.14989992\n",
            "Iteration 115, loss = 0.15187421\n",
            "Iteration 116, loss = 0.15058445\n",
            "Iteration 117, loss = 0.14935730\n",
            "Iteration 118, loss = 0.14721361\n",
            "Iteration 119, loss = 0.14620846\n",
            "Iteration 120, loss = 0.14647466\n",
            "Iteration 121, loss = 0.14709360\n",
            "Iteration 122, loss = 0.14686102\n",
            "Iteration 123, loss = 0.14387142\n",
            "Iteration 124, loss = 0.14488216\n",
            "Iteration 125, loss = 0.14413665\n",
            "Iteration 126, loss = 0.14451898\n",
            "Iteration 127, loss = 0.14367653\n",
            "Iteration 128, loss = 0.14197872\n",
            "Iteration 129, loss = 0.14296384\n",
            "Iteration 130, loss = 0.14269815\n",
            "Iteration 131, loss = 0.14371807\n",
            "Iteration 132, loss = 0.14211830\n",
            "Iteration 133, loss = 0.14376847\n",
            "Iteration 134, loss = 0.14291888\n",
            "Iteration 135, loss = 0.13939334\n",
            "Iteration 136, loss = 0.13985735\n",
            "Iteration 137, loss = 0.14072771\n",
            "Iteration 138, loss = 0.13940341\n",
            "Iteration 139, loss = 0.14037396\n",
            "Iteration 140, loss = 0.13738269\n",
            "Iteration 141, loss = 0.13933669\n",
            "Iteration 142, loss = 0.13653954\n",
            "Iteration 143, loss = 0.13660913\n",
            "Iteration 144, loss = 0.13548829\n",
            "Iteration 145, loss = 0.13591730\n",
            "Iteration 146, loss = 0.13463119\n",
            "Iteration 147, loss = 0.13624565\n",
            "Iteration 148, loss = 0.13695127\n",
            "Iteration 149, loss = 0.13458052\n",
            "Iteration 150, loss = 0.13401845\n",
            "Iteration 151, loss = 0.13625492\n",
            "Iteration 152, loss = 0.13213405\n",
            "Iteration 153, loss = 0.13330844\n",
            "Iteration 154, loss = 0.13439730\n",
            "Iteration 155, loss = 0.13202088\n",
            "Iteration 156, loss = 0.13093181\n",
            "Iteration 157, loss = 0.13296521\n",
            "Iteration 158, loss = 0.13494483\n",
            "Iteration 159, loss = 0.13229467\n",
            "Iteration 160, loss = 0.12978222\n",
            "Iteration 161, loss = 0.13004187\n",
            "Iteration 162, loss = 0.12943481\n",
            "Iteration 163, loss = 0.13130058\n",
            "Iteration 164, loss = 0.12931189\n",
            "Iteration 165, loss = 0.13035689\n",
            "Iteration 166, loss = 0.13026628\n",
            "Iteration 167, loss = 0.12943157\n",
            "Iteration 168, loss = 0.12784475\n",
            "Iteration 169, loss = 0.12882726\n",
            "Iteration 170, loss = 0.13179036\n",
            "Iteration 171, loss = 0.12848444\n",
            "Iteration 172, loss = 0.12715668\n",
            "Iteration 173, loss = 0.12573920\n",
            "Iteration 174, loss = 0.12581287\n",
            "Iteration 175, loss = 0.12529226\n",
            "Iteration 176, loss = 0.12448298\n",
            "Iteration 177, loss = 0.12548095\n",
            "Iteration 178, loss = 0.12685828\n",
            "Iteration 179, loss = 0.12466467\n",
            "Iteration 180, loss = 0.12491115\n",
            "Iteration 181, loss = 0.12440759\n",
            "Iteration 182, loss = 0.12795825\n",
            "Iteration 183, loss = 0.12420874\n",
            "Iteration 184, loss = 0.12361247\n",
            "Iteration 185, loss = 0.12167317\n",
            "Iteration 186, loss = 0.12338118\n",
            "Iteration 187, loss = 0.12370227\n",
            "Iteration 188, loss = 0.12142429\n",
            "Iteration 189, loss = 0.12070928\n",
            "Iteration 190, loss = 0.12045899\n",
            "Iteration 191, loss = 0.12136866\n",
            "Iteration 192, loss = 0.12171172\n",
            "Iteration 193, loss = 0.12228970\n",
            "Iteration 194, loss = 0.11963441\n",
            "Iteration 195, loss = 0.12273509\n",
            "Iteration 196, loss = 0.12283699\n",
            "Iteration 197, loss = 0.12052513\n",
            "Iteration 198, loss = 0.11834446\n",
            "Iteration 199, loss = 0.11914095\n",
            "Iteration 200, loss = 0.11751636\n",
            "Iteration 201, loss = 0.11955984\n",
            "Iteration 202, loss = 0.11587550\n",
            "Iteration 203, loss = 0.11782024\n",
            "Iteration 204, loss = 0.11781168\n",
            "Iteration 205, loss = 0.11953412\n",
            "Iteration 206, loss = 0.11890639\n",
            "Iteration 207, loss = 0.11523596\n",
            "Iteration 208, loss = 0.11650862\n",
            "Iteration 209, loss = 0.11739077\n",
            "Iteration 210, loss = 0.11760533\n",
            "Iteration 211, loss = 0.11860316\n",
            "Iteration 212, loss = 0.11849527\n",
            "Iteration 213, loss = 0.11926536\n",
            "Iteration 214, loss = 0.11612478\n",
            "Iteration 215, loss = 0.11386864\n",
            "Iteration 216, loss = 0.11736349\n",
            "Iteration 217, loss = 0.11538324\n",
            "Iteration 218, loss = 0.11339945\n",
            "Iteration 219, loss = 0.11436373\n",
            "Iteration 220, loss = 0.11540352\n",
            "Iteration 221, loss = 0.11384665\n",
            "Iteration 222, loss = 0.11782829\n",
            "Iteration 223, loss = 0.11316513\n",
            "Iteration 224, loss = 0.11669628\n",
            "Iteration 225, loss = 0.11311700\n",
            "Iteration 226, loss = 0.11297531\n",
            "Iteration 227, loss = 0.11275616\n",
            "Iteration 228, loss = 0.11152789\n",
            "Iteration 229, loss = 0.11233561\n",
            "Iteration 230, loss = 0.11257780\n",
            "Iteration 231, loss = 0.11489497\n",
            "Iteration 232, loss = 0.11740781\n",
            "Iteration 233, loss = 0.11146154\n",
            "Iteration 234, loss = 0.11301854\n",
            "Iteration 235, loss = 0.11217453\n",
            "Iteration 236, loss = 0.11086825\n",
            "Iteration 237, loss = 0.10989435\n",
            "Iteration 238, loss = 0.11053782\n",
            "Iteration 239, loss = 0.10889986\n",
            "Iteration 240, loss = 0.11010566\n",
            "Iteration 241, loss = 0.11027734\n",
            "Iteration 242, loss = 0.11062135\n",
            "Iteration 243, loss = 0.10895182\n",
            "Iteration 244, loss = 0.11178509\n",
            "Iteration 245, loss = 0.11024846\n",
            "Iteration 246, loss = 0.10781263\n",
            "Iteration 247, loss = 0.11172933\n",
            "Iteration 248, loss = 0.11223458\n",
            "Iteration 249, loss = 0.10949026\n",
            "Iteration 250, loss = 0.11257133\n",
            "Iteration 251, loss = 0.10975687\n",
            "Iteration 252, loss = 0.11047578\n",
            "Iteration 253, loss = 0.10975624\n",
            "Iteration 254, loss = 0.10888677\n",
            "Iteration 255, loss = 0.10867104\n",
            "Iteration 256, loss = 0.10855643\n",
            "Iteration 257, loss = 0.11075792\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.36958050\n",
            "Iteration 2, loss = 0.32225896\n",
            "Iteration 3, loss = 0.31096888\n",
            "Iteration 4, loss = 0.30426680\n",
            "Iteration 5, loss = 0.29917140\n",
            "Iteration 6, loss = 0.29551652\n",
            "Iteration 7, loss = 0.29241151\n",
            "Iteration 8, loss = 0.28923845\n",
            "Iteration 9, loss = 0.28629731\n",
            "Iteration 10, loss = 0.28355537\n",
            "Iteration 11, loss = 0.28134335\n",
            "Iteration 12, loss = 0.27974713\n",
            "Iteration 13, loss = 0.27562709\n",
            "Iteration 14, loss = 0.27265817\n",
            "Iteration 15, loss = 0.27241931\n",
            "Iteration 16, loss = 0.26902860\n",
            "Iteration 17, loss = 0.26615523\n",
            "Iteration 18, loss = 0.26402059\n",
            "Iteration 19, loss = 0.26195118\n",
            "Iteration 20, loss = 0.26110795\n",
            "Iteration 21, loss = 0.25773209\n",
            "Iteration 22, loss = 0.25563000\n",
            "Iteration 23, loss = 0.25360411\n",
            "Iteration 24, loss = 0.25164783\n",
            "Iteration 25, loss = 0.24752447\n",
            "Iteration 26, loss = 0.24769725\n",
            "Iteration 27, loss = 0.24548952\n",
            "Iteration 28, loss = 0.24190632\n",
            "Iteration 29, loss = 0.24135395\n",
            "Iteration 30, loss = 0.23842026\n",
            "Iteration 31, loss = 0.23651749\n",
            "Iteration 32, loss = 0.23385054\n",
            "Iteration 33, loss = 0.23106521\n",
            "Iteration 34, loss = 0.23154884\n",
            "Iteration 35, loss = 0.22965493\n",
            "Iteration 36, loss = 0.22741359\n",
            "Iteration 37, loss = 0.22640361\n",
            "Iteration 38, loss = 0.22363944\n",
            "Iteration 39, loss = 0.22187587\n",
            "Iteration 40, loss = 0.22160237\n",
            "Iteration 41, loss = 0.21986542\n",
            "Iteration 42, loss = 0.21824963\n",
            "Iteration 43, loss = 0.21555031\n",
            "Iteration 44, loss = 0.21400142\n",
            "Iteration 45, loss = 0.21330230\n",
            "Iteration 46, loss = 0.21175381\n",
            "Iteration 47, loss = 0.20935731\n",
            "Iteration 48, loss = 0.21036451\n",
            "Iteration 49, loss = 0.20956959\n",
            "Iteration 50, loss = 0.20807793\n",
            "Iteration 51, loss = 0.20583204\n",
            "Iteration 52, loss = 0.20147489\n",
            "Iteration 53, loss = 0.20023151\n",
            "Iteration 54, loss = 0.20074675\n",
            "Iteration 55, loss = 0.19835264\n",
            "Iteration 56, loss = 0.19816179\n",
            "Iteration 57, loss = 0.19787960\n",
            "Iteration 58, loss = 0.19642240\n",
            "Iteration 59, loss = 0.19516536\n",
            "Iteration 60, loss = 0.19329909\n",
            "Iteration 61, loss = 0.19309682\n",
            "Iteration 62, loss = 0.19164365\n",
            "Iteration 63, loss = 0.19198414\n",
            "Iteration 64, loss = 0.18939105\n",
            "Iteration 65, loss = 0.18787580\n",
            "Iteration 66, loss = 0.18768178\n",
            "Iteration 67, loss = 0.18726300\n",
            "Iteration 68, loss = 0.18608816\n",
            "Iteration 69, loss = 0.18514268\n",
            "Iteration 70, loss = 0.18378853\n",
            "Iteration 71, loss = 0.18241169\n",
            "Iteration 72, loss = 0.18175606\n",
            "Iteration 73, loss = 0.18078450\n",
            "Iteration 74, loss = 0.17991173\n",
            "Iteration 75, loss = 0.17966727\n",
            "Iteration 76, loss = 0.17660951\n",
            "Iteration 77, loss = 0.17565885\n",
            "Iteration 78, loss = 0.17641889\n",
            "Iteration 79, loss = 0.17615023\n",
            "Iteration 80, loss = 0.17541099\n",
            "Iteration 81, loss = 0.17474033\n",
            "Iteration 82, loss = 0.17305925\n",
            "Iteration 83, loss = 0.17369293\n",
            "Iteration 84, loss = 0.17215857\n",
            "Iteration 85, loss = 0.17152421\n",
            "Iteration 86, loss = 0.17063307\n",
            "Iteration 87, loss = 0.16884560\n",
            "Iteration 88, loss = 0.16974426\n",
            "Iteration 89, loss = 0.16802546\n",
            "Iteration 90, loss = 0.16567090\n",
            "Iteration 91, loss = 0.16704597\n",
            "Iteration 92, loss = 0.16660355\n",
            "Iteration 93, loss = 0.16535191\n",
            "Iteration 94, loss = 0.16453390\n",
            "Iteration 95, loss = 0.16444279\n",
            "Iteration 96, loss = 0.16609080\n",
            "Iteration 97, loss = 0.16520868\n",
            "Iteration 98, loss = 0.16362989\n",
            "Iteration 99, loss = 0.16205977\n",
            "Iteration 100, loss = 0.16102098\n",
            "Iteration 101, loss = 0.16104988\n",
            "Iteration 102, loss = 0.16079033\n",
            "Iteration 103, loss = 0.15993910\n",
            "Iteration 104, loss = 0.15942496\n",
            "Iteration 105, loss = 0.15891683\n",
            "Iteration 106, loss = 0.15646599\n",
            "Iteration 107, loss = 0.15747040\n",
            "Iteration 108, loss = 0.15684892\n",
            "Iteration 109, loss = 0.15704225\n",
            "Iteration 110, loss = 0.15947701\n",
            "Iteration 111, loss = 0.15851553\n",
            "Iteration 112, loss = 0.15599872\n",
            "Iteration 113, loss = 0.15541385\n",
            "Iteration 114, loss = 0.15575302\n",
            "Iteration 115, loss = 0.15168870\n",
            "Iteration 116, loss = 0.15256886\n",
            "Iteration 117, loss = 0.15195254\n",
            "Iteration 118, loss = 0.15127670\n",
            "Iteration 119, loss = 0.15169276\n",
            "Iteration 120, loss = 0.15261157\n",
            "Iteration 121, loss = 0.14910038\n",
            "Iteration 122, loss = 0.15030487\n",
            "Iteration 123, loss = 0.14905559\n",
            "Iteration 124, loss = 0.14915103\n",
            "Iteration 125, loss = 0.14811622\n",
            "Iteration 126, loss = 0.14853481\n",
            "Iteration 127, loss = 0.14722107\n",
            "Iteration 128, loss = 0.14688575\n",
            "Iteration 129, loss = 0.14700311\n",
            "Iteration 130, loss = 0.14569209\n",
            "Iteration 131, loss = 0.14444936\n",
            "Iteration 132, loss = 0.14670760\n",
            "Iteration 133, loss = 0.14548090\n",
            "Iteration 134, loss = 0.14384168\n",
            "Iteration 135, loss = 0.14269907\n",
            "Iteration 136, loss = 0.14349043\n",
            "Iteration 137, loss = 0.14447340\n",
            "Iteration 138, loss = 0.14470292\n",
            "Iteration 139, loss = 0.14058664\n",
            "Iteration 140, loss = 0.14167290\n",
            "Iteration 141, loss = 0.14192341\n",
            "Iteration 142, loss = 0.14148032\n",
            "Iteration 143, loss = 0.14067062\n",
            "Iteration 144, loss = 0.13894293\n",
            "Iteration 145, loss = 0.14019089\n",
            "Iteration 146, loss = 0.13763722\n",
            "Iteration 147, loss = 0.13909889\n",
            "Iteration 148, loss = 0.13838870\n",
            "Iteration 149, loss = 0.13898843\n",
            "Iteration 150, loss = 0.13913567\n",
            "Iteration 151, loss = 0.13838842\n",
            "Iteration 152, loss = 0.13705385\n",
            "Iteration 153, loss = 0.13847662\n",
            "Iteration 154, loss = 0.13629645\n",
            "Iteration 155, loss = 0.13588186\n",
            "Iteration 156, loss = 0.13659023\n",
            "Iteration 157, loss = 0.13747994\n",
            "Iteration 158, loss = 0.13509841\n",
            "Iteration 159, loss = 0.13388995\n",
            "Iteration 160, loss = 0.13172691\n",
            "Iteration 161, loss = 0.13213625\n",
            "Iteration 162, loss = 0.13370797\n",
            "Iteration 163, loss = 0.13331006\n",
            "Iteration 164, loss = 0.13211976\n",
            "Iteration 165, loss = 0.13321698\n",
            "Iteration 166, loss = 0.13211232\n",
            "Iteration 167, loss = 0.13129529\n",
            "Iteration 168, loss = 0.13150708\n",
            "Iteration 169, loss = 0.13206248\n",
            "Iteration 170, loss = 0.13122465\n",
            "Iteration 171, loss = 0.13155837\n",
            "Iteration 172, loss = 0.12917894\n",
            "Iteration 173, loss = 0.12994013\n",
            "Iteration 174, loss = 0.13010337\n",
            "Iteration 175, loss = 0.12896377\n",
            "Iteration 176, loss = 0.13051310\n",
            "Iteration 177, loss = 0.13118239\n",
            "Iteration 178, loss = 0.13152492\n",
            "Iteration 179, loss = 0.12945441\n",
            "Iteration 180, loss = 0.12727341\n",
            "Iteration 181, loss = 0.12843519\n",
            "Iteration 182, loss = 0.12773199\n",
            "Iteration 183, loss = 0.13129867\n",
            "Iteration 184, loss = 0.12884574\n",
            "Iteration 185, loss = 0.12924721\n",
            "Iteration 186, loss = 0.12675881\n",
            "Iteration 187, loss = 0.12444539\n",
            "Iteration 188, loss = 0.12349872\n",
            "Iteration 189, loss = 0.12638623\n",
            "Iteration 190, loss = 0.12578924\n",
            "Iteration 191, loss = 0.12484337\n",
            "Iteration 192, loss = 0.12406461\n",
            "Iteration 193, loss = 0.12515727\n",
            "Iteration 194, loss = 0.12484872\n",
            "Iteration 195, loss = 0.12567460\n",
            "Iteration 196, loss = 0.12440239\n",
            "Iteration 197, loss = 0.12277614\n",
            "Iteration 198, loss = 0.12183086\n",
            "Iteration 199, loss = 0.12348511\n",
            "Iteration 200, loss = 0.12288215\n",
            "Iteration 201, loss = 0.12553652\n",
            "Iteration 202, loss = 0.12002116\n",
            "Iteration 203, loss = 0.12214674\n",
            "Iteration 204, loss = 0.12184155\n",
            "Iteration 205, loss = 0.12134390\n",
            "Iteration 206, loss = 0.11958703\n",
            "Iteration 207, loss = 0.11913706\n",
            "Iteration 208, loss = 0.12214517\n",
            "Iteration 209, loss = 0.11990715\n",
            "Iteration 210, loss = 0.12558408\n",
            "Iteration 211, loss = 0.12227615\n",
            "Iteration 212, loss = 0.12396528\n",
            "Iteration 213, loss = 0.11961098\n",
            "Iteration 214, loss = 0.12055445\n",
            "Iteration 215, loss = 0.11855986\n",
            "Iteration 216, loss = 0.12205741\n",
            "Iteration 217, loss = 0.11926641\n",
            "Iteration 218, loss = 0.11943470\n",
            "Iteration 219, loss = 0.11956766\n",
            "Iteration 220, loss = 0.11958308\n",
            "Iteration 221, loss = 0.11647426\n",
            "Iteration 222, loss = 0.12008564\n",
            "Iteration 223, loss = 0.11862863\n",
            "Iteration 224, loss = 0.11585782\n",
            "Iteration 225, loss = 0.11877561\n",
            "Iteration 226, loss = 0.11887238\n",
            "Iteration 227, loss = 0.11419568\n",
            "Iteration 228, loss = 0.11599176\n",
            "Iteration 229, loss = 0.11525606\n",
            "Iteration 230, loss = 0.11565499\n",
            "Iteration 231, loss = 0.11481341\n",
            "Iteration 232, loss = 0.11667415\n",
            "Iteration 233, loss = 0.11589853\n",
            "Iteration 234, loss = 0.11733055\n",
            "Iteration 235, loss = 0.11965017\n",
            "Iteration 236, loss = 0.12134146\n",
            "Iteration 237, loss = 0.11855901\n",
            "Iteration 238, loss = 0.11215821\n",
            "Iteration 239, loss = 0.11479387\n",
            "Iteration 240, loss = 0.11435884\n",
            "Iteration 241, loss = 0.11283378\n",
            "Iteration 242, loss = 0.11339349\n",
            "Iteration 243, loss = 0.11276543\n",
            "Iteration 244, loss = 0.11305030\n",
            "Iteration 245, loss = 0.11600918\n",
            "Iteration 246, loss = 0.11409098\n",
            "Iteration 247, loss = 0.11584748\n",
            "Iteration 248, loss = 0.11156696\n",
            "Iteration 249, loss = 0.11559977\n",
            "Iteration 250, loss = 0.11191892\n",
            "Iteration 251, loss = 0.10994041\n",
            "Iteration 252, loss = 0.11295094\n",
            "Iteration 253, loss = 0.11140619\n",
            "Iteration 254, loss = 0.11312922\n",
            "Iteration 255, loss = 0.11384891\n",
            "Iteration 256, loss = 0.11064659\n",
            "Iteration 257, loss = 0.10979797\n",
            "Iteration 258, loss = 0.11205984\n",
            "Iteration 259, loss = 0.11047027\n",
            "Iteration 260, loss = 0.11108333\n",
            "Iteration 261, loss = 0.10947489\n",
            "Iteration 262, loss = 0.10824806\n",
            "Iteration 263, loss = 0.10939871\n",
            "Iteration 264, loss = 0.10983909\n",
            "Iteration 265, loss = 0.11044956\n",
            "Iteration 266, loss = 0.10924019\n",
            "Iteration 267, loss = 0.11207193\n",
            "Iteration 268, loss = 0.10861067\n",
            "Iteration 269, loss = 0.11026398\n",
            "Iteration 270, loss = 0.10863409\n",
            "Iteration 271, loss = 0.11114641\n",
            "Iteration 272, loss = 0.10877599\n",
            "Iteration 273, loss = 0.10819432\n",
            "Iteration 274, loss = 0.10867881\n",
            "Iteration 275, loss = 0.10668437\n",
            "Iteration 276, loss = 0.10856190\n",
            "Iteration 277, loss = 0.10824479\n",
            "Iteration 278, loss = 0.10695516\n",
            "Iteration 279, loss = 0.10832312\n",
            "Iteration 280, loss = 0.10637499\n",
            "Iteration 281, loss = 0.11002227\n",
            "Iteration 282, loss = 0.10651774\n",
            "Iteration 283, loss = 0.10685583\n",
            "Iteration 284, loss = 0.10779035\n",
            "Iteration 285, loss = 0.10872482\n",
            "Iteration 286, loss = 0.10650320\n",
            "Iteration 287, loss = 0.10458040\n",
            "Iteration 288, loss = 0.10531115\n",
            "Iteration 289, loss = 0.10587187\n",
            "Iteration 290, loss = 0.10676892\n",
            "Iteration 291, loss = 0.10476460\n",
            "Iteration 292, loss = 0.10732931\n",
            "Iteration 293, loss = 0.10477040\n",
            "Iteration 294, loss = 0.10410727\n",
            "Iteration 295, loss = 0.10566193\n",
            "Iteration 296, loss = 0.10643932\n",
            "Iteration 297, loss = 0.10487772\n",
            "Iteration 298, loss = 0.10323677\n",
            "Iteration 299, loss = 0.10409356\n",
            "Iteration 300, loss = 0.10342763\n",
            "Iteration 301, loss = 0.10359189\n",
            "Iteration 302, loss = 0.10627339\n",
            "Iteration 303, loss = 0.10338536\n",
            "Iteration 304, loss = 0.10451086\n",
            "Iteration 305, loss = 0.10672870\n",
            "Iteration 306, loss = 0.10674558\n",
            "Iteration 307, loss = 0.10467839\n",
            "Iteration 308, loss = 0.10346575\n",
            "Iteration 309, loss = 0.10597004\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8209206272032492"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}