{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python382jvsc74a57bd019dd5068fd4c406359a86f27249fc517a25f1ed5fa0eb2341423f3354c0b7639",
      "display_name": "Python 3.8.2 32-bit"
    },
    "colab": {
      "name": "UCI_adult_predict.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "thsFFBMxfMHz"
      },
      "source": [
        "#Bibliotécas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiAfKFXzfMH_"
      },
      "source": [
        "#DataFrame\n",
        "headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "\n",
        "df = pd.read_csv('adult.data.csv', names=headers)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUsKO6ZRz8_e"
      },
      "source": [
        "#Tratamento de dados faltantes para o mais representatívo\n",
        "columns = df.columns\n",
        "for i in columns:\n",
        "    missing = df[i].isin([' ?']).sum()\n",
        "    df[i] = df[i].replace(' ?', np.NaN)\n",
        "df = df.apply(lambda x:x.fillna(x.value_counts().index[0]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzLUMIkrfMIA"
      },
      "source": [
        "#Divisão em Parâmetro e Classe\n",
        "X_df = df.iloc[:, 0:14].values\n",
        "y_df = df.iloc[:, 14].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8__DG0IfMIB"
      },
      "source": [
        "#LabelEncoder\n",
        "def labelencoder(pd_serie):\n",
        "    labelencoder = LabelEncoder()\n",
        "    pd_serie = labelencoder.fit_transform(pd_serie)\n",
        "    return pd_serie\n",
        "\n",
        "X_df[:, 1] = labelencoder(X_df[:, 1])\n",
        "X_df[:, 3] = labelencoder(X_df[:, 3])\n",
        "X_df[:, 5] = labelencoder(X_df[:, 5])\n",
        "X_df[:, 6] = labelencoder(X_df[:, 6])\n",
        "X_df[:, 7] = labelencoder(X_df[:, 7])\n",
        "X_df[:, 8] = labelencoder(X_df[:, 8])\n",
        "X_df[:, 9] = labelencoder(X_df[:, 9])\n",
        "X_df[:, 13] = labelencoder(X_df[:, 13])\n",
        "y_df = labelencoder(y_df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYQn9S19fMIC"
      },
      "source": [
        "#One Hot Encoder\n",
        "onehotencorder = ColumnTransformer(transformers=[(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder='passthrough')\n",
        "X_df = onehotencorder.fit_transform(X_df).toarray()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twXAB1iKfMIC"
      },
      "source": [
        "#Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "X_df = scaler.fit_transform(X_df)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "312HacmbfMID"
      },
      "source": [
        "#Divisão df de treinamento e teste 15%\n",
        "X_df_train, X_df_test, y_df_train, y_df_test =  train_test_split(X_df, y_df, test_size=0.15, random_state=0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8E9ZX2KfMID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c40d44-9b25-4fc4-e711-86ea13a1d55b"
      },
      "source": [
        "print(X_df_train.shape, X_df_test.shape, y_df_train.shape, y_df_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27676, 105) (4885, 105) (27676,) (4885,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cp03L4BfMIF"
      },
      "source": [
        "#kNN\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSiD4blLfMIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb717e3d-83ee-47ef-f0d0-ef2680da6a55"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_df_train, y_df_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sul2sYy8fMIG"
      },
      "source": [
        "predict_knn = knn.predict(X_df_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vovqlq6egJt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39581aea-b6a1-42b6-dc3e-07e0bd3f6e5a"
      },
      "source": [
        "accuracy_score(y_df_test, predict_knn)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.828863868986694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiWW-LhtgqYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e68d59d-67a8-4a7d-b9ee-7131de4d2fc3"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_knn)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3439,  254],\n",
              "       [ 582,  610]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAWrWQMGg8S6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14888d2-941d-401a-cb96-b40f83aed33a"
      },
      "source": [
        "print(classification_report(y_df_test, predict_knn))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89      3693\n",
            "           1       0.71      0.51      0.59      1192\n",
            "\n",
            "    accuracy                           0.83      4885\n",
            "   macro avg       0.78      0.72      0.74      4885\n",
            "weighted avg       0.82      0.83      0.82      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TPpC38rfMIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16b633a-9624-484d-d4fc-2147f0134752"
      },
      "source": [
        "#cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "score_knn = cross_val_score(knn, X_df, y_df, cv = kfold)\n",
        "score_knn.mean()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8293354928602088"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLrgqQuNAb8D"
      },
      "source": [
        "#hold\n",
        "knn_hold = knn.fit(X_df, y_df)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYvd9aSofMIH"
      },
      "source": [
        "#random forest\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFY_Lg6mHBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f166fa71-1c5d-4d80-c0a9-1af91a704d30"
      },
      "source": [
        "random_forest = RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_leaf=1, min_samples_split=5, random_state=0)\n",
        "random_forest.fit(X_df_train, y_df_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=5,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfSNGu-7mlJ9"
      },
      "source": [
        "predict_rf = random_forest.predict(X_df_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmHSc6sXtpFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646fa4cc-0751-4cf7-9345-5f7deb89d6b9"
      },
      "source": [
        "accuracy_score(y_df_test, predict_rf)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8534288638689866"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOz-SI0DtpqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c206f63-e13a-453b-cd62-c32b5a1f5593"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_rf)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3430,  263],\n",
              "       [ 453,  739]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqmaUQ2ItsGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0883ecd-b390-4332-c110-eb7fba623c58"
      },
      "source": [
        "print(classification_report(y_df_test, predict_rf))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.91      3693\n",
            "           1       0.74      0.62      0.67      1192\n",
            "\n",
            "    accuracy                           0.85      4885\n",
            "   macro avg       0.81      0.77      0.79      4885\n",
            "weighted avg       0.85      0.85      0.85      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBlGlJqdG07E",
        "outputId": "8d2ed68d-0fb8-48ab-c8cf-4cf9cb07c97a"
      },
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "score_random_forest = cross_val_score(random_forest, X_df, y_df, cv = kfold)\n",
        "score_random_forest.mean()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8584809395601536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnvaPz54A5Km"
      },
      "source": [
        "#hold\n",
        "random_forest_hold = random_forest.fit(X_df, y_df)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk7yxJ3FtyNB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2eaf62d-b21d-4fbd-f60d-dbd9e982ca4c"
      },
      "source": [
        "'''#MLP\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#MLP\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34mRORBC0up4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8d060f94-88c2-48a6-dee8-98664cbf5448"
      },
      "source": [
        "'''mlp_keras = Sequential()\n",
        "mlp_keras.add(Dense(units=55, activation='relu', input_dim=108))\n",
        "mlp_keras.add(Dense(units=55, activation='relu'))\n",
        "mlp_keras.add(Dense(units=1, activation='sigmoid'))\n",
        "mlp_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])'''"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"mlp_keras = Sequential()\\nmlp_keras.add(Dense(units=55, activation='relu', input_dim=108))\\nmlp_keras.add(Dense(units=55, activation='relu'))\\nmlp_keras.add(Dense(units=1, activation='sigmoid'))\\nmlp_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "redL5hNs0wqQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60d7f588-6549-4358-d08e-f79f3a88f7d2"
      },
      "source": [
        "'''mlp_keras.fit(X_df_train, y_df_train, batch_size=10, epochs=100)'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mlp_keras.fit(X_df_train, y_df_train, batch_size=10, epochs=100)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSSLWtIg04HP"
      },
      "source": [
        "#predict_mlp_keras = mlp_keras.predict(X_df_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_80233Kd46qr"
      },
      "source": [
        "#predict_mlp_keras = (predict_mlp_keras > 0.5)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RsY_UwO4mVx"
      },
      "source": [
        "#accuracy_score(y_df_test, predict_mlp_keras)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQhFZaj24wXe"
      },
      "source": [
        "#confusion_matrix(y_df_test, predict_mlp_keras)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5lzhM7b5Hth"
      },
      "source": [
        "#print(classification_report(y_df_test, predict_mlp_keras ))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxd-waKZ5UFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ceb00d-740c-4497-e99c-9c2351d6243d"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(verbose=True, max_iter=1000, tol=0.000010, hidden_layer_sizes=(100, 100))\n",
        "mlp.fit(X_df_train, y_df_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.37726211\n",
            "Iteration 2, loss = 0.32186687\n",
            "Iteration 3, loss = 0.31141765\n",
            "Iteration 4, loss = 0.30399089\n",
            "Iteration 5, loss = 0.29862869\n",
            "Iteration 6, loss = 0.29460698\n",
            "Iteration 7, loss = 0.29163226\n",
            "Iteration 8, loss = 0.28734696\n",
            "Iteration 9, loss = 0.28483126\n",
            "Iteration 10, loss = 0.28341999\n",
            "Iteration 11, loss = 0.28030077\n",
            "Iteration 12, loss = 0.27810648\n",
            "Iteration 13, loss = 0.27394857\n",
            "Iteration 14, loss = 0.27267237\n",
            "Iteration 15, loss = 0.26867806\n",
            "Iteration 16, loss = 0.26699974\n",
            "Iteration 17, loss = 0.26427733\n",
            "Iteration 18, loss = 0.26176899\n",
            "Iteration 19, loss = 0.25985553\n",
            "Iteration 20, loss = 0.25590494\n",
            "Iteration 21, loss = 0.25496811\n",
            "Iteration 22, loss = 0.25211283\n",
            "Iteration 23, loss = 0.24918829\n",
            "Iteration 24, loss = 0.24734152\n",
            "Iteration 25, loss = 0.24465921\n",
            "Iteration 26, loss = 0.24354974\n",
            "Iteration 27, loss = 0.23997196\n",
            "Iteration 28, loss = 0.23830033\n",
            "Iteration 29, loss = 0.23562037\n",
            "Iteration 30, loss = 0.23209469\n",
            "Iteration 31, loss = 0.23229862\n",
            "Iteration 32, loss = 0.22888173\n",
            "Iteration 33, loss = 0.22727739\n",
            "Iteration 34, loss = 0.22486066\n",
            "Iteration 35, loss = 0.22341454\n",
            "Iteration 36, loss = 0.22227807\n",
            "Iteration 37, loss = 0.22019527\n",
            "Iteration 38, loss = 0.21839468\n",
            "Iteration 39, loss = 0.21477163\n",
            "Iteration 40, loss = 0.21525870\n",
            "Iteration 41, loss = 0.21291586\n",
            "Iteration 42, loss = 0.21171810\n",
            "Iteration 43, loss = 0.20861386\n",
            "Iteration 44, loss = 0.20750006\n",
            "Iteration 45, loss = 0.20635469\n",
            "Iteration 46, loss = 0.20360095\n",
            "Iteration 47, loss = 0.20287832\n",
            "Iteration 48, loss = 0.20186041\n",
            "Iteration 49, loss = 0.20000358\n",
            "Iteration 50, loss = 0.19921880\n",
            "Iteration 51, loss = 0.19658366\n",
            "Iteration 52, loss = 0.19606128\n",
            "Iteration 53, loss = 0.19430596\n",
            "Iteration 54, loss = 0.19351362\n",
            "Iteration 55, loss = 0.19267798\n",
            "Iteration 56, loss = 0.19222881\n",
            "Iteration 57, loss = 0.18938104\n",
            "Iteration 58, loss = 0.18772166\n",
            "Iteration 59, loss = 0.18615009\n",
            "Iteration 60, loss = 0.18701517\n",
            "Iteration 61, loss = 0.18398074\n",
            "Iteration 62, loss = 0.18276800\n",
            "Iteration 63, loss = 0.18334215\n",
            "Iteration 64, loss = 0.18342084\n",
            "Iteration 65, loss = 0.18353488\n",
            "Iteration 66, loss = 0.17975193\n",
            "Iteration 67, loss = 0.17912210\n",
            "Iteration 68, loss = 0.17862098\n",
            "Iteration 69, loss = 0.17606444\n",
            "Iteration 70, loss = 0.17477861\n",
            "Iteration 71, loss = 0.17464968\n",
            "Iteration 72, loss = 0.17278218\n",
            "Iteration 73, loss = 0.17103877\n",
            "Iteration 74, loss = 0.17129578\n",
            "Iteration 75, loss = 0.17303875\n",
            "Iteration 76, loss = 0.17081226\n",
            "Iteration 77, loss = 0.17037972\n",
            "Iteration 78, loss = 0.16868026\n",
            "Iteration 79, loss = 0.16889132\n",
            "Iteration 80, loss = 0.16700178\n",
            "Iteration 81, loss = 0.16682932\n",
            "Iteration 82, loss = 0.16568683\n",
            "Iteration 83, loss = 0.16361615\n",
            "Iteration 84, loss = 0.16330403\n",
            "Iteration 85, loss = 0.16214464\n",
            "Iteration 86, loss = 0.16146149\n",
            "Iteration 87, loss = 0.15994383\n",
            "Iteration 88, loss = 0.16018199\n",
            "Iteration 89, loss = 0.15949187\n",
            "Iteration 90, loss = 0.15818316\n",
            "Iteration 91, loss = 0.15937110\n",
            "Iteration 92, loss = 0.15653051\n",
            "Iteration 93, loss = 0.15716475\n",
            "Iteration 94, loss = 0.15497345\n",
            "Iteration 95, loss = 0.15576111\n",
            "Iteration 96, loss = 0.15475251\n",
            "Iteration 97, loss = 0.15357959\n",
            "Iteration 98, loss = 0.15579337\n",
            "Iteration 99, loss = 0.15316018\n",
            "Iteration 100, loss = 0.15360086\n",
            "Iteration 101, loss = 0.15111100\n",
            "Iteration 102, loss = 0.15268792\n",
            "Iteration 103, loss = 0.15124916\n",
            "Iteration 104, loss = 0.15024617\n",
            "Iteration 105, loss = 0.14900600\n",
            "Iteration 106, loss = 0.14862758\n",
            "Iteration 107, loss = 0.14974700\n",
            "Iteration 108, loss = 0.14720053\n",
            "Iteration 109, loss = 0.14533185\n",
            "Iteration 110, loss = 0.14692855\n",
            "Iteration 111, loss = 0.14573136\n",
            "Iteration 112, loss = 0.14669274\n",
            "Iteration 113, loss = 0.14649563\n",
            "Iteration 114, loss = 0.14882246\n",
            "Iteration 115, loss = 0.14446351\n",
            "Iteration 116, loss = 0.14229214\n",
            "Iteration 117, loss = 0.14276767\n",
            "Iteration 118, loss = 0.14432729\n",
            "Iteration 119, loss = 0.14413496\n",
            "Iteration 120, loss = 0.14200657\n",
            "Iteration 121, loss = 0.14204954\n",
            "Iteration 122, loss = 0.14065243\n",
            "Iteration 123, loss = 0.14125224\n",
            "Iteration 124, loss = 0.13969409\n",
            "Iteration 125, loss = 0.13966404\n",
            "Iteration 126, loss = 0.13804123\n",
            "Iteration 127, loss = 0.13727011\n",
            "Iteration 128, loss = 0.13846888\n",
            "Iteration 129, loss = 0.13734009\n",
            "Iteration 130, loss = 0.13735431\n",
            "Iteration 131, loss = 0.13542659\n",
            "Iteration 132, loss = 0.13799445\n",
            "Iteration 133, loss = 0.13566977\n",
            "Iteration 134, loss = 0.13554503\n",
            "Iteration 135, loss = 0.13478809\n",
            "Iteration 136, loss = 0.13357466\n",
            "Iteration 137, loss = 0.13355032\n",
            "Iteration 138, loss = 0.13798707\n",
            "Iteration 139, loss = 0.13331538\n",
            "Iteration 140, loss = 0.13274695\n",
            "Iteration 141, loss = 0.13562934\n",
            "Iteration 142, loss = 0.13337554\n",
            "Iteration 143, loss = 0.13309711\n",
            "Iteration 144, loss = 0.13203537\n",
            "Iteration 145, loss = 0.13259732\n",
            "Iteration 146, loss = 0.13329185\n",
            "Iteration 147, loss = 0.13093086\n",
            "Iteration 148, loss = 0.12754483\n",
            "Iteration 149, loss = 0.13006034\n",
            "Iteration 150, loss = 0.12738729\n",
            "Iteration 151, loss = 0.12902899\n",
            "Iteration 152, loss = 0.12868011\n",
            "Iteration 153, loss = 0.13079626\n",
            "Iteration 154, loss = 0.12753540\n",
            "Iteration 155, loss = 0.12937711\n",
            "Iteration 156, loss = 0.12699770\n",
            "Iteration 157, loss = 0.12962472\n",
            "Iteration 158, loss = 0.12727410\n",
            "Iteration 159, loss = 0.12769116\n",
            "Iteration 160, loss = 0.12844448\n",
            "Iteration 161, loss = 0.12494599\n",
            "Iteration 162, loss = 0.12666592\n",
            "Iteration 163, loss = 0.12339638\n",
            "Iteration 164, loss = 0.12412974\n",
            "Iteration 165, loss = 0.12471135\n",
            "Iteration 166, loss = 0.12469842\n",
            "Iteration 167, loss = 0.12486398\n",
            "Iteration 168, loss = 0.12352835\n",
            "Iteration 169, loss = 0.12279762\n",
            "Iteration 170, loss = 0.12203534\n",
            "Iteration 171, loss = 0.12215238\n",
            "Iteration 172, loss = 0.12348470\n",
            "Iteration 173, loss = 0.12153666\n",
            "Iteration 174, loss = 0.12219039\n",
            "Iteration 175, loss = 0.12179929\n",
            "Iteration 176, loss = 0.12171035\n",
            "Iteration 177, loss = 0.12034109\n",
            "Iteration 178, loss = 0.12050736\n",
            "Iteration 179, loss = 0.11900356\n",
            "Iteration 180, loss = 0.12073721\n",
            "Iteration 181, loss = 0.11993192\n",
            "Iteration 182, loss = 0.12004517\n",
            "Iteration 183, loss = 0.12098857\n",
            "Iteration 184, loss = 0.11777462\n",
            "Iteration 185, loss = 0.12018729\n",
            "Iteration 186, loss = 0.11801443\n",
            "Iteration 187, loss = 0.11960566\n",
            "Iteration 188, loss = 0.11785214\n",
            "Iteration 189, loss = 0.11829429\n",
            "Iteration 190, loss = 0.12063069\n",
            "Iteration 191, loss = 0.12073401\n",
            "Iteration 192, loss = 0.11823981\n",
            "Iteration 193, loss = 0.11809009\n",
            "Iteration 194, loss = 0.12174977\n",
            "Iteration 195, loss = 0.11733547\n",
            "Iteration 196, loss = 0.11452394\n",
            "Iteration 197, loss = 0.11609659\n",
            "Iteration 198, loss = 0.11411252\n",
            "Iteration 199, loss = 0.11613680\n",
            "Iteration 200, loss = 0.11635143\n",
            "Iteration 201, loss = 0.11546029\n",
            "Iteration 202, loss = 0.11541474\n",
            "Iteration 203, loss = 0.11905518\n",
            "Iteration 204, loss = 0.11398445\n",
            "Iteration 205, loss = 0.11227627\n",
            "Iteration 206, loss = 0.11260320\n",
            "Iteration 207, loss = 0.11212353\n",
            "Iteration 208, loss = 0.11308601\n",
            "Iteration 209, loss = 0.11321867\n",
            "Iteration 210, loss = 0.11149247\n",
            "Iteration 211, loss = 0.11292729\n",
            "Iteration 212, loss = 0.11209728\n",
            "Iteration 213, loss = 0.11203569\n",
            "Iteration 214, loss = 0.11646652\n",
            "Iteration 215, loss = 0.11325076\n",
            "Iteration 216, loss = 0.11426110\n",
            "Iteration 217, loss = 0.10901313\n",
            "Iteration 218, loss = 0.11128526\n",
            "Iteration 219, loss = 0.11084593\n",
            "Iteration 220, loss = 0.10978699\n",
            "Iteration 221, loss = 0.11098118\n",
            "Iteration 222, loss = 0.10977088\n",
            "Iteration 223, loss = 0.10851004\n",
            "Iteration 224, loss = 0.10814560\n",
            "Iteration 225, loss = 0.10992807\n",
            "Iteration 226, loss = 0.11022665\n",
            "Iteration 227, loss = 0.11178726\n",
            "Iteration 228, loss = 0.11205318\n",
            "Iteration 229, loss = 0.10755974\n",
            "Iteration 230, loss = 0.10669707\n",
            "Iteration 231, loss = 0.10750990\n",
            "Iteration 232, loss = 0.10729026\n",
            "Iteration 233, loss = 0.10877411\n",
            "Iteration 234, loss = 0.10878370\n",
            "Iteration 235, loss = 0.10895015\n",
            "Iteration 236, loss = 0.10773120\n",
            "Iteration 237, loss = 0.10799771\n",
            "Iteration 238, loss = 0.11005265\n",
            "Iteration 239, loss = 0.10849668\n",
            "Iteration 240, loss = 0.10528511\n",
            "Iteration 241, loss = 0.10529650\n",
            "Iteration 242, loss = 0.10590234\n",
            "Iteration 243, loss = 0.10682464\n",
            "Iteration 244, loss = 0.10645213\n",
            "Iteration 245, loss = 0.10513112\n",
            "Iteration 246, loss = 0.10719059\n",
            "Iteration 247, loss = 0.10942590\n",
            "Iteration 248, loss = 0.10680954\n",
            "Iteration 249, loss = 0.10567402\n",
            "Iteration 250, loss = 0.10306851\n",
            "Iteration 251, loss = 0.10605304\n",
            "Iteration 252, loss = 0.10498800\n",
            "Iteration 253, loss = 0.10440666\n",
            "Iteration 254, loss = 0.10394870\n",
            "Iteration 255, loss = 0.10245555\n",
            "Iteration 256, loss = 0.10465021\n",
            "Iteration 257, loss = 0.10589497\n",
            "Iteration 258, loss = 0.10271610\n",
            "Iteration 259, loss = 0.10671122\n",
            "Iteration 260, loss = 0.10469109\n",
            "Iteration 261, loss = 0.10525464\n",
            "Iteration 262, loss = inf\n",
            "Iteration 263, loss = 0.10353568\n",
            "Iteration 264, loss = 0.10259979\n",
            "Iteration 265, loss = 0.10334219\n",
            "Iteration 266, loss = 0.10488401\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECLqYWNY5w3s"
      },
      "source": [
        "predict_mlp = mlp.predict(X_df_test)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kpdAtSk6doS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2a9684-d73a-4cd7-9f38-e6aa2610994a"
      },
      "source": [
        "accuracy_score(y_df_test, predict_mlp)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8151484135107472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAWSlA476mF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961c1916-da2b-4746-f000-41899f377d13"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_mlp)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3260,  433],\n",
              "       [ 470,  722]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG4t-kps6q4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64937549-4a58-4da6-b57d-978cbdff1269"
      },
      "source": [
        "print(classification_report(y_df_test, predict_mlp))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.88      3693\n",
            "           1       0.63      0.61      0.62      1192\n",
            "\n",
            "    accuracy                           0.82      4885\n",
            "   macro avg       0.75      0.74      0.75      4885\n",
            "weighted avg       0.81      0.82      0.81      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85RO4hLp6tTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97d1e6f-ce1b-4da7-a025-8484b0c70ee8"
      },
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "score_mlp = cross_val_score(mlp, X_df, y_df, cv = kfold)\n",
        "score_mlp.mean()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.36347327\n",
            "Iteration 2, loss = 0.32193810\n",
            "Iteration 3, loss = 0.31005543\n",
            "Iteration 4, loss = 0.30355876\n",
            "Iteration 5, loss = 0.29873867\n",
            "Iteration 6, loss = 0.29359691\n",
            "Iteration 7, loss = 0.29080275\n",
            "Iteration 8, loss = 0.28755749\n",
            "Iteration 9, loss = 0.28460970\n",
            "Iteration 10, loss = 0.28232195\n",
            "Iteration 11, loss = 0.27945129\n",
            "Iteration 12, loss = 0.27766088\n",
            "Iteration 13, loss = 0.27497488\n",
            "Iteration 14, loss = 0.27229352\n",
            "Iteration 15, loss = 0.26949178\n",
            "Iteration 16, loss = 0.26807114\n",
            "Iteration 17, loss = 0.26519590\n",
            "Iteration 18, loss = 0.26283280\n",
            "Iteration 19, loss = 0.26062176\n",
            "Iteration 20, loss = 0.25958707\n",
            "Iteration 21, loss = 0.25556274\n",
            "Iteration 22, loss = 0.25414328\n",
            "Iteration 23, loss = 0.25220389\n",
            "Iteration 24, loss = 0.24947748\n",
            "Iteration 25, loss = 0.24727533\n",
            "Iteration 26, loss = 0.24593682\n",
            "Iteration 27, loss = 0.24345768\n",
            "Iteration 28, loss = 0.24119102\n",
            "Iteration 29, loss = 0.23878119\n",
            "Iteration 30, loss = 0.23850622\n",
            "Iteration 31, loss = 0.23508201\n",
            "Iteration 32, loss = 0.23297610\n",
            "Iteration 33, loss = 0.23200388\n",
            "Iteration 34, loss = 0.22971422\n",
            "Iteration 35, loss = 0.22669005\n",
            "Iteration 36, loss = 0.22607379\n",
            "Iteration 37, loss = 0.22434387\n",
            "Iteration 38, loss = 0.22349320\n",
            "Iteration 39, loss = 0.22087495\n",
            "Iteration 40, loss = 0.21977271\n",
            "Iteration 41, loss = 0.21725207\n",
            "Iteration 42, loss = 0.21647092\n",
            "Iteration 43, loss = 0.21477021\n",
            "Iteration 44, loss = 0.21329875\n",
            "Iteration 45, loss = 0.21044122\n",
            "Iteration 46, loss = 0.21019087\n",
            "Iteration 47, loss = 0.20669588\n",
            "Iteration 48, loss = 0.20673898\n",
            "Iteration 49, loss = 0.20433425\n",
            "Iteration 50, loss = 0.20396219\n",
            "Iteration 51, loss = 0.20286211\n",
            "Iteration 52, loss = 0.20001750\n",
            "Iteration 53, loss = 0.19897771\n",
            "Iteration 54, loss = 0.19909080\n",
            "Iteration 55, loss = 0.19830696\n",
            "Iteration 56, loss = 0.19632056\n",
            "Iteration 57, loss = 0.19479005\n",
            "Iteration 58, loss = 0.19369445\n",
            "Iteration 59, loss = 0.19303166\n",
            "Iteration 60, loss = 0.19139420\n",
            "Iteration 61, loss = 0.19157325\n",
            "Iteration 62, loss = 0.19005725\n",
            "Iteration 63, loss = 0.19043008\n",
            "Iteration 64, loss = 0.19011649\n",
            "Iteration 65, loss = 0.18656014\n",
            "Iteration 66, loss = 0.18477744\n",
            "Iteration 67, loss = 0.18466996\n",
            "Iteration 68, loss = 0.18283851\n",
            "Iteration 69, loss = 0.18267634\n",
            "Iteration 70, loss = 0.18284055\n",
            "Iteration 71, loss = 0.18196832\n",
            "Iteration 72, loss = 0.18031732\n",
            "Iteration 73, loss = 0.17940620\n",
            "Iteration 74, loss = 0.17913985\n",
            "Iteration 75, loss = 0.17624395\n",
            "Iteration 76, loss = 0.17881739\n",
            "Iteration 77, loss = 0.17626607\n",
            "Iteration 78, loss = 0.17607253\n",
            "Iteration 79, loss = 0.17373341\n",
            "Iteration 80, loss = 0.17488163\n",
            "Iteration 81, loss = 0.17304653\n",
            "Iteration 82, loss = 0.17205056\n",
            "Iteration 83, loss = 0.17151102\n",
            "Iteration 84, loss = 0.16998175\n",
            "Iteration 85, loss = 0.16722014\n",
            "Iteration 86, loss = 0.16787375\n",
            "Iteration 87, loss = 0.16796961\n",
            "Iteration 88, loss = 0.16806604\n",
            "Iteration 89, loss = 0.16795216\n",
            "Iteration 90, loss = 0.16586354\n",
            "Iteration 91, loss = 0.16466406\n",
            "Iteration 92, loss = 0.16365694\n",
            "Iteration 93, loss = 0.16334833\n",
            "Iteration 94, loss = 0.16252155\n",
            "Iteration 95, loss = 0.16354736\n",
            "Iteration 96, loss = 0.16323508\n",
            "Iteration 97, loss = 0.16281160\n",
            "Iteration 98, loss = 0.16044545\n",
            "Iteration 99, loss = 0.16008376\n",
            "Iteration 100, loss = 0.16119226\n",
            "Iteration 101, loss = 0.15819460\n",
            "Iteration 102, loss = 0.16054758\n",
            "Iteration 103, loss = 0.15787342\n",
            "Iteration 104, loss = 0.15720317\n",
            "Iteration 105, loss = 0.15655413\n",
            "Iteration 106, loss = 0.15795596\n",
            "Iteration 107, loss = 0.15837685\n",
            "Iteration 108, loss = 0.15661464\n",
            "Iteration 109, loss = 0.15303268\n",
            "Iteration 110, loss = 0.15286075\n",
            "Iteration 111, loss = 0.15244075\n",
            "Iteration 112, loss = 0.15316036\n",
            "Iteration 113, loss = 0.15274864\n",
            "Iteration 114, loss = 0.15189522\n",
            "Iteration 115, loss = 0.15158203\n",
            "Iteration 116, loss = 0.15006719\n",
            "Iteration 117, loss = 0.15127122\n",
            "Iteration 118, loss = 0.15126333\n",
            "Iteration 119, loss = 0.14994526\n",
            "Iteration 120, loss = 0.14891700\n",
            "Iteration 121, loss = 0.14791789\n",
            "Iteration 122, loss = 0.14615924\n",
            "Iteration 123, loss = 0.14528403\n",
            "Iteration 124, loss = 0.14651494\n",
            "Iteration 125, loss = 0.14777086\n",
            "Iteration 126, loss = 0.14564364\n",
            "Iteration 127, loss = 0.14489856\n",
            "Iteration 128, loss = 0.14633826\n",
            "Iteration 129, loss = 0.14437081\n",
            "Iteration 130, loss = 0.14255501\n",
            "Iteration 131, loss = 0.14549537\n",
            "Iteration 132, loss = 0.14324777\n",
            "Iteration 133, loss = 0.14292934\n",
            "Iteration 134, loss = 0.14142438\n",
            "Iteration 135, loss = 0.14265365\n",
            "Iteration 136, loss = 0.14197043\n",
            "Iteration 137, loss = 0.13910284\n",
            "Iteration 138, loss = 0.14000364\n",
            "Iteration 139, loss = 0.14221238\n",
            "Iteration 140, loss = 0.13980707\n",
            "Iteration 141, loss = 0.14137548\n",
            "Iteration 142, loss = 0.13831482\n",
            "Iteration 143, loss = 0.13778219\n",
            "Iteration 144, loss = 0.13901428\n",
            "Iteration 145, loss = 0.14300558\n",
            "Iteration 146, loss = 0.13732109\n",
            "Iteration 147, loss = 0.13863875\n",
            "Iteration 148, loss = 0.13637144\n",
            "Iteration 149, loss = 0.13747060\n",
            "Iteration 150, loss = 0.13513586\n",
            "Iteration 151, loss = 0.13597544\n",
            "Iteration 152, loss = 0.13483127\n",
            "Iteration 153, loss = 0.13364935\n",
            "Iteration 154, loss = 0.13420406\n",
            "Iteration 155, loss = 0.13490328\n",
            "Iteration 156, loss = 0.13436690\n",
            "Iteration 157, loss = 0.13236060\n",
            "Iteration 158, loss = 0.13394569\n",
            "Iteration 159, loss = 0.13278438\n",
            "Iteration 160, loss = 0.13172701\n",
            "Iteration 161, loss = 0.13272111\n",
            "Iteration 162, loss = 0.13148491\n",
            "Iteration 163, loss = 0.13326822\n",
            "Iteration 164, loss = 0.13146495\n",
            "Iteration 165, loss = 0.13187076\n",
            "Iteration 166, loss = 0.13266151\n",
            "Iteration 167, loss = 0.13151707\n",
            "Iteration 168, loss = 0.13038801\n",
            "Iteration 169, loss = 0.12826139\n",
            "Iteration 170, loss = 0.12916440\n",
            "Iteration 171, loss = 0.12719871\n",
            "Iteration 172, loss = 0.13032669\n",
            "Iteration 173, loss = 0.13025251\n",
            "Iteration 174, loss = 0.12748978\n",
            "Iteration 175, loss = 0.12879252\n",
            "Iteration 176, loss = 0.12795884\n",
            "Iteration 177, loss = 0.13138131\n",
            "Iteration 178, loss = 0.13080299\n",
            "Iteration 179, loss = 0.13103664\n",
            "Iteration 180, loss = 0.12931513\n",
            "Iteration 181, loss = 0.12954127\n",
            "Iteration 182, loss = 0.12678939\n",
            "Iteration 183, loss = 0.12648351\n",
            "Iteration 184, loss = 0.12443236\n",
            "Iteration 185, loss = 0.12548262\n",
            "Iteration 186, loss = 0.12423893\n",
            "Iteration 187, loss = 0.12482589\n",
            "Iteration 188, loss = 0.12464710\n",
            "Iteration 189, loss = 0.12806856\n",
            "Iteration 190, loss = 0.12241028\n",
            "Iteration 191, loss = 0.12347435\n",
            "Iteration 192, loss = 0.12294233\n",
            "Iteration 193, loss = 0.12231515\n",
            "Iteration 194, loss = 0.12308804\n",
            "Iteration 195, loss = 0.12348130\n",
            "Iteration 196, loss = 0.12234111\n",
            "Iteration 197, loss = 0.12238996\n",
            "Iteration 198, loss = 0.12449035\n",
            "Iteration 199, loss = 0.12429440\n",
            "Iteration 200, loss = 0.12218286\n",
            "Iteration 201, loss = 0.12071411\n",
            "Iteration 202, loss = 0.12177733\n",
            "Iteration 203, loss = 0.12423581\n",
            "Iteration 204, loss = 0.12372476\n",
            "Iteration 205, loss = 0.12029692\n",
            "Iteration 206, loss = 0.11947100\n",
            "Iteration 207, loss = 0.11914280\n",
            "Iteration 208, loss = 0.11800640\n",
            "Iteration 209, loss = 0.11915345\n",
            "Iteration 210, loss = 0.11872655\n",
            "Iteration 211, loss = 0.11870659\n",
            "Iteration 212, loss = 0.12124001\n",
            "Iteration 213, loss = 0.12148231\n",
            "Iteration 214, loss = 0.11869934\n",
            "Iteration 215, loss = 0.11740417\n",
            "Iteration 216, loss = 0.11718798\n",
            "Iteration 217, loss = 0.11799397\n",
            "Iteration 218, loss = 0.11845972\n",
            "Iteration 219, loss = 0.11935659\n",
            "Iteration 220, loss = 0.11911399\n",
            "Iteration 221, loss = 0.11796454\n",
            "Iteration 222, loss = 0.11669520\n",
            "Iteration 223, loss = 0.11727022\n",
            "Iteration 224, loss = 0.12004458\n",
            "Iteration 225, loss = 0.11527006\n",
            "Iteration 226, loss = 0.11601134\n",
            "Iteration 227, loss = 0.11627731\n",
            "Iteration 228, loss = 0.11763467\n",
            "Iteration 229, loss = 0.11667653\n",
            "Iteration 230, loss = 0.11453851\n",
            "Iteration 231, loss = 0.11789389\n",
            "Iteration 232, loss = 0.11820867\n",
            "Iteration 233, loss = 0.11264242\n",
            "Iteration 234, loss = 0.11670190\n",
            "Iteration 235, loss = 0.11590166\n",
            "Iteration 236, loss = 0.11475961\n",
            "Iteration 237, loss = 0.11410549\n",
            "Iteration 238, loss = 0.11147213\n",
            "Iteration 239, loss = 0.11631569\n",
            "Iteration 240, loss = 0.11488465\n",
            "Iteration 241, loss = 0.11306958\n",
            "Iteration 242, loss = 0.11230436\n",
            "Iteration 243, loss = 0.11364884\n",
            "Iteration 244, loss = 0.11068554\n",
            "Iteration 245, loss = 0.11399731\n",
            "Iteration 246, loss = 0.11278784\n",
            "Iteration 247, loss = 0.11380537\n",
            "Iteration 248, loss = 0.11148585\n",
            "Iteration 249, loss = 0.11144678\n",
            "Iteration 250, loss = 0.10991921\n",
            "Iteration 251, loss = 0.11001804\n",
            "Iteration 252, loss = 0.11096323\n",
            "Iteration 253, loss = 0.11431264\n",
            "Iteration 254, loss = 0.11053971\n",
            "Iteration 255, loss = 0.11124914\n",
            "Iteration 256, loss = 0.11090094\n",
            "Iteration 257, loss = 0.11162096\n",
            "Iteration 258, loss = 0.11192434\n",
            "Iteration 259, loss = 0.11152882\n",
            "Iteration 260, loss = 0.10979613\n",
            "Iteration 261, loss = 0.10968517\n",
            "Iteration 262, loss = 0.11243775\n",
            "Iteration 263, loss = 0.11176960\n",
            "Iteration 264, loss = 0.11015517\n",
            "Iteration 265, loss = 0.10847335\n",
            "Iteration 266, loss = 0.10899914\n",
            "Iteration 267, loss = 0.10955809\n",
            "Iteration 268, loss = 0.10874333\n",
            "Iteration 269, loss = 0.10698700\n",
            "Iteration 270, loss = 0.10886622\n",
            "Iteration 271, loss = 0.10895983\n",
            "Iteration 272, loss = 0.10802125\n",
            "Iteration 273, loss = 0.10966420\n",
            "Iteration 274, loss = 0.10792476\n",
            "Iteration 275, loss = 0.10705329\n",
            "Iteration 276, loss = 0.10949993\n",
            "Iteration 277, loss = 0.10595871\n",
            "Iteration 278, loss = 0.10536853\n",
            "Iteration 279, loss = 0.10621923\n",
            "Iteration 280, loss = 0.10524812\n",
            "Iteration 281, loss = 0.10494959\n",
            "Iteration 282, loss = 0.10804397\n",
            "Iteration 283, loss = 0.10758151\n",
            "Iteration 284, loss = 0.10507265\n",
            "Iteration 285, loss = 0.10489156\n",
            "Iteration 286, loss = 0.10438789\n",
            "Iteration 287, loss = 0.10557893\n",
            "Iteration 288, loss = 0.10491307\n",
            "Iteration 289, loss = 0.10707040\n",
            "Iteration 290, loss = 0.10700483\n",
            "Iteration 291, loss = 0.10592671\n",
            "Iteration 292, loss = 0.10492456\n",
            "Iteration 293, loss = 0.10558303\n",
            "Iteration 294, loss = 0.10562818\n",
            "Iteration 295, loss = 0.10840598\n",
            "Iteration 296, loss = 0.10510666\n",
            "Iteration 297, loss = 0.10352063\n",
            "Iteration 298, loss = 0.10299341\n",
            "Iteration 299, loss = 0.10452928\n",
            "Iteration 300, loss = 0.10349714\n",
            "Iteration 301, loss = 0.10159831\n",
            "Iteration 302, loss = 0.10390069\n",
            "Iteration 303, loss = 0.10390423\n",
            "Iteration 304, loss = 0.10307503\n",
            "Iteration 305, loss = 0.10399439\n",
            "Iteration 306, loss = 0.10624541\n",
            "Iteration 307, loss = 0.10455651\n",
            "Iteration 308, loss = 0.10269976\n",
            "Iteration 309, loss = 0.10346725\n",
            "Iteration 310, loss = 0.10015900\n",
            "Iteration 311, loss = 0.10378329\n",
            "Iteration 312, loss = 0.10558010\n",
            "Iteration 313, loss = 0.10318358\n",
            "Iteration 314, loss = 0.10291797\n",
            "Iteration 315, loss = 0.10509793\n",
            "Iteration 316, loss = 0.10387740\n",
            "Iteration 317, loss = 0.09862092\n",
            "Iteration 318, loss = 0.10021119\n",
            "Iteration 319, loss = 0.09969389\n",
            "Iteration 320, loss = 0.09987643\n",
            "Iteration 321, loss = 0.10130445\n",
            "Iteration 322, loss = 0.10284192\n",
            "Iteration 323, loss = 0.10109601\n",
            "Iteration 324, loss = 0.10163715\n",
            "Iteration 325, loss = 0.10116650\n",
            "Iteration 326, loss = 0.10197433\n",
            "Iteration 327, loss = 0.10052037\n",
            "Iteration 328, loss = 0.09858175\n",
            "Iteration 329, loss = 0.09927089\n",
            "Iteration 330, loss = 0.09840365\n",
            "Iteration 331, loss = 0.10206270\n",
            "Iteration 332, loss = 0.10152232\n",
            "Iteration 333, loss = 0.10192712\n",
            "Iteration 334, loss = 0.09768233\n",
            "Iteration 335, loss = 0.09801656\n",
            "Iteration 336, loss = 0.10061289\n",
            "Iteration 337, loss = 0.09999557\n",
            "Iteration 338, loss = 0.10202704\n",
            "Iteration 339, loss = 0.09999035\n",
            "Iteration 340, loss = 0.09891384\n",
            "Iteration 341, loss = 0.09945505\n",
            "Iteration 342, loss = 0.09846610\n",
            "Iteration 343, loss = 0.09910715\n",
            "Iteration 344, loss = 0.09602992\n",
            "Iteration 345, loss = 0.09786766\n",
            "Iteration 346, loss = 0.09710441\n",
            "Iteration 347, loss = 0.09968114\n",
            "Iteration 348, loss = 0.09649853\n",
            "Iteration 349, loss = 0.09861290\n",
            "Iteration 350, loss = 0.09811986\n",
            "Iteration 351, loss = 0.09506557\n",
            "Iteration 352, loss = 0.10054691\n",
            "Iteration 353, loss = 0.09610763\n",
            "Iteration 354, loss = 0.09644366\n",
            "Iteration 355, loss = 0.09669638\n",
            "Iteration 356, loss = 0.09885268\n",
            "Iteration 357, loss = 0.09593254\n",
            "Iteration 358, loss = 0.09704069\n",
            "Iteration 359, loss = 0.09745661\n",
            "Iteration 360, loss = 0.09707083\n",
            "Iteration 361, loss = 0.09709248\n",
            "Iteration 362, loss = 0.09557587\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.36529877\n",
            "Iteration 2, loss = 0.32058602\n",
            "Iteration 3, loss = 0.31127511\n",
            "Iteration 4, loss = 0.30446759\n",
            "Iteration 5, loss = 0.29994506\n",
            "Iteration 6, loss = 0.29674701\n",
            "Iteration 7, loss = 0.29367088\n",
            "Iteration 8, loss = 0.29055866\n",
            "Iteration 9, loss = 0.28684183\n",
            "Iteration 10, loss = 0.28533157\n",
            "Iteration 11, loss = 0.28203908\n",
            "Iteration 12, loss = 0.28091062\n",
            "Iteration 13, loss = 0.27827304\n",
            "Iteration 14, loss = 0.27509288\n",
            "Iteration 15, loss = 0.27251492\n",
            "Iteration 16, loss = 0.26981114\n",
            "Iteration 17, loss = 0.26805035\n",
            "Iteration 18, loss = 0.26420780\n",
            "Iteration 19, loss = 0.26337705\n",
            "Iteration 20, loss = 0.26031349\n",
            "Iteration 21, loss = 0.25834587\n",
            "Iteration 22, loss = 0.25451633\n",
            "Iteration 23, loss = 0.25369754\n",
            "Iteration 24, loss = 0.25060846\n",
            "Iteration 25, loss = 0.24866831\n",
            "Iteration 26, loss = 0.24670749\n",
            "Iteration 27, loss = 0.24470780\n",
            "Iteration 28, loss = 0.24211695\n",
            "Iteration 29, loss = 0.23998091\n",
            "Iteration 30, loss = 0.23795421\n",
            "Iteration 31, loss = 0.23578033\n",
            "Iteration 32, loss = 0.23460099\n",
            "Iteration 33, loss = 0.23205379\n",
            "Iteration 34, loss = 0.23066400\n",
            "Iteration 35, loss = 0.22918535\n",
            "Iteration 36, loss = 0.22648323\n",
            "Iteration 37, loss = 0.22437996\n",
            "Iteration 38, loss = 0.22314077\n",
            "Iteration 39, loss = 0.22172689\n",
            "Iteration 40, loss = 0.21971408\n",
            "Iteration 41, loss = 0.21914992\n",
            "Iteration 42, loss = 0.21598149\n",
            "Iteration 43, loss = 0.21729171\n",
            "Iteration 44, loss = 0.21331399\n",
            "Iteration 45, loss = 0.21248814\n",
            "Iteration 46, loss = 0.21036079\n",
            "Iteration 47, loss = 0.21041749\n",
            "Iteration 48, loss = 0.20871426\n",
            "Iteration 49, loss = 0.20586909\n",
            "Iteration 50, loss = 0.20760711\n",
            "Iteration 51, loss = 0.20465650\n",
            "Iteration 52, loss = 0.20430201\n",
            "Iteration 53, loss = 0.20225680\n",
            "Iteration 54, loss = 0.20157086\n",
            "Iteration 55, loss = 0.19923348\n",
            "Iteration 56, loss = 0.19849868\n",
            "Iteration 57, loss = 0.19767851\n",
            "Iteration 58, loss = 0.19571666\n",
            "Iteration 59, loss = 0.19469724\n",
            "Iteration 60, loss = 0.19426171\n",
            "Iteration 61, loss = 0.19087681\n",
            "Iteration 62, loss = 0.18936842\n",
            "Iteration 63, loss = 0.18904739\n",
            "Iteration 64, loss = 0.18871366\n",
            "Iteration 65, loss = 0.18757949\n",
            "Iteration 66, loss = 0.18522187\n",
            "Iteration 67, loss = 0.18650414\n",
            "Iteration 68, loss = 0.18731575\n",
            "Iteration 69, loss = 0.18520241\n",
            "Iteration 70, loss = 0.18405618\n",
            "Iteration 71, loss = 0.18371517\n",
            "Iteration 72, loss = 0.18209647\n",
            "Iteration 73, loss = 0.18146174\n",
            "Iteration 74, loss = 0.18004948\n",
            "Iteration 75, loss = 0.17984852\n",
            "Iteration 76, loss = 0.17996433\n",
            "Iteration 77, loss = 0.17948019\n",
            "Iteration 78, loss = 0.17689580\n",
            "Iteration 79, loss = 0.17526750\n",
            "Iteration 80, loss = 0.17373055\n",
            "Iteration 81, loss = 0.17515206\n",
            "Iteration 82, loss = 0.17345110\n",
            "Iteration 83, loss = 0.17298696\n",
            "Iteration 84, loss = 0.17545603\n",
            "Iteration 85, loss = 0.17282226\n",
            "Iteration 86, loss = 0.17041578\n",
            "Iteration 87, loss = 0.17115964\n",
            "Iteration 88, loss = 0.16990864\n",
            "Iteration 89, loss = 0.16915798\n",
            "Iteration 90, loss = 0.16681646\n",
            "Iteration 91, loss = 0.16662773\n",
            "Iteration 92, loss = 0.16755176\n",
            "Iteration 93, loss = 0.16588311\n",
            "Iteration 94, loss = 0.16316925\n",
            "Iteration 95, loss = 0.16536548\n",
            "Iteration 96, loss = 0.16388313\n",
            "Iteration 97, loss = 0.16228229\n",
            "Iteration 98, loss = 0.16345217\n",
            "Iteration 99, loss = 0.16207735\n",
            "Iteration 100, loss = 0.16183700\n",
            "Iteration 101, loss = 0.16394792\n",
            "Iteration 102, loss = 0.16032346\n",
            "Iteration 103, loss = 0.16118486\n",
            "Iteration 104, loss = 0.15987458\n",
            "Iteration 105, loss = 0.15809544\n",
            "Iteration 106, loss = 0.15715561\n",
            "Iteration 107, loss = 0.15648081\n",
            "Iteration 108, loss = 0.15764516\n",
            "Iteration 109, loss = 0.15792703\n",
            "Iteration 110, loss = 0.15797461\n",
            "Iteration 111, loss = 0.15564160\n",
            "Iteration 112, loss = 0.15391985\n",
            "Iteration 113, loss = 0.15691905\n",
            "Iteration 114, loss = 0.15454361\n",
            "Iteration 115, loss = 0.15438814\n",
            "Iteration 116, loss = 0.15484587\n",
            "Iteration 117, loss = 0.15318741\n",
            "Iteration 118, loss = 0.15107311\n",
            "Iteration 119, loss = 0.15193171\n",
            "Iteration 120, loss = 0.15078198\n",
            "Iteration 121, loss = 0.15020174\n",
            "Iteration 122, loss = 0.14994013\n",
            "Iteration 123, loss = 0.14932627\n",
            "Iteration 124, loss = 0.14868395\n",
            "Iteration 125, loss = 0.14902918\n",
            "Iteration 126, loss = 0.14856679\n",
            "Iteration 127, loss = 0.14949761\n",
            "Iteration 128, loss = 0.14501722\n",
            "Iteration 129, loss = 0.14708840\n",
            "Iteration 130, loss = 0.14700162\n",
            "Iteration 131, loss = 0.14407148\n",
            "Iteration 132, loss = 0.14515787\n",
            "Iteration 133, loss = 0.14480180\n",
            "Iteration 134, loss = 0.14478516\n",
            "Iteration 135, loss = 0.14288343\n",
            "Iteration 136, loss = 0.14283184\n",
            "Iteration 137, loss = 0.14370964\n",
            "Iteration 138, loss = 0.14291908\n",
            "Iteration 139, loss = 0.14328939\n",
            "Iteration 140, loss = 0.14140168\n",
            "Iteration 141, loss = 0.14109467\n",
            "Iteration 142, loss = 0.14057731\n",
            "Iteration 143, loss = 0.14036591\n",
            "Iteration 144, loss = 0.14067508\n",
            "Iteration 145, loss = 0.14181820\n",
            "Iteration 146, loss = 0.13957614\n",
            "Iteration 147, loss = 0.13842855\n",
            "Iteration 148, loss = 0.13868512\n",
            "Iteration 149, loss = 0.13813986\n",
            "Iteration 150, loss = 0.13798146\n",
            "Iteration 151, loss = 0.14132510\n",
            "Iteration 152, loss = 0.14092041\n",
            "Iteration 153, loss = 0.13751274\n",
            "Iteration 154, loss = 0.13852595\n",
            "Iteration 155, loss = 0.13737845\n",
            "Iteration 156, loss = 0.13652288\n",
            "Iteration 157, loss = 0.13477072\n",
            "Iteration 158, loss = 0.13563790\n",
            "Iteration 159, loss = 0.13683921\n",
            "Iteration 160, loss = 0.13454779\n",
            "Iteration 161, loss = 0.13463354\n",
            "Iteration 162, loss = 0.13355346\n",
            "Iteration 163, loss = 0.13223558\n",
            "Iteration 164, loss = 0.13267883\n",
            "Iteration 165, loss = 0.13199356\n",
            "Iteration 166, loss = 0.13108896\n",
            "Iteration 167, loss = 0.13189885\n",
            "Iteration 168, loss = 0.13271093\n",
            "Iteration 169, loss = 0.13357136\n",
            "Iteration 170, loss = 0.13093259\n",
            "Iteration 171, loss = 0.13369423\n",
            "Iteration 172, loss = 0.13171421\n",
            "Iteration 173, loss = 0.13139050\n",
            "Iteration 174, loss = 0.13064129\n",
            "Iteration 175, loss = 0.12904813\n",
            "Iteration 176, loss = 0.13253158\n",
            "Iteration 177, loss = 0.13110743\n",
            "Iteration 178, loss = 0.12918062\n",
            "Iteration 179, loss = 0.12967732\n",
            "Iteration 180, loss = 0.12818430\n",
            "Iteration 181, loss = 0.12776121\n",
            "Iteration 182, loss = 0.12937183\n",
            "Iteration 183, loss = 0.12812892\n",
            "Iteration 184, loss = 0.12708198\n",
            "Iteration 185, loss = 0.12903728\n",
            "Iteration 186, loss = 0.12982267\n",
            "Iteration 187, loss = 0.12888360\n",
            "Iteration 188, loss = 0.12814730\n",
            "Iteration 189, loss = 0.12492566\n",
            "Iteration 190, loss = 0.12713844\n",
            "Iteration 191, loss = 0.12509751\n",
            "Iteration 192, loss = 0.12594881\n",
            "Iteration 193, loss = 0.12294878\n",
            "Iteration 194, loss = 0.12367165\n",
            "Iteration 195, loss = 0.12450472\n",
            "Iteration 196, loss = 0.12515085\n",
            "Iteration 197, loss = 0.12295237\n",
            "Iteration 198, loss = 0.12364819\n",
            "Iteration 199, loss = 0.12273018\n",
            "Iteration 200, loss = 0.12508234\n",
            "Iteration 201, loss = 0.12293338\n",
            "Iteration 202, loss = 0.12452325\n",
            "Iteration 203, loss = 0.12383588\n",
            "Iteration 204, loss = 0.12569621\n",
            "Iteration 205, loss = 0.12123475\n",
            "Iteration 206, loss = 0.12020354\n",
            "Iteration 207, loss = 0.12274225\n",
            "Iteration 208, loss = 0.12031502\n",
            "Iteration 209, loss = 0.12009133\n",
            "Iteration 210, loss = 0.12230936\n",
            "Iteration 211, loss = 0.12460951\n",
            "Iteration 212, loss = 0.11871642\n",
            "Iteration 213, loss = 0.12707405\n",
            "Iteration 214, loss = 0.12334397\n",
            "Iteration 215, loss = 0.12327708\n",
            "Iteration 216, loss = 0.12477171\n",
            "Iteration 217, loss = 0.12104883\n",
            "Iteration 218, loss = 0.12011468\n",
            "Iteration 219, loss = 0.11798716\n",
            "Iteration 220, loss = 0.11690639\n",
            "Iteration 221, loss = 0.11673824\n",
            "Iteration 222, loss = 0.11684959\n",
            "Iteration 223, loss = 0.11782301\n",
            "Iteration 224, loss = 0.11502581\n",
            "Iteration 225, loss = 0.11890223\n",
            "Iteration 226, loss = 0.11717843\n",
            "Iteration 227, loss = 0.11882595\n",
            "Iteration 228, loss = 0.11830326\n",
            "Iteration 229, loss = 0.11470133\n",
            "Iteration 230, loss = 0.11560356\n",
            "Iteration 231, loss = 0.11385940\n",
            "Iteration 232, loss = 0.11631785\n",
            "Iteration 233, loss = 0.11428016\n",
            "Iteration 234, loss = 0.11770014\n",
            "Iteration 235, loss = 0.11476192\n",
            "Iteration 236, loss = 0.11778199\n",
            "Iteration 237, loss = 0.11467987\n",
            "Iteration 238, loss = 0.11526278\n",
            "Iteration 239, loss = 0.11613377\n",
            "Iteration 240, loss = 0.11776075\n",
            "Iteration 241, loss = 0.11831333\n",
            "Iteration 242, loss = 0.11445550\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.36882661\n",
            "Iteration 2, loss = 0.32254134\n",
            "Iteration 3, loss = 0.30964459\n",
            "Iteration 4, loss = 0.30395126\n",
            "Iteration 5, loss = 0.29772553\n",
            "Iteration 6, loss = 0.29352912\n",
            "Iteration 7, loss = 0.29040534\n",
            "Iteration 8, loss = 0.28782071\n",
            "Iteration 9, loss = 0.28437287\n",
            "Iteration 10, loss = 0.28237910\n",
            "Iteration 11, loss = 0.27977896\n",
            "Iteration 12, loss = 0.27875433\n",
            "Iteration 13, loss = 0.27599681\n",
            "Iteration 14, loss = 0.27306937\n",
            "Iteration 15, loss = 0.27044097\n",
            "Iteration 16, loss = 0.26863241\n",
            "Iteration 17, loss = 0.26708508\n",
            "Iteration 18, loss = 0.26511759\n",
            "Iteration 19, loss = 0.26320963\n",
            "Iteration 20, loss = 0.26012152\n",
            "Iteration 21, loss = 0.25922046\n",
            "Iteration 22, loss = 0.25625283\n",
            "Iteration 23, loss = 0.25283346\n",
            "Iteration 24, loss = 0.25226660\n",
            "Iteration 25, loss = 0.24975354\n",
            "Iteration 26, loss = 0.24682794\n",
            "Iteration 27, loss = 0.24618977\n",
            "Iteration 28, loss = 0.24338160\n",
            "Iteration 29, loss = 0.24143415\n",
            "Iteration 30, loss = 0.23992674\n",
            "Iteration 31, loss = 0.23862823\n",
            "Iteration 32, loss = 0.23698968\n",
            "Iteration 33, loss = 0.23484667\n",
            "Iteration 34, loss = 0.23202351\n",
            "Iteration 35, loss = 0.23044597\n",
            "Iteration 36, loss = 0.22744139\n",
            "Iteration 37, loss = 0.22823296\n",
            "Iteration 38, loss = 0.22465246\n",
            "Iteration 39, loss = 0.22443174\n",
            "Iteration 40, loss = 0.22195908\n",
            "Iteration 41, loss = 0.22071114\n",
            "Iteration 42, loss = 0.21925387\n",
            "Iteration 43, loss = 0.21770169\n",
            "Iteration 44, loss = 0.21701700\n",
            "Iteration 45, loss = 0.21364112\n",
            "Iteration 46, loss = 0.21398450\n",
            "Iteration 47, loss = 0.21237218\n",
            "Iteration 48, loss = 0.20988964\n",
            "Iteration 49, loss = 0.20982997\n",
            "Iteration 50, loss = 0.20758287\n",
            "Iteration 51, loss = 0.20572618\n",
            "Iteration 52, loss = 0.20424027\n",
            "Iteration 53, loss = 0.20351981\n",
            "Iteration 54, loss = 0.20219434\n",
            "Iteration 55, loss = 0.20128854\n",
            "Iteration 56, loss = 0.20118318\n",
            "Iteration 57, loss = 0.19831724\n",
            "Iteration 58, loss = 0.19855470\n",
            "Iteration 59, loss = 0.19842584\n",
            "Iteration 60, loss = 0.19661482\n",
            "Iteration 61, loss = 0.19558609\n",
            "Iteration 62, loss = 0.19313596\n",
            "Iteration 63, loss = 0.19470819\n",
            "Iteration 64, loss = 0.19349881\n",
            "Iteration 65, loss = 0.18899779\n",
            "Iteration 66, loss = 0.19159085\n",
            "Iteration 67, loss = 0.18799564\n",
            "Iteration 68, loss = 0.18586939\n",
            "Iteration 69, loss = 0.18558078\n",
            "Iteration 70, loss = 0.18473427\n",
            "Iteration 71, loss = 0.18409234\n",
            "Iteration 72, loss = 0.18415588\n",
            "Iteration 73, loss = 0.18163683\n",
            "Iteration 74, loss = 0.18257335\n",
            "Iteration 75, loss = 0.18310021\n",
            "Iteration 76, loss = 0.17898957\n",
            "Iteration 77, loss = 0.17907061\n",
            "Iteration 78, loss = 0.17773438\n",
            "Iteration 79, loss = 0.17796134\n",
            "Iteration 80, loss = 0.17582450\n",
            "Iteration 81, loss = 0.17608738\n",
            "Iteration 82, loss = 0.17449567\n",
            "Iteration 83, loss = 0.17380130\n",
            "Iteration 84, loss = 0.17334696\n",
            "Iteration 85, loss = 0.17308179\n",
            "Iteration 86, loss = 0.17132534\n",
            "Iteration 87, loss = 0.17120269\n",
            "Iteration 88, loss = 0.17152777\n",
            "Iteration 89, loss = 0.16872524\n",
            "Iteration 90, loss = 0.16711721\n",
            "Iteration 91, loss = 0.16843352\n",
            "Iteration 92, loss = 0.16686304\n",
            "Iteration 93, loss = 0.16497438\n",
            "Iteration 94, loss = 0.16525343\n",
            "Iteration 95, loss = 0.16508664\n",
            "Iteration 96, loss = 0.16697378\n",
            "Iteration 97, loss = 0.16319185\n",
            "Iteration 98, loss = 0.16251096\n",
            "Iteration 99, loss = 0.16369115\n",
            "Iteration 100, loss = 0.16283463\n",
            "Iteration 101, loss = 0.16149947\n",
            "Iteration 102, loss = 0.16046045\n",
            "Iteration 103, loss = 0.15970448\n",
            "Iteration 104, loss = 0.16053465\n",
            "Iteration 105, loss = 0.15873988\n",
            "Iteration 106, loss = 0.15834065\n",
            "Iteration 107, loss = 0.15813419\n",
            "Iteration 108, loss = 0.15696237\n",
            "Iteration 109, loss = 0.15747095\n",
            "Iteration 110, loss = 0.15520155\n",
            "Iteration 111, loss = 0.15614617\n",
            "Iteration 112, loss = 0.15387160\n",
            "Iteration 113, loss = 0.15349226\n",
            "Iteration 114, loss = 0.15214634\n",
            "Iteration 115, loss = 0.15337160\n",
            "Iteration 116, loss = 0.15234268\n",
            "Iteration 117, loss = 0.15333499\n",
            "Iteration 118, loss = 0.15455533\n",
            "Iteration 119, loss = 0.15126230\n",
            "Iteration 120, loss = 0.14991686\n",
            "Iteration 121, loss = 0.15295922\n",
            "Iteration 122, loss = 0.14930576\n",
            "Iteration 123, loss = 0.14834137\n",
            "Iteration 124, loss = 0.14938526\n",
            "Iteration 125, loss = 0.14776587\n",
            "Iteration 126, loss = 0.14712872\n",
            "Iteration 127, loss = 0.14624642\n",
            "Iteration 128, loss = 0.14558437\n",
            "Iteration 129, loss = 0.14667354\n",
            "Iteration 130, loss = 0.14728291\n",
            "Iteration 131, loss = 0.14533357\n",
            "Iteration 132, loss = 0.14483219\n",
            "Iteration 133, loss = 0.14401451\n",
            "Iteration 134, loss = 0.14601109\n",
            "Iteration 135, loss = 0.14602002\n",
            "Iteration 136, loss = 0.14254431\n",
            "Iteration 137, loss = 0.14302693\n",
            "Iteration 138, loss = 0.14172252\n",
            "Iteration 139, loss = 0.14240022\n",
            "Iteration 140, loss = 0.14147350\n",
            "Iteration 141, loss = 0.14496994\n",
            "Iteration 142, loss = 0.14135480\n",
            "Iteration 143, loss = 0.13966283\n",
            "Iteration 144, loss = 0.14213174\n",
            "Iteration 145, loss = 0.14001579\n",
            "Iteration 146, loss = 0.13993921\n",
            "Iteration 147, loss = 0.13819563\n",
            "Iteration 148, loss = 0.13792477\n",
            "Iteration 149, loss = 0.13886806\n",
            "Iteration 150, loss = 0.13947807\n",
            "Iteration 151, loss = 0.13760916\n",
            "Iteration 152, loss = 0.13719614\n",
            "Iteration 153, loss = 0.14098705\n",
            "Iteration 154, loss = 0.13656373\n",
            "Iteration 155, loss = 0.13614006\n",
            "Iteration 156, loss = 0.13731719\n",
            "Iteration 157, loss = 0.13739662\n",
            "Iteration 158, loss = 0.13419932\n",
            "Iteration 159, loss = 0.13380742\n",
            "Iteration 160, loss = 0.13375262\n",
            "Iteration 161, loss = 0.13349765\n",
            "Iteration 162, loss = 0.13395202\n",
            "Iteration 163, loss = 0.13201074\n",
            "Iteration 164, loss = 0.13326911\n",
            "Iteration 165, loss = 0.13116436\n",
            "Iteration 166, loss = 0.13262686\n",
            "Iteration 167, loss = 0.13151441\n",
            "Iteration 168, loss = 0.13150667\n",
            "Iteration 169, loss = 0.13177560\n",
            "Iteration 170, loss = 0.13121436\n",
            "Iteration 171, loss = 0.13002338\n",
            "Iteration 172, loss = 0.12849411\n",
            "Iteration 173, loss = 0.12946045\n",
            "Iteration 174, loss = 0.13085893\n",
            "Iteration 175, loss = 0.12996340\n",
            "Iteration 176, loss = 0.12764247\n",
            "Iteration 177, loss = 0.12912838\n",
            "Iteration 178, loss = 0.13076267\n",
            "Iteration 179, loss = 0.12940160\n",
            "Iteration 180, loss = 0.12931830\n",
            "Iteration 181, loss = 0.12750323\n",
            "Iteration 182, loss = 0.12854143\n",
            "Iteration 183, loss = 0.12907288\n",
            "Iteration 184, loss = 0.12662197\n",
            "Iteration 185, loss = 0.12837305\n",
            "Iteration 186, loss = 0.12921446\n",
            "Iteration 187, loss = 0.12858563\n",
            "Iteration 188, loss = 0.12412648\n",
            "Iteration 189, loss = 0.12575929\n",
            "Iteration 190, loss = 0.12351635\n",
            "Iteration 191, loss = 0.12314175\n",
            "Iteration 192, loss = 0.12186504\n",
            "Iteration 193, loss = 0.12447618\n",
            "Iteration 194, loss = 0.12347266\n",
            "Iteration 195, loss = 0.12359612\n",
            "Iteration 196, loss = 0.12361879\n",
            "Iteration 197, loss = 0.12246633\n",
            "Iteration 198, loss = 0.12333221\n",
            "Iteration 199, loss = 0.12331006\n",
            "Iteration 200, loss = 0.12396148\n",
            "Iteration 201, loss = 0.12553413\n",
            "Iteration 202, loss = 0.12173907\n",
            "Iteration 203, loss = 0.12075285\n",
            "Iteration 204, loss = 0.12284005\n",
            "Iteration 205, loss = 0.12153528\n",
            "Iteration 206, loss = 0.11907816\n",
            "Iteration 207, loss = 0.11984860\n",
            "Iteration 208, loss = 0.12032424\n",
            "Iteration 209, loss = 0.12127961\n",
            "Iteration 210, loss = 0.11970292\n",
            "Iteration 211, loss = 0.12127664\n",
            "Iteration 212, loss = 0.12030761\n",
            "Iteration 213, loss = 0.12130843\n",
            "Iteration 214, loss = 0.11811193\n",
            "Iteration 215, loss = 0.11990259\n",
            "Iteration 216, loss = 0.11959099\n",
            "Iteration 217, loss = 0.12127274\n",
            "Iteration 218, loss = 0.11627801\n",
            "Iteration 219, loss = 0.11952671\n",
            "Iteration 220, loss = 0.11904470\n",
            "Iteration 221, loss = 0.11719877\n",
            "Iteration 222, loss = 0.11854580\n",
            "Iteration 223, loss = 0.11691755\n",
            "Iteration 224, loss = 0.11709432\n",
            "Iteration 225, loss = 0.11879443\n",
            "Iteration 226, loss = 0.11856300\n",
            "Iteration 227, loss = 0.11453911\n",
            "Iteration 228, loss = 0.11837787\n",
            "Iteration 229, loss = 0.11527862\n",
            "Iteration 230, loss = 0.11539096\n",
            "Iteration 231, loss = 0.11464716\n",
            "Iteration 232, loss = 0.11430455\n",
            "Iteration 233, loss = 0.11554661\n",
            "Iteration 234, loss = 0.11384741\n",
            "Iteration 235, loss = 0.11445673\n",
            "Iteration 236, loss = 0.11260564\n",
            "Iteration 237, loss = 0.11448268\n",
            "Iteration 238, loss = 0.11534670\n",
            "Iteration 239, loss = 0.11561981\n",
            "Iteration 240, loss = 0.11569740\n",
            "Iteration 241, loss = 0.11464836\n",
            "Iteration 242, loss = 0.11422380\n",
            "Iteration 243, loss = 0.11339494\n",
            "Iteration 244, loss = 0.11298214\n",
            "Iteration 245, loss = 0.11360905\n",
            "Iteration 246, loss = 0.11574044\n",
            "Iteration 247, loss = 0.11363100\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.38939181\n",
            "Iteration 2, loss = 0.32322139\n",
            "Iteration 3, loss = 0.31235807\n",
            "Iteration 4, loss = 0.30463384\n",
            "Iteration 5, loss = 0.30006708\n",
            "Iteration 6, loss = 0.29700492\n",
            "Iteration 7, loss = 0.29213424\n",
            "Iteration 8, loss = 0.28956011\n",
            "Iteration 9, loss = 0.28639375\n",
            "Iteration 10, loss = 0.28340342\n",
            "Iteration 11, loss = 0.28050358\n",
            "Iteration 12, loss = 0.27824425\n",
            "Iteration 13, loss = 0.27560737\n",
            "Iteration 14, loss = 0.27315660\n",
            "Iteration 15, loss = 0.27049350\n",
            "Iteration 16, loss = 0.26902039\n",
            "Iteration 17, loss = 0.26605796\n",
            "Iteration 18, loss = 0.26467210\n",
            "Iteration 19, loss = 0.26180091\n",
            "Iteration 20, loss = 0.25956501\n",
            "Iteration 21, loss = 0.25733303\n",
            "Iteration 22, loss = 0.25481543\n",
            "Iteration 23, loss = 0.25390971\n",
            "Iteration 24, loss = 0.25020198\n",
            "Iteration 25, loss = 0.24873160\n",
            "Iteration 26, loss = 0.24646076\n",
            "Iteration 27, loss = 0.24578510\n",
            "Iteration 28, loss = 0.24352303\n",
            "Iteration 29, loss = 0.24288249\n",
            "Iteration 30, loss = 0.23945338\n",
            "Iteration 31, loss = 0.23680879\n",
            "Iteration 32, loss = 0.23583468\n",
            "Iteration 33, loss = 0.23214115\n",
            "Iteration 34, loss = 0.23202601\n",
            "Iteration 35, loss = 0.22858339\n",
            "Iteration 36, loss = 0.22953020\n",
            "Iteration 37, loss = 0.22705486\n",
            "Iteration 38, loss = 0.22438901\n",
            "Iteration 39, loss = 0.22241563\n",
            "Iteration 40, loss = 0.22028389\n",
            "Iteration 41, loss = 0.22102876\n",
            "Iteration 42, loss = 0.21833714\n",
            "Iteration 43, loss = 0.22022501\n",
            "Iteration 44, loss = 0.21643931\n",
            "Iteration 45, loss = 0.21413123\n",
            "Iteration 46, loss = 0.21259104\n",
            "Iteration 47, loss = 0.21166670\n",
            "Iteration 48, loss = 0.20888454\n",
            "Iteration 49, loss = 0.20782934\n",
            "Iteration 50, loss = 0.20677448\n",
            "Iteration 51, loss = 0.20621996\n",
            "Iteration 52, loss = 0.20516781\n",
            "Iteration 53, loss = 0.20349193\n",
            "Iteration 54, loss = 0.20175182\n",
            "Iteration 55, loss = 0.20159017\n",
            "Iteration 56, loss = 0.19957944\n",
            "Iteration 57, loss = 0.19963324\n",
            "Iteration 58, loss = 0.19988845\n",
            "Iteration 59, loss = 0.19630449\n",
            "Iteration 60, loss = 0.19541127\n",
            "Iteration 61, loss = 0.19454167\n",
            "Iteration 62, loss = 0.19259488\n",
            "Iteration 63, loss = 0.19106624\n",
            "Iteration 64, loss = 0.19088972\n",
            "Iteration 65, loss = 0.18859818\n",
            "Iteration 66, loss = 0.18802184\n",
            "Iteration 67, loss = 0.18752966\n",
            "Iteration 68, loss = 0.18611042\n",
            "Iteration 69, loss = 0.18575288\n",
            "Iteration 70, loss = 0.18578592\n",
            "Iteration 71, loss = 0.18466002\n",
            "Iteration 72, loss = 0.18240603\n",
            "Iteration 73, loss = 0.18166119\n",
            "Iteration 74, loss = 0.18004674\n",
            "Iteration 75, loss = 0.17940906\n",
            "Iteration 76, loss = 0.17904345\n",
            "Iteration 77, loss = 0.17801618\n",
            "Iteration 78, loss = 0.17577794\n",
            "Iteration 79, loss = 0.17747061\n",
            "Iteration 80, loss = 0.17675696\n",
            "Iteration 81, loss = 0.17584359\n",
            "Iteration 82, loss = 0.17413929\n",
            "Iteration 83, loss = 0.17306152\n",
            "Iteration 84, loss = 0.17241526\n",
            "Iteration 85, loss = 0.17219317\n",
            "Iteration 86, loss = 0.17018574\n",
            "Iteration 87, loss = 0.17069728\n",
            "Iteration 88, loss = 0.17022908\n",
            "Iteration 89, loss = 0.17006876\n",
            "Iteration 90, loss = 0.16782892\n",
            "Iteration 91, loss = 0.16870277\n",
            "Iteration 92, loss = 0.16622599\n",
            "Iteration 93, loss = 0.16458558\n",
            "Iteration 94, loss = 0.16417267\n",
            "Iteration 95, loss = 0.16366279\n",
            "Iteration 96, loss = 0.16420882\n",
            "Iteration 97, loss = 0.16385513\n",
            "Iteration 98, loss = 0.16269235\n",
            "Iteration 99, loss = 0.16327743\n",
            "Iteration 100, loss = 0.16136680\n",
            "Iteration 101, loss = 0.15994672\n",
            "Iteration 102, loss = 0.16054671\n",
            "Iteration 103, loss = 0.15983665\n",
            "Iteration 104, loss = 0.15832789\n",
            "Iteration 105, loss = 0.15870172\n",
            "Iteration 106, loss = 0.15762385\n",
            "Iteration 107, loss = 0.15581494\n",
            "Iteration 108, loss = 0.15537570\n",
            "Iteration 109, loss = 0.15545138\n",
            "Iteration 110, loss = 0.15587382\n",
            "Iteration 111, loss = 0.15568665\n",
            "Iteration 112, loss = 0.15412417\n",
            "Iteration 113, loss = 0.15516943\n",
            "Iteration 114, loss = 0.15190826\n",
            "Iteration 115, loss = 0.15288202\n",
            "Iteration 116, loss = 0.15276566\n",
            "Iteration 117, loss = 0.15218249\n",
            "Iteration 118, loss = 0.15126838\n",
            "Iteration 119, loss = 0.15076776\n",
            "Iteration 120, loss = 0.14913155\n",
            "Iteration 121, loss = 0.14872320\n",
            "Iteration 122, loss = 0.14886274\n",
            "Iteration 123, loss = 0.14801538\n",
            "Iteration 124, loss = 0.14631821\n",
            "Iteration 125, loss = 0.14527056\n",
            "Iteration 126, loss = 0.14637545\n",
            "Iteration 127, loss = 0.14599949\n",
            "Iteration 128, loss = 0.14605786\n",
            "Iteration 129, loss = 0.14662722\n",
            "Iteration 130, loss = 0.14524760\n",
            "Iteration 131, loss = 0.14423629\n",
            "Iteration 132, loss = 0.14468201\n",
            "Iteration 133, loss = 0.14524705\n",
            "Iteration 134, loss = 0.14830413\n",
            "Iteration 135, loss = 0.14235010\n",
            "Iteration 136, loss = 0.14293244\n",
            "Iteration 137, loss = 0.14075456\n",
            "Iteration 138, loss = 0.14171279\n",
            "Iteration 139, loss = 0.13918207\n",
            "Iteration 140, loss = 0.14145210\n",
            "Iteration 141, loss = 0.13976164\n",
            "Iteration 142, loss = 0.14045558\n",
            "Iteration 143, loss = 0.13846445\n",
            "Iteration 144, loss = 0.13960604\n",
            "Iteration 145, loss = 0.13822777\n",
            "Iteration 146, loss = 0.13734033\n",
            "Iteration 147, loss = 0.13685155\n",
            "Iteration 148, loss = 0.13904804\n",
            "Iteration 149, loss = 0.13789243\n",
            "Iteration 150, loss = 0.13786712\n",
            "Iteration 151, loss = 0.13605756\n",
            "Iteration 152, loss = 0.13663299\n",
            "Iteration 153, loss = 0.13522434\n",
            "Iteration 154, loss = 0.13454932\n",
            "Iteration 155, loss = 0.13711138\n",
            "Iteration 156, loss = 0.13493618\n",
            "Iteration 157, loss = 0.13276450\n",
            "Iteration 158, loss = 0.13503629\n",
            "Iteration 159, loss = 0.13285795\n",
            "Iteration 160, loss = 0.13068013\n",
            "Iteration 161, loss = 0.13162699\n",
            "Iteration 162, loss = 0.13178870\n",
            "Iteration 163, loss = 0.13237324\n",
            "Iteration 164, loss = 0.13253339\n",
            "Iteration 165, loss = 0.13133127\n",
            "Iteration 166, loss = 0.13085725\n",
            "Iteration 167, loss = 0.13158578\n",
            "Iteration 168, loss = 0.12959652\n",
            "Iteration 169, loss = 0.12948024\n",
            "Iteration 170, loss = 0.12940909\n",
            "Iteration 171, loss = 0.12864456\n",
            "Iteration 172, loss = 0.12789989\n",
            "Iteration 173, loss = 0.12663500\n",
            "Iteration 174, loss = 0.12926682\n",
            "Iteration 175, loss = 0.12947734\n",
            "Iteration 176, loss = 0.12816615\n",
            "Iteration 177, loss = 0.13041590\n",
            "Iteration 178, loss = inf\n",
            "Iteration 179, loss = 0.12741718\n",
            "Iteration 180, loss = 0.12735113\n",
            "Iteration 181, loss = 0.12648734\n",
            "Iteration 182, loss = 0.12660326\n",
            "Iteration 183, loss = 0.12519305\n",
            "Iteration 184, loss = 0.12482105\n",
            "Iteration 185, loss = 0.12547077\n",
            "Iteration 186, loss = 0.12557972\n",
            "Iteration 187, loss = 0.12354624\n",
            "Iteration 188, loss = 0.12536766\n",
            "Iteration 189, loss = 0.12683315\n",
            "Iteration 190, loss = 0.12530274\n",
            "Iteration 191, loss = 0.12478659\n",
            "Iteration 192, loss = 0.12617469\n",
            "Iteration 193, loss = 0.12393662\n",
            "Iteration 194, loss = 0.12824031\n",
            "Iteration 195, loss = 0.12407574\n",
            "Iteration 196, loss = 0.12353335\n",
            "Iteration 197, loss = 0.12192483\n",
            "Iteration 198, loss = 0.12048443\n",
            "Iteration 199, loss = 0.12243210\n",
            "Iteration 200, loss = 0.12243527\n",
            "Iteration 201, loss = 0.12020992\n",
            "Iteration 202, loss = 0.12185156\n",
            "Iteration 203, loss = 0.11916769\n",
            "Iteration 204, loss = 0.12101089\n",
            "Iteration 205, loss = 0.11905632\n",
            "Iteration 206, loss = 0.11965804\n",
            "Iteration 207, loss = 0.11940708\n",
            "Iteration 208, loss = 0.12072470\n",
            "Iteration 209, loss = 0.12044484\n",
            "Iteration 210, loss = 0.11750981\n",
            "Iteration 211, loss = 0.11885822\n",
            "Iteration 212, loss = 0.12441147\n",
            "Iteration 213, loss = 0.12034237\n",
            "Iteration 214, loss = 0.11850234\n",
            "Iteration 215, loss = 0.11827481\n",
            "Iteration 216, loss = 0.11744021\n",
            "Iteration 217, loss = 0.11918869\n",
            "Iteration 218, loss = 0.11760433\n",
            "Iteration 219, loss = 0.11592437\n",
            "Iteration 220, loss = 0.11648311\n",
            "Iteration 221, loss = 0.11748877\n",
            "Iteration 222, loss = 0.11759835\n",
            "Iteration 223, loss = 0.11591046\n",
            "Iteration 224, loss = 0.11580597\n",
            "Iteration 225, loss = 0.11584356\n",
            "Iteration 226, loss = 0.11743596\n",
            "Iteration 227, loss = 0.11676994\n",
            "Iteration 228, loss = 0.11756292\n",
            "Iteration 229, loss = 0.11547802\n",
            "Iteration 230, loss = 0.11427828\n",
            "Iteration 231, loss = 0.11417456\n",
            "Iteration 232, loss = 0.11614080\n",
            "Iteration 233, loss = 0.11594368\n",
            "Iteration 234, loss = 0.11604358\n",
            "Iteration 235, loss = 0.11414943\n",
            "Iteration 236, loss = 0.11350580\n",
            "Iteration 237, loss = 0.11193873\n",
            "Iteration 238, loss = 0.11334620\n",
            "Iteration 239, loss = 0.11365996\n",
            "Iteration 240, loss = 0.11629556\n",
            "Iteration 241, loss = 0.11633447\n",
            "Iteration 242, loss = 0.11355406\n",
            "Iteration 243, loss = 0.11167407\n",
            "Iteration 244, loss = 0.11396978\n",
            "Iteration 245, loss = 0.11114601\n",
            "Iteration 246, loss = 0.11258196\n",
            "Iteration 247, loss = 0.11450605\n",
            "Iteration 248, loss = 0.11253885\n",
            "Iteration 249, loss = 0.11079093\n",
            "Iteration 250, loss = 0.11075134\n",
            "Iteration 251, loss = 0.11424032\n",
            "Iteration 252, loss = 0.11111213\n",
            "Iteration 253, loss = 0.11272352\n",
            "Iteration 254, loss = 0.11076066\n",
            "Iteration 255, loss = 0.10991557\n",
            "Iteration 256, loss = 0.11239334\n",
            "Iteration 257, loss = 0.10975263\n",
            "Iteration 258, loss = 0.11158670\n",
            "Iteration 259, loss = 0.10957103\n",
            "Iteration 260, loss = 0.11244556\n",
            "Iteration 261, loss = 0.10855457\n",
            "Iteration 262, loss = 0.10914088\n",
            "Iteration 263, loss = 0.10964574\n",
            "Iteration 264, loss = 0.10788628\n",
            "Iteration 265, loss = 0.10724055\n",
            "Iteration 266, loss = 0.10860771\n",
            "Iteration 267, loss = inf\n",
            "Iteration 268, loss = 0.11078635\n",
            "Iteration 269, loss = 0.11295048\n",
            "Iteration 270, loss = 0.11350938\n",
            "Iteration 271, loss = 0.11238060\n",
            "Iteration 272, loss = inf\n",
            "Iteration 273, loss = 0.10757740\n",
            "Iteration 274, loss = 0.10617686\n",
            "Iteration 275, loss = 0.10514315\n",
            "Iteration 276, loss = 0.10488007\n",
            "Iteration 277, loss = 0.11245871\n",
            "Iteration 278, loss = 0.11076248\n",
            "Iteration 279, loss = 0.10570826\n",
            "Iteration 280, loss = 0.10531696\n",
            "Iteration 281, loss = 0.10726465\n",
            "Iteration 282, loss = 0.10662651\n",
            "Iteration 283, loss = 0.10532700\n",
            "Iteration 284, loss = 0.10741745\n",
            "Iteration 285, loss = 0.10522276\n",
            "Iteration 286, loss = 0.10651127\n",
            "Iteration 287, loss = 0.10621982\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.36831407\n",
            "Iteration 2, loss = 0.32370518\n",
            "Iteration 3, loss = 0.31186836\n",
            "Iteration 4, loss = 0.30610610\n",
            "Iteration 5, loss = 0.30141432\n",
            "Iteration 6, loss = 0.29636183\n",
            "Iteration 7, loss = 0.29293919\n",
            "Iteration 8, loss = 0.28968332\n",
            "Iteration 9, loss = 0.28676700\n",
            "Iteration 10, loss = 0.28561818\n",
            "Iteration 11, loss = 0.28182846\n",
            "Iteration 12, loss = 0.28076045\n",
            "Iteration 13, loss = 0.27818392\n",
            "Iteration 14, loss = 0.27637020\n",
            "Iteration 15, loss = 0.27422818\n",
            "Iteration 16, loss = 0.27088901\n",
            "Iteration 17, loss = 0.27022942\n",
            "Iteration 18, loss = 0.26670621\n",
            "Iteration 19, loss = 0.26512016\n",
            "Iteration 20, loss = 0.26338973\n",
            "Iteration 21, loss = 0.26083941\n",
            "Iteration 22, loss = 0.25938366\n",
            "Iteration 23, loss = 0.25643352\n",
            "Iteration 24, loss = 0.25543958\n",
            "Iteration 25, loss = 0.25252032\n",
            "Iteration 26, loss = 0.25045884\n",
            "Iteration 27, loss = 0.25064837\n",
            "Iteration 28, loss = 0.24674394\n",
            "Iteration 29, loss = 0.24430018\n",
            "Iteration 30, loss = 0.24167082\n",
            "Iteration 31, loss = 0.23970999\n",
            "Iteration 32, loss = 0.23786984\n",
            "Iteration 33, loss = 0.23653125\n",
            "Iteration 34, loss = 0.23515228\n",
            "Iteration 35, loss = 0.23276257\n",
            "Iteration 36, loss = 0.23016617\n",
            "Iteration 37, loss = 0.22980186\n",
            "Iteration 38, loss = 0.22711232\n",
            "Iteration 39, loss = 0.22651519\n",
            "Iteration 40, loss = 0.22417490\n",
            "Iteration 41, loss = 0.22189097\n",
            "Iteration 42, loss = 0.22174621\n",
            "Iteration 43, loss = 0.22063277\n",
            "Iteration 44, loss = 0.21826747\n",
            "Iteration 45, loss = 0.21477565\n",
            "Iteration 46, loss = 0.21455986\n",
            "Iteration 47, loss = 0.21239385\n",
            "Iteration 48, loss = 0.21092059\n",
            "Iteration 49, loss = 0.20903542\n",
            "Iteration 50, loss = 0.20818080\n",
            "Iteration 51, loss = 0.20695557\n",
            "Iteration 52, loss = 0.20607956\n",
            "Iteration 53, loss = 0.20522825\n",
            "Iteration 54, loss = 0.20342924\n",
            "Iteration 55, loss = 0.20195547\n",
            "Iteration 56, loss = 0.20012775\n",
            "Iteration 57, loss = 0.19873482\n",
            "Iteration 58, loss = 0.19748388\n",
            "Iteration 59, loss = 0.19675473\n",
            "Iteration 60, loss = 0.19630506\n",
            "Iteration 61, loss = 0.19572400\n",
            "Iteration 62, loss = 0.19388697\n",
            "Iteration 63, loss = 0.19116627\n",
            "Iteration 64, loss = 0.18978534\n",
            "Iteration 65, loss = 0.18962288\n",
            "Iteration 66, loss = 0.19142806\n",
            "Iteration 67, loss = 0.18882555\n",
            "Iteration 68, loss = 0.18647513\n",
            "Iteration 69, loss = 0.18506234\n",
            "Iteration 70, loss = 0.18619322\n",
            "Iteration 71, loss = 0.18254204\n",
            "Iteration 72, loss = 0.18256631\n",
            "Iteration 73, loss = 0.18186191\n",
            "Iteration 74, loss = 0.18079644\n",
            "Iteration 75, loss = 0.18041485\n",
            "Iteration 76, loss = 0.18013569\n",
            "Iteration 77, loss = 0.17779063\n",
            "Iteration 78, loss = 0.17837535\n",
            "Iteration 79, loss = 0.17709828\n",
            "Iteration 80, loss = 0.17588808\n",
            "Iteration 81, loss = 0.17473643\n",
            "Iteration 82, loss = 0.17235215\n",
            "Iteration 83, loss = 0.17323546\n",
            "Iteration 84, loss = 0.17127589\n",
            "Iteration 85, loss = 0.17115823\n",
            "Iteration 86, loss = 0.17110410\n",
            "Iteration 87, loss = 0.16848418\n",
            "Iteration 88, loss = 0.17033152\n",
            "Iteration 89, loss = 0.16916607\n",
            "Iteration 90, loss = 0.16648312\n",
            "Iteration 91, loss = 0.16673178\n",
            "Iteration 92, loss = 0.16631539\n",
            "Iteration 93, loss = 0.16691263\n",
            "Iteration 94, loss = 0.16492381\n",
            "Iteration 95, loss = 0.16525825\n",
            "Iteration 96, loss = 0.16198688\n",
            "Iteration 97, loss = 0.16209217\n",
            "Iteration 98, loss = 0.16094111\n",
            "Iteration 99, loss = 0.16091164\n",
            "Iteration 100, loss = 0.16029565\n",
            "Iteration 101, loss = 0.16148845\n",
            "Iteration 102, loss = 0.16130199\n",
            "Iteration 103, loss = 0.15782167\n",
            "Iteration 104, loss = 0.15905967\n",
            "Iteration 105, loss = 0.15694506\n",
            "Iteration 106, loss = 0.15586073\n",
            "Iteration 107, loss = 0.15849930\n",
            "Iteration 108, loss = 0.15555435\n",
            "Iteration 109, loss = 0.15656942\n",
            "Iteration 110, loss = 0.15525501\n",
            "Iteration 111, loss = 0.15530146\n",
            "Iteration 112, loss = 0.15319736\n",
            "Iteration 113, loss = 0.15281501\n",
            "Iteration 114, loss = 0.15221581\n",
            "Iteration 115, loss = 0.15388505\n",
            "Iteration 116, loss = 0.15023323\n",
            "Iteration 117, loss = 0.15020059\n",
            "Iteration 118, loss = 0.15056004\n",
            "Iteration 119, loss = 0.14783870\n",
            "Iteration 120, loss = 0.15025360\n",
            "Iteration 121, loss = 0.14703327\n",
            "Iteration 122, loss = 0.14970806\n",
            "Iteration 123, loss = 0.14806370\n",
            "Iteration 124, loss = 0.15036661\n",
            "Iteration 125, loss = 0.14869160\n",
            "Iteration 126, loss = 0.14697631\n",
            "Iteration 127, loss = 0.14705694\n",
            "Iteration 128, loss = 0.14496172\n",
            "Iteration 129, loss = 0.14618009\n",
            "Iteration 130, loss = 0.14451827\n",
            "Iteration 131, loss = 0.14311605\n",
            "Iteration 132, loss = inf\n",
            "Iteration 133, loss = 0.14651408\n",
            "Iteration 134, loss = 0.14618642\n",
            "Iteration 135, loss = 0.14279242\n",
            "Iteration 136, loss = 0.14233890\n",
            "Iteration 137, loss = 0.14505308\n",
            "Iteration 138, loss = 0.14156846\n",
            "Iteration 139, loss = 0.13965004\n",
            "Iteration 140, loss = 0.14111115\n",
            "Iteration 141, loss = 0.14315087\n",
            "Iteration 142, loss = 0.13922920\n",
            "Iteration 143, loss = 0.14025728\n",
            "Iteration 144, loss = 0.13904259\n",
            "Iteration 145, loss = 0.14034869\n",
            "Iteration 146, loss = 0.13815819\n",
            "Iteration 147, loss = 0.13871931\n",
            "Iteration 148, loss = 0.13953621\n",
            "Iteration 149, loss = 0.13919102\n",
            "Iteration 150, loss = 0.13801371\n",
            "Iteration 151, loss = 0.13529778\n",
            "Iteration 152, loss = 0.13521856\n",
            "Iteration 153, loss = 0.13472479\n",
            "Iteration 154, loss = 0.13727990\n",
            "Iteration 155, loss = 0.13531610\n",
            "Iteration 156, loss = 0.13612479\n",
            "Iteration 157, loss = 0.13804286\n",
            "Iteration 158, loss = 0.13239059\n",
            "Iteration 159, loss = 0.13473140\n",
            "Iteration 160, loss = 0.13370713\n",
            "Iteration 161, loss = 0.13199748\n",
            "Iteration 162, loss = 0.13273691\n",
            "Iteration 163, loss = 0.13231069\n",
            "Iteration 164, loss = 0.13521413\n",
            "Iteration 165, loss = 0.13168820\n",
            "Iteration 166, loss = 0.13425656\n",
            "Iteration 167, loss = 0.13113577\n",
            "Iteration 168, loss = 0.13092799\n",
            "Iteration 169, loss = 0.12990577\n",
            "Iteration 170, loss = 0.12993964\n",
            "Iteration 171, loss = 0.13061969\n",
            "Iteration 172, loss = 0.12998525\n",
            "Iteration 173, loss = 0.12884723\n",
            "Iteration 174, loss = 0.12861578\n",
            "Iteration 175, loss = 0.13051868\n",
            "Iteration 176, loss = 0.12694780\n",
            "Iteration 177, loss = 0.12735162\n",
            "Iteration 178, loss = 0.12804925\n",
            "Iteration 179, loss = 0.12614367\n",
            "Iteration 180, loss = 0.12983358\n",
            "Iteration 181, loss = 0.12734609\n",
            "Iteration 182, loss = 0.12699498\n",
            "Iteration 183, loss = 0.12653172\n",
            "Iteration 184, loss = 0.12557019\n",
            "Iteration 185, loss = 0.12431120\n",
            "Iteration 186, loss = 0.12643196\n",
            "Iteration 187, loss = 0.12514315\n",
            "Iteration 188, loss = 0.12687643\n",
            "Iteration 189, loss = 0.12674917\n",
            "Iteration 190, loss = 0.12493223\n",
            "Iteration 191, loss = 0.12376006\n",
            "Iteration 192, loss = 0.12283310\n",
            "Iteration 193, loss = 0.12269321\n",
            "Iteration 194, loss = 0.12498614\n",
            "Iteration 195, loss = 0.12168748\n",
            "Iteration 196, loss = 0.12335056\n",
            "Iteration 197, loss = 0.12214192\n",
            "Iteration 198, loss = 0.12105226\n",
            "Iteration 199, loss = 0.12204511\n",
            "Iteration 200, loss = 0.12250702\n",
            "Iteration 201, loss = 0.12156396\n",
            "Iteration 202, loss = 0.12074078\n",
            "Iteration 203, loss = 0.12077332\n",
            "Iteration 204, loss = 0.12114837\n",
            "Iteration 205, loss = 0.12093716\n",
            "Iteration 206, loss = 0.12024601\n",
            "Iteration 207, loss = 0.11970982\n",
            "Iteration 208, loss = 0.11870227\n",
            "Iteration 209, loss = 0.11907321\n",
            "Iteration 210, loss = 0.12006503\n",
            "Iteration 211, loss = 0.12095606\n",
            "Iteration 212, loss = 0.11854781\n",
            "Iteration 213, loss = 0.11831017\n",
            "Iteration 214, loss = 0.11743841\n",
            "Iteration 215, loss = 0.11834287\n",
            "Iteration 216, loss = 0.11814783\n",
            "Iteration 217, loss = 0.11565753\n",
            "Iteration 218, loss = 0.11898480\n",
            "Iteration 219, loss = 0.11857650\n",
            "Iteration 220, loss = 0.11598803\n",
            "Iteration 221, loss = 0.11544587\n",
            "Iteration 222, loss = 0.11697500\n",
            "Iteration 223, loss = 0.11430037\n",
            "Iteration 224, loss = 0.11702102\n",
            "Iteration 225, loss = 0.11755090\n",
            "Iteration 226, loss = 0.11736191\n",
            "Iteration 227, loss = 0.12100119\n",
            "Iteration 228, loss = 0.11961177\n",
            "Iteration 229, loss = 0.11602963\n",
            "Iteration 230, loss = 0.11538195\n",
            "Iteration 231, loss = 0.11605964\n",
            "Iteration 232, loss = 0.11556037\n",
            "Iteration 233, loss = 0.11395469\n",
            "Iteration 234, loss = 0.11325014\n",
            "Iteration 235, loss = 0.11504090\n",
            "Iteration 236, loss = 0.11527440\n",
            "Iteration 237, loss = 0.11433177\n",
            "Iteration 238, loss = 0.11204858\n",
            "Iteration 239, loss = 0.11372961\n",
            "Iteration 240, loss = 0.11251036\n",
            "Iteration 241, loss = 0.11119145\n",
            "Iteration 242, loss = 0.11068402\n",
            "Iteration 243, loss = 0.11254290\n",
            "Iteration 244, loss = 0.11738570\n",
            "Iteration 245, loss = 0.11474602\n",
            "Iteration 246, loss = 0.10899660\n",
            "Iteration 247, loss = 0.11066590\n",
            "Iteration 248, loss = 0.11052661\n",
            "Iteration 249, loss = 0.11000255\n",
            "Iteration 250, loss = 0.11117052\n",
            "Iteration 251, loss = 0.11115458\n",
            "Iteration 252, loss = 0.11055804\n",
            "Iteration 253, loss = 0.11375841\n",
            "Iteration 254, loss = 0.11123960\n",
            "Iteration 255, loss = 0.11364544\n",
            "Iteration 256, loss = 0.11249830\n",
            "Iteration 257, loss = 0.11826107\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.37626232\n",
            "Iteration 2, loss = 0.32204412\n",
            "Iteration 3, loss = 0.31166057\n",
            "Iteration 4, loss = 0.30429896\n",
            "Iteration 5, loss = 0.29939232\n",
            "Iteration 6, loss = 0.29470559\n",
            "Iteration 7, loss = 0.29215932\n",
            "Iteration 8, loss = 0.28869409\n",
            "Iteration 9, loss = 0.28575783\n",
            "Iteration 10, loss = 0.28238268\n",
            "Iteration 11, loss = 0.27958058\n",
            "Iteration 12, loss = 0.27748182\n",
            "Iteration 13, loss = 0.27512351\n",
            "Iteration 14, loss = 0.27190220\n",
            "Iteration 15, loss = 0.26946747\n",
            "Iteration 16, loss = 0.26763788\n",
            "Iteration 17, loss = 0.26454345\n",
            "Iteration 18, loss = 0.26161550\n",
            "Iteration 19, loss = 0.25928685\n",
            "Iteration 20, loss = 0.25702837\n",
            "Iteration 21, loss = 0.25413500\n",
            "Iteration 22, loss = 0.25201834\n",
            "Iteration 23, loss = 0.24900800\n",
            "Iteration 24, loss = 0.24822020\n",
            "Iteration 25, loss = 0.24504666\n",
            "Iteration 26, loss = 0.24212083\n",
            "Iteration 27, loss = 0.23945087\n",
            "Iteration 28, loss = 0.23825134\n",
            "Iteration 29, loss = 0.23597595\n",
            "Iteration 30, loss = 0.23273559\n",
            "Iteration 31, loss = 0.23239716\n",
            "Iteration 32, loss = 0.22897281\n",
            "Iteration 33, loss = 0.22755459\n",
            "Iteration 34, loss = 0.22552772\n",
            "Iteration 35, loss = 0.22443366\n",
            "Iteration 36, loss = 0.22131679\n",
            "Iteration 37, loss = 0.21920361\n",
            "Iteration 38, loss = 0.21820366\n",
            "Iteration 39, loss = 0.21557501\n",
            "Iteration 40, loss = 0.21337032\n",
            "Iteration 41, loss = 0.21287216\n",
            "Iteration 42, loss = 0.21036560\n",
            "Iteration 43, loss = 0.21000924\n",
            "Iteration 44, loss = 0.20755133\n",
            "Iteration 45, loss = 0.20630694\n",
            "Iteration 46, loss = 0.20503834\n",
            "Iteration 47, loss = 0.20320788\n",
            "Iteration 48, loss = 0.20298235\n",
            "Iteration 49, loss = 0.20130997\n",
            "Iteration 50, loss = 0.19935940\n",
            "Iteration 51, loss = 0.19738452\n",
            "Iteration 52, loss = 0.19744668\n",
            "Iteration 53, loss = 0.19482086\n",
            "Iteration 54, loss = 0.19556071\n",
            "Iteration 55, loss = 0.19238024\n",
            "Iteration 56, loss = 0.19040780\n",
            "Iteration 57, loss = 0.19035051\n",
            "Iteration 58, loss = 0.18907716\n",
            "Iteration 59, loss = 0.18894646\n",
            "Iteration 60, loss = 0.18837177\n",
            "Iteration 61, loss = 0.18668916\n",
            "Iteration 62, loss = 0.18471284\n",
            "Iteration 63, loss = 0.18341453\n",
            "Iteration 64, loss = 0.18310303\n",
            "Iteration 65, loss = 0.18280366\n",
            "Iteration 66, loss = 0.18194537\n",
            "Iteration 67, loss = 0.18035479\n",
            "Iteration 68, loss = 0.18004598\n",
            "Iteration 69, loss = 0.17687441\n",
            "Iteration 70, loss = 0.17681707\n",
            "Iteration 71, loss = 0.17572610\n",
            "Iteration 72, loss = 0.17534854\n",
            "Iteration 73, loss = 0.17507941\n",
            "Iteration 74, loss = 0.17339364\n",
            "Iteration 75, loss = 0.17275935\n",
            "Iteration 76, loss = 0.17228550\n",
            "Iteration 77, loss = 0.17159569\n",
            "Iteration 78, loss = 0.17085851\n",
            "Iteration 79, loss = 0.16946718\n",
            "Iteration 80, loss = 0.16855860\n",
            "Iteration 81, loss = 0.16782179\n",
            "Iteration 82, loss = 0.16904058\n",
            "Iteration 83, loss = 0.16785626\n",
            "Iteration 84, loss = 0.16674648\n",
            "Iteration 85, loss = 0.16684633\n",
            "Iteration 86, loss = 0.16363471\n",
            "Iteration 87, loss = 0.16362717\n",
            "Iteration 88, loss = 0.16194098\n",
            "Iteration 89, loss = 0.16049975\n",
            "Iteration 90, loss = 0.16175993\n",
            "Iteration 91, loss = 0.15991589\n",
            "Iteration 92, loss = 0.15939189\n",
            "Iteration 93, loss = 0.15915040\n",
            "Iteration 94, loss = 0.15806759\n",
            "Iteration 95, loss = 0.15882344\n",
            "Iteration 96, loss = 0.15740014\n",
            "Iteration 97, loss = 0.15804177\n",
            "Iteration 98, loss = 0.15860461\n",
            "Iteration 99, loss = 0.15630900\n",
            "Iteration 100, loss = 0.15705024\n",
            "Iteration 101, loss = 0.15804294\n",
            "Iteration 102, loss = 0.15376370\n",
            "Iteration 103, loss = 0.15373586\n",
            "Iteration 104, loss = 0.15215199\n",
            "Iteration 105, loss = 0.15408182\n",
            "Iteration 106, loss = 0.15274559\n",
            "Iteration 107, loss = 0.15374022\n",
            "Iteration 108, loss = 0.15042571\n",
            "Iteration 109, loss = 0.15124389\n",
            "Iteration 110, loss = 0.15028366\n",
            "Iteration 111, loss = 0.14812660\n",
            "Iteration 112, loss = 0.14808445\n",
            "Iteration 113, loss = 0.14959730\n",
            "Iteration 114, loss = 0.14813716\n",
            "Iteration 115, loss = 0.14700680\n",
            "Iteration 116, loss = 0.14619521\n",
            "Iteration 117, loss = 0.14593270\n",
            "Iteration 118, loss = 0.14932438\n",
            "Iteration 119, loss = 0.14421734\n",
            "Iteration 120, loss = 0.14637538\n",
            "Iteration 121, loss = 0.14314438\n",
            "Iteration 122, loss = 0.14368096\n",
            "Iteration 123, loss = 0.14333354\n",
            "Iteration 124, loss = 0.14392000\n",
            "Iteration 125, loss = 0.14179428\n",
            "Iteration 126, loss = 0.14348698\n",
            "Iteration 127, loss = 0.14174477\n",
            "Iteration 128, loss = 0.14167529\n",
            "Iteration 129, loss = 0.14097789\n",
            "Iteration 130, loss = 0.14048502\n",
            "Iteration 131, loss = 0.13980602\n",
            "Iteration 132, loss = 0.14031974\n",
            "Iteration 133, loss = 0.13967022\n",
            "Iteration 134, loss = 0.14120754\n",
            "Iteration 135, loss = 0.13887647\n",
            "Iteration 136, loss = 0.13997982\n",
            "Iteration 137, loss = 0.13883405\n",
            "Iteration 138, loss = 0.13628525\n",
            "Iteration 139, loss = 0.13805979\n",
            "Iteration 140, loss = 0.13628511\n",
            "Iteration 141, loss = 0.13570403\n",
            "Iteration 142, loss = 0.13606631\n",
            "Iteration 143, loss = 0.13807908\n",
            "Iteration 144, loss = 0.14226482\n",
            "Iteration 145, loss = 0.13627582\n",
            "Iteration 146, loss = 0.13419076\n",
            "Iteration 147, loss = 0.13348021\n",
            "Iteration 148, loss = 0.13261458\n",
            "Iteration 149, loss = 0.13423855\n",
            "Iteration 150, loss = 0.13153929\n",
            "Iteration 151, loss = 0.13019710\n",
            "Iteration 152, loss = 0.13183610\n",
            "Iteration 153, loss = 0.12966584\n",
            "Iteration 154, loss = 0.13319315\n",
            "Iteration 155, loss = 0.12899125\n",
            "Iteration 156, loss = 0.13213729\n",
            "Iteration 157, loss = 0.13026305\n",
            "Iteration 158, loss = 0.12947994\n",
            "Iteration 159, loss = 0.13116826\n",
            "Iteration 160, loss = 0.12875521\n",
            "Iteration 161, loss = 0.12870330\n",
            "Iteration 162, loss = 0.12889929\n",
            "Iteration 163, loss = 0.12975128\n",
            "Iteration 164, loss = 0.12866251\n",
            "Iteration 165, loss = 0.12865575\n",
            "Iteration 166, loss = 0.12924386\n",
            "Iteration 167, loss = 0.12636980\n",
            "Iteration 168, loss = 0.13067723\n",
            "Iteration 169, loss = 0.12779637\n",
            "Iteration 170, loss = 0.12749609\n",
            "Iteration 171, loss = 0.12442070\n",
            "Iteration 172, loss = 0.12448474\n",
            "Iteration 173, loss = 0.12380967\n",
            "Iteration 174, loss = 0.12517850\n",
            "Iteration 175, loss = 0.12690606\n",
            "Iteration 176, loss = 0.12702601\n",
            "Iteration 177, loss = 0.12255368\n",
            "Iteration 178, loss = 0.12182810\n",
            "Iteration 179, loss = 0.12520353\n",
            "Iteration 180, loss = 0.12496601\n",
            "Iteration 181, loss = 0.12135560\n",
            "Iteration 182, loss = 0.12313766\n",
            "Iteration 183, loss = 0.12587908\n",
            "Iteration 184, loss = 0.12272642\n",
            "Iteration 185, loss = 0.12270769\n",
            "Iteration 186, loss = 0.12272760\n",
            "Iteration 187, loss = 0.12134791\n",
            "Iteration 188, loss = 0.12219332\n",
            "Iteration 189, loss = 0.12223667\n",
            "Iteration 190, loss = 0.11971084\n",
            "Iteration 191, loss = 0.12089873\n",
            "Iteration 192, loss = 0.12100363\n",
            "Iteration 193, loss = 0.11975987\n",
            "Iteration 194, loss = 0.12091593\n",
            "Iteration 195, loss = 0.11943186\n",
            "Iteration 196, loss = 0.11959537\n",
            "Iteration 197, loss = 0.12052608\n",
            "Iteration 198, loss = 0.11859015\n",
            "Iteration 199, loss = 0.11817162\n",
            "Iteration 200, loss = 0.11866239\n",
            "Iteration 201, loss = 0.11963424\n",
            "Iteration 202, loss = 0.11743890\n",
            "Iteration 203, loss = 0.11985845\n",
            "Iteration 204, loss = 0.11968159\n",
            "Iteration 205, loss = 0.11740708\n",
            "Iteration 206, loss = 0.11901709\n",
            "Iteration 207, loss = 0.11666192\n",
            "Iteration 208, loss = 0.11635316\n",
            "Iteration 209, loss = 0.11542027\n",
            "Iteration 210, loss = 0.11705824\n",
            "Iteration 211, loss = 0.11826205\n",
            "Iteration 212, loss = 0.11730776\n",
            "Iteration 213, loss = 0.11559328\n",
            "Iteration 214, loss = 0.11433713\n",
            "Iteration 215, loss = 0.11563513\n",
            "Iteration 216, loss = 0.11456638\n",
            "Iteration 217, loss = 0.11535588\n",
            "Iteration 218, loss = 0.11562322\n",
            "Iteration 219, loss = 0.11407017\n",
            "Iteration 220, loss = 0.11604083\n",
            "Iteration 221, loss = 0.11546417\n",
            "Iteration 222, loss = 0.11561145\n",
            "Iteration 223, loss = 0.11394662\n",
            "Iteration 224, loss = 0.11539518\n",
            "Iteration 225, loss = 0.11630206\n",
            "Iteration 226, loss = 0.11231803\n",
            "Iteration 227, loss = 0.11405721\n",
            "Iteration 228, loss = 0.11472889\n",
            "Iteration 229, loss = 0.11307102\n",
            "Iteration 230, loss = 0.11084726\n",
            "Iteration 231, loss = 0.11182043\n",
            "Iteration 232, loss = 0.11409785\n",
            "Iteration 233, loss = 0.11161588\n",
            "Iteration 234, loss = 0.11290517\n",
            "Iteration 235, loss = 0.11110413\n",
            "Iteration 236, loss = 0.11099569\n",
            "Iteration 237, loss = 0.11273304\n",
            "Iteration 238, loss = 0.10996453\n",
            "Iteration 239, loss = 0.10870327\n",
            "Iteration 240, loss = 0.10972296\n",
            "Iteration 241, loss = 0.11067042\n",
            "Iteration 242, loss = 0.10889424\n",
            "Iteration 243, loss = 0.10925142\n",
            "Iteration 244, loss = 0.11122909\n",
            "Iteration 245, loss = 0.10850243\n",
            "Iteration 246, loss = 0.10993096\n",
            "Iteration 247, loss = 0.10922518\n",
            "Iteration 248, loss = 0.10909968\n",
            "Iteration 249, loss = 0.11189315\n",
            "Iteration 250, loss = 0.11048222\n",
            "Iteration 251, loss = inf\n",
            "Iteration 252, loss = 0.10872484\n",
            "Iteration 253, loss = 0.10868065\n",
            "Iteration 254, loss = 0.10720158\n",
            "Iteration 255, loss = 0.10625370\n",
            "Iteration 256, loss = 0.10920596\n",
            "Iteration 257, loss = 0.10713946\n",
            "Iteration 258, loss = 0.10755079\n",
            "Iteration 259, loss = 0.10944900\n",
            "Iteration 260, loss = 0.10848147\n",
            "Iteration 261, loss = 0.10713038\n",
            "Iteration 262, loss = 0.10641947\n",
            "Iteration 263, loss = 0.10710542\n",
            "Iteration 264, loss = 0.10755330\n",
            "Iteration 265, loss = 0.10774648\n",
            "Iteration 266, loss = 0.10600372\n",
            "Iteration 267, loss = 0.10585504\n",
            "Iteration 268, loss = 0.10648726\n",
            "Iteration 269, loss = 0.10749512\n",
            "Iteration 270, loss = 0.10717889\n",
            "Iteration 271, loss = 0.10665508\n",
            "Iteration 272, loss = 0.10695763\n",
            "Iteration 273, loss = 0.10762534\n",
            "Iteration 274, loss = 0.10709750\n",
            "Iteration 275, loss = 0.10565797\n",
            "Iteration 276, loss = 0.11002740\n",
            "Iteration 277, loss = 0.10907509\n",
            "Iteration 278, loss = 0.10238918\n",
            "Iteration 279, loss = 0.10991921\n",
            "Iteration 280, loss = 0.10711230\n",
            "Iteration 281, loss = 0.10229475\n",
            "Iteration 282, loss = 0.10281241\n",
            "Iteration 283, loss = 0.10372325\n",
            "Iteration 284, loss = 0.10544869\n",
            "Iteration 285, loss = 0.10549183\n",
            "Iteration 286, loss = 0.10215636\n",
            "Iteration 287, loss = 0.10337600\n",
            "Iteration 288, loss = 0.10163070\n",
            "Iteration 289, loss = 0.10115847\n",
            "Iteration 290, loss = 0.10424511\n",
            "Iteration 291, loss = 0.10687995\n",
            "Iteration 292, loss = 0.10340151\n",
            "Iteration 293, loss = 0.10134520\n",
            "Iteration 294, loss = 0.10075035\n",
            "Iteration 295, loss = 0.10482085\n",
            "Iteration 296, loss = 0.10121559\n",
            "Iteration 297, loss = 0.10274313\n",
            "Iteration 298, loss = 0.10158486\n",
            "Iteration 299, loss = 0.10049144\n",
            "Iteration 300, loss = 0.10084338\n",
            "Iteration 301, loss = 0.10368820\n",
            "Iteration 302, loss = 0.10040185\n",
            "Iteration 303, loss = 0.10091182\n",
            "Iteration 304, loss = 0.09972465\n",
            "Iteration 305, loss = 0.10372083\n",
            "Iteration 306, loss = 0.10192998\n",
            "Iteration 307, loss = 0.10282843\n",
            "Iteration 308, loss = 0.10120469\n",
            "Iteration 309, loss = 0.10121722\n",
            "Iteration 310, loss = 0.10182189\n",
            "Iteration 311, loss = 0.10099191\n",
            "Iteration 312, loss = 0.10053375\n",
            "Iteration 313, loss = 0.09802556\n",
            "Iteration 314, loss = 0.10028201\n",
            "Iteration 315, loss = 0.09686147\n",
            "Iteration 316, loss = 0.09977176\n",
            "Iteration 317, loss = 0.09853231\n",
            "Iteration 318, loss = 0.09823463\n",
            "Iteration 319, loss = 0.10155319\n",
            "Iteration 320, loss = 0.09827765\n",
            "Iteration 321, loss = 0.09723617\n",
            "Iteration 322, loss = 0.09809926\n",
            "Iteration 323, loss = 0.09946042\n",
            "Iteration 324, loss = 0.09662695\n",
            "Iteration 325, loss = 0.09669578\n",
            "Iteration 326, loss = 0.09819927\n",
            "Iteration 327, loss = 0.09713044\n",
            "Iteration 328, loss = 0.09827233\n",
            "Iteration 329, loss = 0.09748949\n",
            "Iteration 330, loss = 0.10023845\n",
            "Iteration 331, loss = 0.10505828\n",
            "Iteration 332, loss = 0.10485889\n",
            "Iteration 333, loss = 0.10278603\n",
            "Iteration 334, loss = 0.09721549\n",
            "Iteration 335, loss = 0.10096355\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.37609340\n",
            "Iteration 2, loss = 0.32303515\n",
            "Iteration 3, loss = 0.31208068\n",
            "Iteration 4, loss = 0.30540676\n",
            "Iteration 5, loss = 0.30023969\n",
            "Iteration 6, loss = 0.29657301\n",
            "Iteration 7, loss = 0.29399969\n",
            "Iteration 8, loss = 0.28871681\n",
            "Iteration 9, loss = 0.28735077\n",
            "Iteration 10, loss = 0.28467640\n",
            "Iteration 11, loss = 0.28240233\n",
            "Iteration 12, loss = 0.28021452\n",
            "Iteration 13, loss = 0.27711445\n",
            "Iteration 14, loss = 0.27419748\n",
            "Iteration 15, loss = 0.27203517\n",
            "Iteration 16, loss = 0.26978555\n",
            "Iteration 17, loss = 0.26777409\n",
            "Iteration 18, loss = 0.26435081\n",
            "Iteration 19, loss = 0.26302656\n",
            "Iteration 20, loss = 0.26015228\n",
            "Iteration 21, loss = 0.25797983\n",
            "Iteration 22, loss = 0.25579861\n",
            "Iteration 23, loss = 0.25327780\n",
            "Iteration 24, loss = 0.25092373\n",
            "Iteration 25, loss = 0.24847013\n",
            "Iteration 26, loss = 0.24660319\n",
            "Iteration 27, loss = 0.24341749\n",
            "Iteration 28, loss = 0.24216338\n",
            "Iteration 29, loss = 0.23907827\n",
            "Iteration 30, loss = 0.23788103\n",
            "Iteration 31, loss = 0.23503344\n",
            "Iteration 32, loss = 0.23452414\n",
            "Iteration 33, loss = 0.23131422\n",
            "Iteration 34, loss = 0.22966887\n",
            "Iteration 35, loss = 0.22793812\n",
            "Iteration 36, loss = 0.22460550\n",
            "Iteration 37, loss = 0.22398990\n",
            "Iteration 38, loss = 0.22119653\n",
            "Iteration 39, loss = 0.21927992\n",
            "Iteration 40, loss = 0.21802969\n",
            "Iteration 41, loss = 0.21771365\n",
            "Iteration 42, loss = 0.21487508\n",
            "Iteration 43, loss = 0.21138515\n",
            "Iteration 44, loss = 0.21129920\n",
            "Iteration 45, loss = 0.20990913\n",
            "Iteration 46, loss = 0.20908861\n",
            "Iteration 47, loss = 0.20782042\n",
            "Iteration 48, loss = 0.20566990\n",
            "Iteration 49, loss = 0.20370034\n",
            "Iteration 50, loss = 0.20365285\n",
            "Iteration 51, loss = 0.20159798\n",
            "Iteration 52, loss = 0.20051837\n",
            "Iteration 53, loss = 0.19813884\n",
            "Iteration 54, loss = 0.19682723\n",
            "Iteration 55, loss = 0.19586439\n",
            "Iteration 56, loss = 0.19457431\n",
            "Iteration 57, loss = 0.19422686\n",
            "Iteration 58, loss = 0.19215142\n",
            "Iteration 59, loss = 0.19130541\n",
            "Iteration 60, loss = 0.18988536\n",
            "Iteration 61, loss = 0.18879881\n",
            "Iteration 62, loss = 0.18655088\n",
            "Iteration 63, loss = 0.18588811\n",
            "Iteration 64, loss = 0.18682482\n",
            "Iteration 65, loss = 0.18443390\n",
            "Iteration 66, loss = 0.18344320\n",
            "Iteration 67, loss = 0.18134932\n",
            "Iteration 68, loss = 0.18178592\n",
            "Iteration 69, loss = 0.17999258\n",
            "Iteration 70, loss = 0.17918140\n",
            "Iteration 71, loss = 0.18004559\n",
            "Iteration 72, loss = 0.17874170\n",
            "Iteration 73, loss = 0.17676553\n",
            "Iteration 74, loss = 0.17715425\n",
            "Iteration 75, loss = 0.17426896\n",
            "Iteration 76, loss = 0.17467301\n",
            "Iteration 77, loss = 0.17159976\n",
            "Iteration 78, loss = 0.17270676\n",
            "Iteration 79, loss = 0.17286970\n",
            "Iteration 80, loss = 0.17214406\n",
            "Iteration 81, loss = 0.17007660\n",
            "Iteration 82, loss = 0.16925729\n",
            "Iteration 83, loss = 0.16863422\n",
            "Iteration 84, loss = 0.16850360\n",
            "Iteration 85, loss = 0.16517937\n",
            "Iteration 86, loss = 0.16527735\n",
            "Iteration 87, loss = 0.16468231\n",
            "Iteration 88, loss = 0.16328830\n",
            "Iteration 89, loss = 0.16343944\n",
            "Iteration 90, loss = 0.16260572\n",
            "Iteration 91, loss = 0.16176681\n",
            "Iteration 92, loss = 0.16191568\n",
            "Iteration 93, loss = 0.16156927\n",
            "Iteration 94, loss = 0.16011282\n",
            "Iteration 95, loss = 0.15954692\n",
            "Iteration 96, loss = 0.15917561\n",
            "Iteration 97, loss = 0.15813692\n",
            "Iteration 98, loss = 0.15684817\n",
            "Iteration 99, loss = 0.15661299\n",
            "Iteration 100, loss = 0.15591990\n",
            "Iteration 101, loss = 0.15671845\n",
            "Iteration 102, loss = 0.15635479\n",
            "Iteration 103, loss = 0.15531615\n",
            "Iteration 104, loss = 0.15363341\n",
            "Iteration 105, loss = 0.15320384\n",
            "Iteration 106, loss = 0.15145156\n",
            "Iteration 107, loss = 0.15289981\n",
            "Iteration 108, loss = 0.15147204\n",
            "Iteration 109, loss = 0.15152109\n",
            "Iteration 110, loss = 0.15023371\n",
            "Iteration 111, loss = 0.15023128\n",
            "Iteration 112, loss = 0.14871610\n",
            "Iteration 113, loss = 0.14976101\n",
            "Iteration 114, loss = 0.14907511\n",
            "Iteration 115, loss = 0.14833228\n",
            "Iteration 116, loss = 0.14796608\n",
            "Iteration 117, loss = 0.14691819\n",
            "Iteration 118, loss = 0.14494001\n",
            "Iteration 119, loss = 0.14395760\n",
            "Iteration 120, loss = 0.14495905\n",
            "Iteration 121, loss = 0.14628185\n",
            "Iteration 122, loss = 0.14511726\n",
            "Iteration 123, loss = 0.14629055\n",
            "Iteration 124, loss = 0.14518444\n",
            "Iteration 125, loss = 0.14356180\n",
            "Iteration 126, loss = 0.14327957\n",
            "Iteration 127, loss = 0.14297470\n",
            "Iteration 128, loss = 0.14150240\n",
            "Iteration 129, loss = 0.14112865\n",
            "Iteration 130, loss = 0.14195324\n",
            "Iteration 131, loss = 0.14144990\n",
            "Iteration 132, loss = 0.14076131\n",
            "Iteration 133, loss = 0.14076927\n",
            "Iteration 134, loss = 0.13953847\n",
            "Iteration 135, loss = 0.13817736\n",
            "Iteration 136, loss = 0.13737215\n",
            "Iteration 137, loss = 0.13737983\n",
            "Iteration 138, loss = 0.13808567\n",
            "Iteration 139, loss = 0.13599334\n",
            "Iteration 140, loss = 0.13926474\n",
            "Iteration 141, loss = 0.13596992\n",
            "Iteration 142, loss = 0.13718997\n",
            "Iteration 143, loss = 0.13672871\n",
            "Iteration 144, loss = 0.13528203\n",
            "Iteration 145, loss = 0.13496544\n",
            "Iteration 146, loss = 0.13608784\n",
            "Iteration 147, loss = 0.13502513\n",
            "Iteration 148, loss = 0.13903799\n",
            "Iteration 149, loss = 0.13398943\n",
            "Iteration 150, loss = 0.13196693\n",
            "Iteration 151, loss = 0.13102449\n",
            "Iteration 152, loss = 0.13113320\n",
            "Iteration 153, loss = 0.13436149\n",
            "Iteration 154, loss = 0.13231766\n",
            "Iteration 155, loss = 0.13238526\n",
            "Iteration 156, loss = 0.13154409\n",
            "Iteration 157, loss = 0.13143378\n",
            "Iteration 158, loss = 0.13390385\n",
            "Iteration 159, loss = 0.12997725\n",
            "Iteration 160, loss = 0.12968190\n",
            "Iteration 161, loss = 0.13261730\n",
            "Iteration 162, loss = 0.13208287\n",
            "Iteration 163, loss = 0.12788836\n",
            "Iteration 164, loss = 0.12951926\n",
            "Iteration 165, loss = 0.12828981\n",
            "Iteration 166, loss = 0.12702630\n",
            "Iteration 167, loss = 0.12824381\n",
            "Iteration 168, loss = 0.12709197\n",
            "Iteration 169, loss = 0.12822235\n",
            "Iteration 170, loss = 0.12591869\n",
            "Iteration 171, loss = 0.12475599\n",
            "Iteration 172, loss = 0.12621707\n",
            "Iteration 173, loss = 0.12768811\n",
            "Iteration 174, loss = 0.12469987\n",
            "Iteration 175, loss = 0.12371497\n",
            "Iteration 176, loss = 0.12584955\n",
            "Iteration 177, loss = 0.12412743\n",
            "Iteration 178, loss = 0.12377417\n",
            "Iteration 179, loss = 0.12378015\n",
            "Iteration 180, loss = 0.12339925\n",
            "Iteration 181, loss = 0.12369954\n",
            "Iteration 182, loss = 0.12307033\n",
            "Iteration 183, loss = 0.12212248\n",
            "Iteration 184, loss = 0.12366977\n",
            "Iteration 185, loss = 0.12142421\n",
            "Iteration 186, loss = 0.12329777\n",
            "Iteration 187, loss = 0.12057151\n",
            "Iteration 188, loss = 0.12304545\n",
            "Iteration 189, loss = 0.12574455\n",
            "Iteration 190, loss = 0.12175407\n",
            "Iteration 191, loss = 0.11984842\n",
            "Iteration 192, loss = 0.12001121\n",
            "Iteration 193, loss = 0.11879235\n",
            "Iteration 194, loss = 0.12047534\n",
            "Iteration 195, loss = 0.12128420\n",
            "Iteration 196, loss = 0.11840133\n",
            "Iteration 197, loss = 0.11885609\n",
            "Iteration 198, loss = 0.11717509\n",
            "Iteration 199, loss = 0.11700611\n",
            "Iteration 200, loss = 0.11708720\n",
            "Iteration 201, loss = 0.11827921\n",
            "Iteration 202, loss = 0.11808028\n",
            "Iteration 203, loss = 0.12158067\n",
            "Iteration 204, loss = 0.11949334\n",
            "Iteration 205, loss = 0.11794858\n",
            "Iteration 206, loss = 0.11912104\n",
            "Iteration 207, loss = 0.11597296\n",
            "Iteration 208, loss = 0.11599578\n",
            "Iteration 209, loss = 0.11501277\n",
            "Iteration 210, loss = 0.11584862\n",
            "Iteration 211, loss = 0.11726080\n",
            "Iteration 212, loss = 0.11629028\n",
            "Iteration 213, loss = 0.11558156\n",
            "Iteration 214, loss = 0.11642539\n",
            "Iteration 215, loss = 0.11654003\n",
            "Iteration 216, loss = 0.11486829\n",
            "Iteration 217, loss = 0.11647032\n",
            "Iteration 218, loss = 0.11495456\n",
            "Iteration 219, loss = 0.11509405\n",
            "Iteration 220, loss = 0.11433408\n",
            "Iteration 221, loss = 0.11343742\n",
            "Iteration 222, loss = 0.11236212\n",
            "Iteration 223, loss = 0.11151371\n",
            "Iteration 224, loss = 0.11490394\n",
            "Iteration 225, loss = 0.11246539\n",
            "Iteration 226, loss = 0.11393199\n",
            "Iteration 227, loss = 0.11218647\n",
            "Iteration 228, loss = 0.11569725\n",
            "Iteration 229, loss = 0.11373099\n",
            "Iteration 230, loss = 0.11117781\n",
            "Iteration 231, loss = 0.11172357\n",
            "Iteration 232, loss = 0.11169418\n",
            "Iteration 233, loss = 0.11008254\n",
            "Iteration 234, loss = 0.10971284\n",
            "Iteration 235, loss = 0.11061622\n",
            "Iteration 236, loss = 0.11111450\n",
            "Iteration 237, loss = 0.11077795\n",
            "Iteration 238, loss = 0.11307774\n",
            "Iteration 239, loss = 0.10972286\n",
            "Iteration 240, loss = 0.10802662\n",
            "Iteration 241, loss = 0.10934069\n",
            "Iteration 242, loss = 0.10960557\n",
            "Iteration 243, loss = 0.10832928\n",
            "Iteration 244, loss = 0.10863523\n",
            "Iteration 245, loss = 0.10959263\n",
            "Iteration 246, loss = 0.10766167\n",
            "Iteration 247, loss = 0.10991601\n",
            "Iteration 248, loss = 0.10725878\n",
            "Iteration 249, loss = 0.10981493\n",
            "Iteration 250, loss = 0.11032790\n",
            "Iteration 251, loss = 0.10784015\n",
            "Iteration 252, loss = 0.10908724\n",
            "Iteration 253, loss = 0.10875127\n",
            "Iteration 254, loss = 0.10707809\n",
            "Iteration 255, loss = 0.10647651\n",
            "Iteration 256, loss = 0.10870377\n",
            "Iteration 257, loss = 0.11084535\n",
            "Iteration 258, loss = 0.10929445\n",
            "Iteration 259, loss = 0.10912188\n",
            "Iteration 260, loss = 0.10630443\n",
            "Iteration 261, loss = 0.10921216\n",
            "Iteration 262, loss = 0.10684481\n",
            "Iteration 263, loss = 0.10488156\n",
            "Iteration 264, loss = 0.10432198\n",
            "Iteration 265, loss = 0.10377110\n",
            "Iteration 266, loss = 0.10526596\n",
            "Iteration 267, loss = 0.10332181\n",
            "Iteration 268, loss = 0.10675082\n",
            "Iteration 269, loss = 0.10373949\n",
            "Iteration 270, loss = 0.10390721\n",
            "Iteration 271, loss = 0.10694907\n",
            "Iteration 272, loss = 0.10377553\n",
            "Iteration 273, loss = 0.10660585\n",
            "Iteration 274, loss = 0.10342446\n",
            "Iteration 275, loss = 0.10518645\n",
            "Iteration 276, loss = 0.10393738\n",
            "Iteration 277, loss = 0.10263348\n",
            "Iteration 278, loss = 0.10215729\n",
            "Iteration 279, loss = 0.10284251\n",
            "Iteration 280, loss = 0.10383321\n",
            "Iteration 281, loss = 0.10139801\n",
            "Iteration 282, loss = 0.10156202\n",
            "Iteration 283, loss = 0.10250684\n",
            "Iteration 284, loss = 0.10247072\n",
            "Iteration 285, loss = 0.10365069\n",
            "Iteration 286, loss = 0.10169730\n",
            "Iteration 287, loss = 0.10164223\n",
            "Iteration 288, loss = 0.10235881\n",
            "Iteration 289, loss = 0.10424756\n",
            "Iteration 290, loss = 0.10457312\n",
            "Iteration 291, loss = 0.10036439\n",
            "Iteration 292, loss = 0.10251463\n",
            "Iteration 293, loss = 0.10009555\n",
            "Iteration 294, loss = 0.10011920\n",
            "Iteration 295, loss = 0.10083273\n",
            "Iteration 296, loss = 0.10143011\n",
            "Iteration 297, loss = 0.10266323\n",
            "Iteration 298, loss = 0.10066720\n",
            "Iteration 299, loss = 0.09843028\n",
            "Iteration 300, loss = 0.10169554\n",
            "Iteration 301, loss = 0.10112864\n",
            "Iteration 302, loss = 0.10133198\n",
            "Iteration 303, loss = 0.10019505\n",
            "Iteration 304, loss = 0.09942593\n",
            "Iteration 305, loss = 0.10001118\n",
            "Iteration 306, loss = 0.09869646\n",
            "Iteration 307, loss = 0.09935546\n",
            "Iteration 308, loss = 0.10364349\n",
            "Iteration 309, loss = 0.10111290\n",
            "Iteration 310, loss = 0.09717227\n",
            "Iteration 311, loss = 0.09751514\n",
            "Iteration 312, loss = 0.09697931\n",
            "Iteration 313, loss = 0.09670127\n",
            "Iteration 314, loss = 0.09780458\n",
            "Iteration 315, loss = 0.10000300\n",
            "Iteration 316, loss = 0.09812817\n",
            "Iteration 317, loss = 0.09803659\n",
            "Iteration 318, loss = 0.09958878\n",
            "Iteration 319, loss = 0.09805566\n",
            "Iteration 320, loss = 0.09920492\n",
            "Iteration 321, loss = 0.10579656\n",
            "Iteration 322, loss = 0.09810665\n",
            "Iteration 323, loss = 0.09816635\n",
            "Iteration 324, loss = 0.09463766\n",
            "Iteration 325, loss = 0.09653192\n",
            "Iteration 326, loss = 0.09440964\n",
            "Iteration 327, loss = 0.09539476\n",
            "Iteration 328, loss = 0.09498320\n",
            "Iteration 329, loss = 0.09485578\n",
            "Iteration 330, loss = 0.09722966\n",
            "Iteration 331, loss = 0.09476036\n",
            "Iteration 332, loss = 0.09856527\n",
            "Iteration 333, loss = 0.09676983\n",
            "Iteration 334, loss = 0.09925423\n",
            "Iteration 335, loss = 0.09948113\n",
            "Iteration 336, loss = 0.09732549\n",
            "Iteration 337, loss = 0.09705935\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.37496925\n",
            "Iteration 2, loss = 0.32370914\n",
            "Iteration 3, loss = 0.31262924\n",
            "Iteration 4, loss = 0.30479653\n",
            "Iteration 5, loss = 0.29979235\n",
            "Iteration 6, loss = 0.29548674\n",
            "Iteration 7, loss = 0.29216557\n",
            "Iteration 8, loss = 0.29021057\n",
            "Iteration 9, loss = 0.28621101\n",
            "Iteration 10, loss = 0.28293694\n",
            "Iteration 11, loss = 0.28112132\n",
            "Iteration 12, loss = 0.27786387\n",
            "Iteration 13, loss = 0.27594511\n",
            "Iteration 14, loss = 0.27433403\n",
            "Iteration 15, loss = 0.27217411\n",
            "Iteration 16, loss = 0.26922280\n",
            "Iteration 17, loss = 0.26627863\n",
            "Iteration 18, loss = 0.26503421\n",
            "Iteration 19, loss = 0.26206325\n",
            "Iteration 20, loss = 0.25957347\n",
            "Iteration 21, loss = 0.25938167\n",
            "Iteration 22, loss = 0.25638388\n",
            "Iteration 23, loss = 0.25391878\n",
            "Iteration 24, loss = 0.25008192\n",
            "Iteration 25, loss = 0.25017795\n",
            "Iteration 26, loss = 0.24612886\n",
            "Iteration 27, loss = 0.24431569\n",
            "Iteration 28, loss = 0.24033421\n",
            "Iteration 29, loss = 0.23981806\n",
            "Iteration 30, loss = 0.23886461\n",
            "Iteration 31, loss = 0.23607884\n",
            "Iteration 32, loss = 0.23341253\n",
            "Iteration 33, loss = 0.23148429\n",
            "Iteration 34, loss = 0.22972204\n",
            "Iteration 35, loss = 0.22824897\n",
            "Iteration 36, loss = 0.22533904\n",
            "Iteration 37, loss = 0.22386404\n",
            "Iteration 38, loss = 0.22216558\n",
            "Iteration 39, loss = 0.22060782\n",
            "Iteration 40, loss = 0.21908674\n",
            "Iteration 41, loss = 0.21792213\n",
            "Iteration 42, loss = 0.21571305\n",
            "Iteration 43, loss = 0.21405736\n",
            "Iteration 44, loss = 0.21273284\n",
            "Iteration 45, loss = 0.21009985\n",
            "Iteration 46, loss = 0.20831202\n",
            "Iteration 47, loss = 0.20798791\n",
            "Iteration 48, loss = 0.20629008\n",
            "Iteration 49, loss = 0.20442487\n",
            "Iteration 50, loss = 0.20300236\n",
            "Iteration 51, loss = 0.20113679\n",
            "Iteration 52, loss = 0.19858195\n",
            "Iteration 53, loss = 0.19887252\n",
            "Iteration 54, loss = 0.19827092\n",
            "Iteration 55, loss = 0.19681484\n",
            "Iteration 56, loss = 0.19472178\n",
            "Iteration 57, loss = 0.19386722\n",
            "Iteration 58, loss = 0.19168558\n",
            "Iteration 59, loss = 0.19210292\n",
            "Iteration 60, loss = 0.19021001\n",
            "Iteration 61, loss = 0.18871306\n",
            "Iteration 62, loss = 0.18974509\n",
            "Iteration 63, loss = 0.18655452\n",
            "Iteration 64, loss = 0.18468485\n",
            "Iteration 65, loss = 0.18432052\n",
            "Iteration 66, loss = 0.18305593\n",
            "Iteration 67, loss = 0.18387470\n",
            "Iteration 68, loss = 0.18431565\n",
            "Iteration 69, loss = 0.18235842\n",
            "Iteration 70, loss = 0.18087598\n",
            "Iteration 71, loss = 0.17851205\n",
            "Iteration 72, loss = 0.17833306\n",
            "Iteration 73, loss = 0.17688469\n",
            "Iteration 74, loss = 0.17742357\n",
            "Iteration 75, loss = 0.17416839\n",
            "Iteration 76, loss = 0.17322425\n",
            "Iteration 77, loss = 0.17277582\n",
            "Iteration 78, loss = 0.17133064\n",
            "Iteration 79, loss = 0.17113185\n",
            "Iteration 80, loss = 0.17041054\n",
            "Iteration 81, loss = 0.16870135\n",
            "Iteration 82, loss = 0.16947315\n",
            "Iteration 83, loss = 0.16910018\n",
            "Iteration 84, loss = 0.17048414\n",
            "Iteration 85, loss = 0.17008436\n",
            "Iteration 86, loss = 0.16632125\n",
            "Iteration 87, loss = 0.16577364\n",
            "Iteration 88, loss = 0.16474487\n",
            "Iteration 89, loss = 0.16431255\n",
            "Iteration 90, loss = 0.16201815\n",
            "Iteration 91, loss = 0.16161270\n",
            "Iteration 92, loss = 0.16315108\n",
            "Iteration 93, loss = 0.16078014\n",
            "Iteration 94, loss = 0.15873719\n",
            "Iteration 95, loss = 0.15963570\n",
            "Iteration 96, loss = 0.15928620\n",
            "Iteration 97, loss = 0.15846433\n",
            "Iteration 98, loss = 0.15983686\n",
            "Iteration 99, loss = 0.15669015\n",
            "Iteration 100, loss = 0.15675523\n",
            "Iteration 101, loss = 0.15737943\n",
            "Iteration 102, loss = 0.15414421\n",
            "Iteration 103, loss = 0.15512924\n",
            "Iteration 104, loss = 0.15423050\n",
            "Iteration 105, loss = 0.15254141\n",
            "Iteration 106, loss = 0.15274920\n",
            "Iteration 107, loss = 0.15347013\n",
            "Iteration 108, loss = 0.15362111\n",
            "Iteration 109, loss = 0.15009613\n",
            "Iteration 110, loss = 0.15003185\n",
            "Iteration 111, loss = 0.14847722\n",
            "Iteration 112, loss = 0.15091153\n",
            "Iteration 113, loss = 0.14970943\n",
            "Iteration 114, loss = 0.15030517\n",
            "Iteration 115, loss = 0.14918553\n",
            "Iteration 116, loss = 0.14830278\n",
            "Iteration 117, loss = 0.14721735\n",
            "Iteration 118, loss = 0.14684651\n",
            "Iteration 119, loss = 0.14760548\n",
            "Iteration 120, loss = 0.14590514\n",
            "Iteration 121, loss = 0.14635546\n",
            "Iteration 122, loss = 0.14573864\n",
            "Iteration 123, loss = 0.14783248\n",
            "Iteration 124, loss = 0.14452299\n",
            "Iteration 125, loss = 0.14283922\n",
            "Iteration 126, loss = 0.14260557\n",
            "Iteration 127, loss = 0.14318305\n",
            "Iteration 128, loss = 0.14304104\n",
            "Iteration 129, loss = 0.14217183\n",
            "Iteration 130, loss = 0.14346756\n",
            "Iteration 131, loss = 0.14029394\n",
            "Iteration 132, loss = 0.14145699\n",
            "Iteration 133, loss = 0.14034114\n",
            "Iteration 134, loss = 0.13916466\n",
            "Iteration 135, loss = 0.13989448\n",
            "Iteration 136, loss = 0.13976017\n",
            "Iteration 137, loss = 0.14075091\n",
            "Iteration 138, loss = 0.13832634\n",
            "Iteration 139, loss = 0.13882210\n",
            "Iteration 140, loss = 0.13770561\n",
            "Iteration 141, loss = 0.13739851\n",
            "Iteration 142, loss = 0.13638343\n",
            "Iteration 143, loss = 0.13582414\n",
            "Iteration 144, loss = 0.13464661\n",
            "Iteration 145, loss = 0.13475270\n",
            "Iteration 146, loss = 0.13577044\n",
            "Iteration 147, loss = 0.13510787\n",
            "Iteration 148, loss = 0.13499870\n",
            "Iteration 149, loss = 0.13435273\n",
            "Iteration 150, loss = 0.13462755\n",
            "Iteration 151, loss = 0.13351047\n",
            "Iteration 152, loss = 0.13320109\n",
            "Iteration 153, loss = 0.13227460\n",
            "Iteration 154, loss = 0.13160426\n",
            "Iteration 155, loss = 0.13239047\n",
            "Iteration 156, loss = 0.13447688\n",
            "Iteration 157, loss = 0.13277066\n",
            "Iteration 158, loss = 0.13190605\n",
            "Iteration 159, loss = 0.13100467\n",
            "Iteration 160, loss = 0.13006064\n",
            "Iteration 161, loss = 0.13019354\n",
            "Iteration 162, loss = 0.12866393\n",
            "Iteration 163, loss = 0.13103978\n",
            "Iteration 164, loss = 0.12917335\n",
            "Iteration 165, loss = 0.12827186\n",
            "Iteration 166, loss = 0.12814694\n",
            "Iteration 167, loss = 0.12885649\n",
            "Iteration 168, loss = 0.12664997\n",
            "Iteration 169, loss = 0.12839351\n",
            "Iteration 170, loss = 0.12598250\n",
            "Iteration 171, loss = 0.12653845\n",
            "Iteration 172, loss = 0.12664798\n",
            "Iteration 173, loss = 0.12667360\n",
            "Iteration 174, loss = 0.12370478\n",
            "Iteration 175, loss = 0.12518789\n",
            "Iteration 176, loss = 0.12511357\n",
            "Iteration 177, loss = 0.12668878\n",
            "Iteration 178, loss = 0.12471914\n",
            "Iteration 179, loss = 0.12579517\n",
            "Iteration 180, loss = 0.12286298\n",
            "Iteration 181, loss = 0.12354693\n",
            "Iteration 182, loss = 0.12316217\n",
            "Iteration 183, loss = 0.12358525\n",
            "Iteration 184, loss = 0.12273309\n",
            "Iteration 185, loss = 0.12347908\n",
            "Iteration 186, loss = 0.12400509\n",
            "Iteration 187, loss = 0.12184226\n",
            "Iteration 188, loss = 0.12525416\n",
            "Iteration 189, loss = 0.12437083\n",
            "Iteration 190, loss = 0.12323433\n",
            "Iteration 191, loss = 0.12235348\n",
            "Iteration 192, loss = 0.12053677\n",
            "Iteration 193, loss = 0.12243926\n",
            "Iteration 194, loss = 0.12202444\n",
            "Iteration 195, loss = 0.11979820\n",
            "Iteration 196, loss = 0.12265943\n",
            "Iteration 197, loss = 0.12103773\n",
            "Iteration 198, loss = 0.11835853\n",
            "Iteration 199, loss = 0.11945066\n",
            "Iteration 200, loss = 0.11807231\n",
            "Iteration 201, loss = 0.11887949\n",
            "Iteration 202, loss = 0.11805774\n",
            "Iteration 203, loss = 0.11929872\n",
            "Iteration 204, loss = 0.12007471\n",
            "Iteration 205, loss = 0.11788694\n",
            "Iteration 206, loss = 0.11708301\n",
            "Iteration 207, loss = 0.11736516\n",
            "Iteration 208, loss = 0.11534285\n",
            "Iteration 209, loss = 0.11628484\n",
            "Iteration 210, loss = 0.11683502\n",
            "Iteration 211, loss = 0.11485912\n",
            "Iteration 212, loss = 0.11611252\n",
            "Iteration 213, loss = 0.12022848\n",
            "Iteration 214, loss = 0.11820503\n",
            "Iteration 215, loss = 0.11872724\n",
            "Iteration 216, loss = 0.11926091\n",
            "Iteration 217, loss = 0.12187133\n",
            "Iteration 218, loss = 0.11867984\n",
            "Iteration 219, loss = 0.11466463\n",
            "Iteration 220, loss = 0.11675801\n",
            "Iteration 221, loss = 0.11441688\n",
            "Iteration 222, loss = 0.11440601\n",
            "Iteration 223, loss = 0.11424478\n",
            "Iteration 224, loss = 0.11373153\n",
            "Iteration 225, loss = 0.11488168\n",
            "Iteration 226, loss = 0.11281897\n",
            "Iteration 227, loss = 0.11290902\n",
            "Iteration 228, loss = 0.11239045\n",
            "Iteration 229, loss = 0.11228460\n",
            "Iteration 230, loss = 0.11412070\n",
            "Iteration 231, loss = 0.11440261\n",
            "Iteration 232, loss = 0.11356333\n",
            "Iteration 233, loss = 0.11194782\n",
            "Iteration 234, loss = 0.11290944\n",
            "Iteration 235, loss = 0.11217348\n",
            "Iteration 236, loss = 0.11349022\n",
            "Iteration 237, loss = 0.11340551\n",
            "Iteration 238, loss = 0.11227346\n",
            "Iteration 239, loss = 0.11054758\n",
            "Iteration 240, loss = 0.11468197\n",
            "Iteration 241, loss = 0.11171137\n",
            "Iteration 242, loss = 0.11197147\n",
            "Iteration 243, loss = 0.10978207\n",
            "Iteration 244, loss = 0.10974574\n",
            "Iteration 245, loss = 0.11212569\n",
            "Iteration 246, loss = 0.11093033\n",
            "Iteration 247, loss = 0.11013515\n",
            "Iteration 248, loss = 0.10927058\n",
            "Iteration 249, loss = 0.10926294\n",
            "Iteration 250, loss = 0.10793268\n",
            "Iteration 251, loss = 0.10954769\n",
            "Iteration 252, loss = 0.10778603\n",
            "Iteration 253, loss = 0.10805633\n",
            "Iteration 254, loss = 0.11290369\n",
            "Iteration 255, loss = 0.10795670\n",
            "Iteration 256, loss = 0.10786714\n",
            "Iteration 257, loss = 0.10931217\n",
            "Iteration 258, loss = 0.10881017\n",
            "Iteration 259, loss = 0.10981356\n",
            "Iteration 260, loss = 0.10602722\n",
            "Iteration 261, loss = 0.10640920\n",
            "Iteration 262, loss = 0.10743381\n",
            "Iteration 263, loss = 0.10608653\n",
            "Iteration 264, loss = 0.10788529\n",
            "Iteration 265, loss = 0.10600108\n",
            "Iteration 266, loss = 0.10477220\n",
            "Iteration 267, loss = 0.10631988\n",
            "Iteration 268, loss = 0.11029713\n",
            "Iteration 269, loss = 0.10871477\n",
            "Iteration 270, loss = 0.10676823\n",
            "Iteration 271, loss = 0.10626325\n",
            "Iteration 272, loss = 0.10515034\n",
            "Iteration 273, loss = 0.10466416\n",
            "Iteration 274, loss = 0.10444129\n",
            "Iteration 275, loss = 0.10602545\n",
            "Iteration 276, loss = 0.10502220\n",
            "Iteration 277, loss = 0.10451647\n",
            "Iteration 278, loss = 0.10540666\n",
            "Iteration 279, loss = 0.10581053\n",
            "Iteration 280, loss = 0.10650346\n",
            "Iteration 281, loss = 0.10329984\n",
            "Iteration 282, loss = 0.10472638\n",
            "Iteration 283, loss = 0.10307047\n",
            "Iteration 284, loss = 0.10355801\n",
            "Iteration 285, loss = 0.10310275\n",
            "Iteration 286, loss = 0.10203980\n",
            "Iteration 287, loss = 0.10585409\n",
            "Iteration 288, loss = 0.10715573\n",
            "Iteration 289, loss = 0.10270874\n",
            "Iteration 290, loss = 0.10352167\n",
            "Iteration 291, loss = 0.10265106\n",
            "Iteration 292, loss = 0.10339627\n",
            "Iteration 293, loss = 0.10277724\n",
            "Iteration 294, loss = 0.10280247\n",
            "Iteration 295, loss = 0.10383322\n",
            "Iteration 296, loss = 0.10143543\n",
            "Iteration 297, loss = 0.10124843\n",
            "Iteration 298, loss = 0.10408753\n",
            "Iteration 299, loss = 0.10343507\n",
            "Iteration 300, loss = 0.10355653\n",
            "Iteration 301, loss = 0.10021658\n",
            "Iteration 302, loss = 0.10040420\n",
            "Iteration 303, loss = 0.10384760\n",
            "Iteration 304, loss = 0.10146363\n",
            "Iteration 305, loss = 0.10199960\n",
            "Iteration 306, loss = 0.10079962\n",
            "Iteration 307, loss = inf\n",
            "Iteration 308, loss = 0.10584339\n",
            "Iteration 309, loss = 0.10824969\n",
            "Iteration 310, loss = 0.10488069\n",
            "Iteration 311, loss = 0.10140524\n",
            "Iteration 312, loss = 0.10338395\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.38158729\n",
            "Iteration 2, loss = 0.32218018\n",
            "Iteration 3, loss = 0.31053018\n",
            "Iteration 4, loss = 0.30408099\n",
            "Iteration 5, loss = 0.29918094\n",
            "Iteration 6, loss = 0.29614931\n",
            "Iteration 7, loss = 0.29283462\n",
            "Iteration 8, loss = 0.28929165\n",
            "Iteration 9, loss = 0.28649408\n",
            "Iteration 10, loss = 0.28339028\n",
            "Iteration 11, loss = 0.28089133\n",
            "Iteration 12, loss = 0.27838996\n",
            "Iteration 13, loss = 0.27581773\n",
            "Iteration 14, loss = 0.27402062\n",
            "Iteration 15, loss = 0.27056663\n",
            "Iteration 16, loss = 0.26925787\n",
            "Iteration 17, loss = 0.26682495\n",
            "Iteration 18, loss = 0.26355000\n",
            "Iteration 19, loss = 0.26088884\n",
            "Iteration 20, loss = 0.25834607\n",
            "Iteration 21, loss = 0.25623358\n",
            "Iteration 22, loss = 0.25368588\n",
            "Iteration 23, loss = 0.25187067\n",
            "Iteration 24, loss = 0.24942471\n",
            "Iteration 25, loss = 0.24658758\n",
            "Iteration 26, loss = 0.24457163\n",
            "Iteration 27, loss = 0.24154822\n",
            "Iteration 28, loss = 0.24055231\n",
            "Iteration 29, loss = 0.23857427\n",
            "Iteration 30, loss = 0.23519671\n",
            "Iteration 31, loss = 0.23481960\n",
            "Iteration 32, loss = 0.23371021\n",
            "Iteration 33, loss = 0.22888293\n",
            "Iteration 34, loss = 0.22754989\n",
            "Iteration 35, loss = 0.22597763\n",
            "Iteration 36, loss = 0.22458426\n",
            "Iteration 37, loss = 0.22254280\n",
            "Iteration 38, loss = 0.21963826\n",
            "Iteration 39, loss = 0.21825271\n",
            "Iteration 40, loss = 0.21734040\n",
            "Iteration 41, loss = 0.21620595\n",
            "Iteration 42, loss = 0.21435374\n",
            "Iteration 43, loss = 0.21183242\n",
            "Iteration 44, loss = 0.20980673\n",
            "Iteration 45, loss = 0.20957827\n",
            "Iteration 46, loss = 0.20846059\n",
            "Iteration 47, loss = 0.20621457\n",
            "Iteration 48, loss = 0.20546777\n",
            "Iteration 49, loss = 0.20361152\n",
            "Iteration 50, loss = 0.20197660\n",
            "Iteration 51, loss = 0.20058232\n",
            "Iteration 52, loss = 0.19834758\n",
            "Iteration 53, loss = 0.19973778\n",
            "Iteration 54, loss = 0.19878919\n",
            "Iteration 55, loss = 0.19611463\n",
            "Iteration 56, loss = 0.19467293\n",
            "Iteration 57, loss = 0.19394539\n",
            "Iteration 58, loss = 0.19225270\n",
            "Iteration 59, loss = 0.19143964\n",
            "Iteration 60, loss = 0.19055101\n",
            "Iteration 61, loss = 0.19066714\n",
            "Iteration 62, loss = 0.18832720\n",
            "Iteration 63, loss = 0.18576236\n",
            "Iteration 64, loss = 0.18553058\n",
            "Iteration 65, loss = 0.18528452\n",
            "Iteration 66, loss = 0.18518939\n",
            "Iteration 67, loss = 0.18199162\n",
            "Iteration 68, loss = 0.18219026\n",
            "Iteration 69, loss = 0.18212076\n",
            "Iteration 70, loss = 0.17941938\n",
            "Iteration 71, loss = 0.17965580\n",
            "Iteration 72, loss = 0.17789818\n",
            "Iteration 73, loss = 0.17726973\n",
            "Iteration 74, loss = 0.17752051\n",
            "Iteration 75, loss = 0.17448040\n",
            "Iteration 76, loss = 0.17548445\n",
            "Iteration 77, loss = 0.17547014\n",
            "Iteration 78, loss = 0.17320290\n",
            "Iteration 79, loss = 0.17224496\n",
            "Iteration 80, loss = 0.17149835\n",
            "Iteration 81, loss = 0.16923568\n",
            "Iteration 82, loss = 0.16980745\n",
            "Iteration 83, loss = 0.17037167\n",
            "Iteration 84, loss = 0.16977343\n",
            "Iteration 85, loss = 0.16805310\n",
            "Iteration 86, loss = 0.16561818\n",
            "Iteration 87, loss = 0.16538821\n",
            "Iteration 88, loss = 0.16818285\n",
            "Iteration 89, loss = 0.16502153\n",
            "Iteration 90, loss = 0.16428334\n",
            "Iteration 91, loss = 0.16255997\n",
            "Iteration 92, loss = 0.16165472\n",
            "Iteration 93, loss = 0.16272141\n",
            "Iteration 94, loss = 0.16232271\n",
            "Iteration 95, loss = 0.16182720\n",
            "Iteration 96, loss = 0.16146560\n",
            "Iteration 97, loss = 0.15940883\n",
            "Iteration 98, loss = 0.15774018\n",
            "Iteration 99, loss = 0.15636289\n",
            "Iteration 100, loss = 0.15793936\n",
            "Iteration 101, loss = 0.15643434\n",
            "Iteration 102, loss = 0.15944562\n",
            "Iteration 103, loss = 0.15588250\n",
            "Iteration 104, loss = 0.15596767\n",
            "Iteration 105, loss = 0.15346052\n",
            "Iteration 106, loss = 0.15202846\n",
            "Iteration 107, loss = 0.15231077\n",
            "Iteration 108, loss = 0.15276412\n",
            "Iteration 109, loss = 0.15243689\n",
            "Iteration 110, loss = 0.15294605\n",
            "Iteration 111, loss = 0.15071505\n",
            "Iteration 112, loss = 0.15194775\n",
            "Iteration 113, loss = 0.14938243\n",
            "Iteration 114, loss = 0.15045098\n",
            "Iteration 115, loss = 0.14777877\n",
            "Iteration 116, loss = 0.14700598\n",
            "Iteration 117, loss = 0.14581963\n",
            "Iteration 118, loss = 0.15042531\n",
            "Iteration 119, loss = 0.14865987\n",
            "Iteration 120, loss = 0.14660326\n",
            "Iteration 121, loss = 0.14459012\n",
            "Iteration 122, loss = 0.14651385\n",
            "Iteration 123, loss = 0.14559191\n",
            "Iteration 124, loss = 0.14653161\n",
            "Iteration 125, loss = 0.14596162\n",
            "Iteration 126, loss = 0.14315495\n",
            "Iteration 127, loss = 0.14374023\n",
            "Iteration 128, loss = 0.14270151\n",
            "Iteration 129, loss = 0.14039371\n",
            "Iteration 130, loss = 0.13917531\n",
            "Iteration 131, loss = 0.14262774\n",
            "Iteration 132, loss = 0.14449003\n",
            "Iteration 133, loss = 0.14090614\n",
            "Iteration 134, loss = 0.14034548\n",
            "Iteration 135, loss = 0.13930524\n",
            "Iteration 136, loss = 0.13921990\n",
            "Iteration 137, loss = 0.13946712\n",
            "Iteration 138, loss = 0.14042760\n",
            "Iteration 139, loss = 0.14047410\n",
            "Iteration 140, loss = 0.13786810\n",
            "Iteration 141, loss = 0.13820960\n",
            "Iteration 142, loss = 0.13620829\n",
            "Iteration 143, loss = 0.13546558\n",
            "Iteration 144, loss = 0.13594877\n",
            "Iteration 145, loss = 0.13603737\n",
            "Iteration 146, loss = 0.13522864\n",
            "Iteration 147, loss = 0.13394050\n",
            "Iteration 148, loss = 0.13431692\n",
            "Iteration 149, loss = 0.13464956\n",
            "Iteration 150, loss = 0.13512924\n",
            "Iteration 151, loss = 0.13454973\n",
            "Iteration 152, loss = 0.13289910\n",
            "Iteration 153, loss = 0.13354374\n",
            "Iteration 154, loss = 0.13220431\n",
            "Iteration 155, loss = 0.13271386\n",
            "Iteration 156, loss = 0.13188436\n",
            "Iteration 157, loss = 0.13325004\n",
            "Iteration 158, loss = 0.13195251\n",
            "Iteration 159, loss = 0.13153882\n",
            "Iteration 160, loss = 0.13066560\n",
            "Iteration 161, loss = 0.13256930\n",
            "Iteration 162, loss = 0.12884259\n",
            "Iteration 163, loss = 0.12873194\n",
            "Iteration 164, loss = 0.12786151\n",
            "Iteration 165, loss = 0.12832497\n",
            "Iteration 166, loss = 0.12784697\n",
            "Iteration 167, loss = 0.12941102\n",
            "Iteration 168, loss = 0.12929965\n",
            "Iteration 169, loss = 0.12730114\n",
            "Iteration 170, loss = 0.12840107\n",
            "Iteration 171, loss = 0.12807382\n",
            "Iteration 172, loss = 0.12667661\n",
            "Iteration 173, loss = 0.12682297\n",
            "Iteration 174, loss = 0.12714114\n",
            "Iteration 175, loss = 0.12571690\n",
            "Iteration 176, loss = 0.12498229\n",
            "Iteration 177, loss = 0.12535475\n",
            "Iteration 178, loss = 0.12491647\n",
            "Iteration 179, loss = 0.12397089\n",
            "Iteration 180, loss = 0.12501362\n",
            "Iteration 181, loss = 0.12430846\n",
            "Iteration 182, loss = 0.12329019\n",
            "Iteration 183, loss = 0.12402642\n",
            "Iteration 184, loss = 0.12494598\n",
            "Iteration 185, loss = 0.12357504\n",
            "Iteration 186, loss = 0.12229454\n",
            "Iteration 187, loss = 0.12304732\n",
            "Iteration 188, loss = 0.12235333\n",
            "Iteration 189, loss = 0.12299135\n",
            "Iteration 190, loss = 0.12350053\n",
            "Iteration 191, loss = 0.12075619\n",
            "Iteration 192, loss = 0.12009973\n",
            "Iteration 193, loss = 0.12299939\n",
            "Iteration 194, loss = 0.11899171\n",
            "Iteration 195, loss = 0.12232019\n",
            "Iteration 196, loss = 0.12413643\n",
            "Iteration 197, loss = 0.12350209\n",
            "Iteration 198, loss = 0.11993678\n",
            "Iteration 199, loss = 0.11984173\n",
            "Iteration 200, loss = 0.11886749\n",
            "Iteration 201, loss = 0.11598574\n",
            "Iteration 202, loss = 0.12167755\n",
            "Iteration 203, loss = 0.12069959\n",
            "Iteration 204, loss = 0.11863394\n",
            "Iteration 205, loss = 0.11809716\n",
            "Iteration 206, loss = 0.11976520\n",
            "Iteration 207, loss = 0.12029567\n",
            "Iteration 208, loss = 0.11655017\n",
            "Iteration 209, loss = 0.11703911\n",
            "Iteration 210, loss = 0.11887106\n",
            "Iteration 211, loss = 0.11573610\n",
            "Iteration 212, loss = 0.11874855\n",
            "Iteration 213, loss = 0.11976481\n",
            "Iteration 214, loss = 0.11600475\n",
            "Iteration 215, loss = 0.11805367\n",
            "Iteration 216, loss = 0.11453066\n",
            "Iteration 217, loss = 0.11478408\n",
            "Iteration 218, loss = 0.11528889\n",
            "Iteration 219, loss = 0.11667269\n",
            "Iteration 220, loss = 0.11571933\n",
            "Iteration 221, loss = 0.11441680\n",
            "Iteration 222, loss = 0.11480507\n",
            "Iteration 223, loss = 0.11568209\n",
            "Iteration 224, loss = 0.11683107\n",
            "Iteration 225, loss = 0.11305706\n",
            "Iteration 226, loss = 0.11229408\n",
            "Iteration 227, loss = 0.11400998\n",
            "Iteration 228, loss = 0.11445404\n",
            "Iteration 229, loss = 0.11386411\n",
            "Iteration 230, loss = 0.11607407\n",
            "Iteration 231, loss = 0.11170696\n",
            "Iteration 232, loss = 0.11199006\n",
            "Iteration 233, loss = 0.11510935\n",
            "Iteration 234, loss = 0.11202495\n",
            "Iteration 235, loss = 0.11389801\n",
            "Iteration 236, loss = 0.11256072\n",
            "Iteration 237, loss = 0.11222126\n",
            "Iteration 238, loss = 0.11317806\n",
            "Iteration 239, loss = 0.11183955\n",
            "Iteration 240, loss = 0.11224426\n",
            "Iteration 241, loss = 0.11342639\n",
            "Iteration 242, loss = 0.10878423\n",
            "Iteration 243, loss = 0.11233065\n",
            "Iteration 244, loss = 0.11048470\n",
            "Iteration 245, loss = 0.11210926\n",
            "Iteration 246, loss = 0.11161150\n",
            "Iteration 247, loss = 0.11363636\n",
            "Iteration 248, loss = 0.11409056\n",
            "Iteration 249, loss = 0.11064181\n",
            "Iteration 250, loss = 0.11012204\n",
            "Iteration 251, loss = 0.10732010\n",
            "Iteration 252, loss = 0.10827628\n",
            "Iteration 253, loss = 0.10952034\n",
            "Iteration 254, loss = 0.10842393\n",
            "Iteration 255, loss = 0.10835601\n",
            "Iteration 256, loss = 0.10907535\n",
            "Iteration 257, loss = 0.10720189\n",
            "Iteration 258, loss = 0.10629476\n",
            "Iteration 259, loss = 0.10784142\n",
            "Iteration 260, loss = 0.10599899\n",
            "Iteration 261, loss = 0.10723628\n",
            "Iteration 262, loss = 0.10793754\n",
            "Iteration 263, loss = 0.10666208\n",
            "Iteration 264, loss = 0.10652000\n",
            "Iteration 265, loss = 0.10627267\n",
            "Iteration 266, loss = 0.10676188\n",
            "Iteration 267, loss = 0.10692590\n",
            "Iteration 268, loss = 0.10855566\n",
            "Iteration 269, loss = 0.10859067\n",
            "Iteration 270, loss = 0.10649260\n",
            "Iteration 271, loss = 0.10654561\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.37750877\n",
            "Iteration 2, loss = 0.32156677\n",
            "Iteration 3, loss = 0.30942503\n",
            "Iteration 4, loss = 0.30384761\n",
            "Iteration 5, loss = 0.29928112\n",
            "Iteration 6, loss = 0.29479992\n",
            "Iteration 7, loss = 0.29018258\n",
            "Iteration 8, loss = 0.28783694\n",
            "Iteration 9, loss = 0.28492637\n",
            "Iteration 10, loss = 0.28198461\n",
            "Iteration 11, loss = 0.28114518\n",
            "Iteration 12, loss = 0.27700509\n",
            "Iteration 13, loss = 0.27556433\n",
            "Iteration 14, loss = 0.27339005\n",
            "Iteration 15, loss = 0.26990994\n",
            "Iteration 16, loss = 0.26707321\n",
            "Iteration 17, loss = 0.26564546\n",
            "Iteration 18, loss = 0.26275634\n",
            "Iteration 19, loss = 0.26024974\n",
            "Iteration 20, loss = 0.25870038\n",
            "Iteration 21, loss = 0.25697321\n",
            "Iteration 22, loss = 0.25475207\n",
            "Iteration 23, loss = 0.25221469\n",
            "Iteration 24, loss = 0.24905984\n",
            "Iteration 25, loss = 0.24763854\n",
            "Iteration 26, loss = 0.24477492\n",
            "Iteration 27, loss = 0.24342696\n",
            "Iteration 28, loss = 0.24012711\n",
            "Iteration 29, loss = 0.23736157\n",
            "Iteration 30, loss = 0.23615749\n",
            "Iteration 31, loss = 0.23325440\n",
            "Iteration 32, loss = 0.23294861\n",
            "Iteration 33, loss = 0.22891846\n",
            "Iteration 34, loss = 0.22774264\n",
            "Iteration 35, loss = 0.22634065\n",
            "Iteration 36, loss = 0.22449472\n",
            "Iteration 37, loss = 0.22130759\n",
            "Iteration 38, loss = 0.22037264\n",
            "Iteration 39, loss = 0.21860133\n",
            "Iteration 40, loss = 0.21671416\n",
            "Iteration 41, loss = 0.21479128\n",
            "Iteration 42, loss = 0.21400029\n",
            "Iteration 43, loss = 0.21198066\n",
            "Iteration 44, loss = 0.20883340\n",
            "Iteration 45, loss = 0.20939277\n",
            "Iteration 46, loss = 0.20722965\n",
            "Iteration 47, loss = 0.20426019\n",
            "Iteration 48, loss = 0.20333015\n",
            "Iteration 49, loss = 0.20215714\n",
            "Iteration 50, loss = 0.19953504\n",
            "Iteration 51, loss = 0.19965672\n",
            "Iteration 52, loss = 0.19763793\n",
            "Iteration 53, loss = 0.19578984\n",
            "Iteration 54, loss = 0.19356701\n",
            "Iteration 55, loss = 0.19411593\n",
            "Iteration 56, loss = 0.19232361\n",
            "Iteration 57, loss = 0.19026936\n",
            "Iteration 58, loss = 0.18882749\n",
            "Iteration 59, loss = 0.18804351\n",
            "Iteration 60, loss = 0.18622226\n",
            "Iteration 61, loss = 0.18582817\n",
            "Iteration 62, loss = 0.18480487\n",
            "Iteration 63, loss = 0.18382565\n",
            "Iteration 64, loss = 0.18266418\n",
            "Iteration 65, loss = 0.18355187\n",
            "Iteration 66, loss = 0.17935004\n",
            "Iteration 67, loss = 0.18024193\n",
            "Iteration 68, loss = 0.17724660\n",
            "Iteration 69, loss = 0.17587981\n",
            "Iteration 70, loss = 0.17611883\n",
            "Iteration 71, loss = 0.17532405\n",
            "Iteration 72, loss = 0.17441383\n",
            "Iteration 73, loss = 0.17305503\n",
            "Iteration 74, loss = 0.17236795\n",
            "Iteration 75, loss = 0.17210013\n",
            "Iteration 76, loss = 0.16919269\n",
            "Iteration 77, loss = 0.16933427\n",
            "Iteration 78, loss = 0.16848390\n",
            "Iteration 79, loss = 0.16692380\n",
            "Iteration 80, loss = 0.16812157\n",
            "Iteration 81, loss = 0.16577241\n",
            "Iteration 82, loss = 0.16569028\n",
            "Iteration 83, loss = 0.16355882\n",
            "Iteration 84, loss = 0.16465351\n",
            "Iteration 85, loss = 0.16429251\n",
            "Iteration 86, loss = 0.16133977\n",
            "Iteration 87, loss = 0.16218137\n",
            "Iteration 88, loss = 0.16079526\n",
            "Iteration 89, loss = 0.15935760\n",
            "Iteration 90, loss = 0.15852735\n",
            "Iteration 91, loss = 0.15731989\n",
            "Iteration 92, loss = 0.15703569\n",
            "Iteration 93, loss = 0.15918359\n",
            "Iteration 94, loss = 0.15618986\n",
            "Iteration 95, loss = 0.15730592\n",
            "Iteration 96, loss = 0.15504429\n",
            "Iteration 97, loss = 0.15456130\n",
            "Iteration 98, loss = 0.15327175\n",
            "Iteration 99, loss = 0.15168746\n",
            "Iteration 100, loss = 0.15161229\n",
            "Iteration 101, loss = 0.15054759\n",
            "Iteration 102, loss = 0.15101262\n",
            "Iteration 103, loss = 0.14870951\n",
            "Iteration 104, loss = 0.14877505\n",
            "Iteration 105, loss = 0.14819540\n",
            "Iteration 106, loss = 0.15146090\n",
            "Iteration 107, loss = 0.14856724\n",
            "Iteration 108, loss = 0.14854456\n",
            "Iteration 109, loss = 0.14616860\n",
            "Iteration 110, loss = 0.14692297\n",
            "Iteration 111, loss = 0.14773059\n",
            "Iteration 112, loss = 0.14828003\n",
            "Iteration 113, loss = 0.14563453\n",
            "Iteration 114, loss = 0.14432672\n",
            "Iteration 115, loss = 0.14384616\n",
            "Iteration 116, loss = 0.14177198\n",
            "Iteration 117, loss = 0.14452585\n",
            "Iteration 118, loss = 0.14265599\n",
            "Iteration 119, loss = 0.14249907\n",
            "Iteration 120, loss = 0.14198653\n",
            "Iteration 121, loss = 0.13854208\n",
            "Iteration 122, loss = 0.13983737\n",
            "Iteration 123, loss = 0.13976260\n",
            "Iteration 124, loss = 0.13791650\n",
            "Iteration 125, loss = 0.13949078\n",
            "Iteration 126, loss = 0.13680141\n",
            "Iteration 127, loss = 0.13863502\n",
            "Iteration 128, loss = 0.13681838\n",
            "Iteration 129, loss = 0.13434745\n",
            "Iteration 130, loss = 0.13773575\n",
            "Iteration 131, loss = 0.13599582\n",
            "Iteration 132, loss = 0.13437761\n",
            "Iteration 133, loss = 0.13537135\n",
            "Iteration 134, loss = 0.13335662\n",
            "Iteration 135, loss = 0.13236475\n",
            "Iteration 136, loss = 0.13530711\n",
            "Iteration 137, loss = 0.13233076\n",
            "Iteration 138, loss = 0.13238438\n",
            "Iteration 139, loss = 0.13340185\n",
            "Iteration 140, loss = 0.13216899\n",
            "Iteration 141, loss = 0.13209988\n",
            "Iteration 142, loss = 0.13070993\n",
            "Iteration 143, loss = 0.12903914\n",
            "Iteration 144, loss = 0.12955213\n",
            "Iteration 145, loss = 0.13005948\n",
            "Iteration 146, loss = 0.12987049\n",
            "Iteration 147, loss = 0.12799612\n",
            "Iteration 148, loss = 0.12837619\n",
            "Iteration 149, loss = 0.12857699\n",
            "Iteration 150, loss = 0.12763195\n",
            "Iteration 151, loss = 0.12910608\n",
            "Iteration 152, loss = 0.12845934\n",
            "Iteration 153, loss = 0.12708044\n",
            "Iteration 154, loss = 0.12726405\n",
            "Iteration 155, loss = 0.12745081\n",
            "Iteration 156, loss = 0.12699534\n",
            "Iteration 157, loss = 0.12810954\n",
            "Iteration 158, loss = 0.12596209\n",
            "Iteration 159, loss = 0.12565360\n",
            "Iteration 160, loss = 0.13019569\n",
            "Iteration 161, loss = 0.12483043\n",
            "Iteration 162, loss = 0.12604384\n",
            "Iteration 163, loss = 0.12422281\n",
            "Iteration 164, loss = 0.12293003\n",
            "Iteration 165, loss = 0.12235207\n",
            "Iteration 166, loss = 0.12484736\n",
            "Iteration 167, loss = 0.12152741\n",
            "Iteration 168, loss = 0.12110088\n",
            "Iteration 169, loss = 0.12056187\n",
            "Iteration 170, loss = 0.12421303\n",
            "Iteration 171, loss = 0.12243498\n",
            "Iteration 172, loss = 0.12266203\n",
            "Iteration 173, loss = 0.11847940\n",
            "Iteration 174, loss = 0.12077314\n",
            "Iteration 175, loss = 0.12127141\n",
            "Iteration 176, loss = 0.12177608\n",
            "Iteration 177, loss = 0.11968890\n",
            "Iteration 178, loss = 0.11993687\n",
            "Iteration 179, loss = 0.11869975\n",
            "Iteration 180, loss = 0.11716128\n",
            "Iteration 181, loss = 0.11862782\n",
            "Iteration 182, loss = 0.11772831\n",
            "Iteration 183, loss = 0.11810168\n",
            "Iteration 184, loss = 0.11554315\n",
            "Iteration 185, loss = 0.11637667\n",
            "Iteration 186, loss = 0.11575594\n",
            "Iteration 187, loss = 0.11473864\n",
            "Iteration 188, loss = 0.11927182\n",
            "Iteration 189, loss = 0.11462900\n",
            "Iteration 190, loss = 0.11682216\n",
            "Iteration 191, loss = 0.11459026\n",
            "Iteration 192, loss = 0.11514429\n",
            "Iteration 193, loss = 0.11521778\n",
            "Iteration 194, loss = 0.11472330\n",
            "Iteration 195, loss = 0.11487541\n",
            "Iteration 196, loss = 0.11252722\n",
            "Iteration 197, loss = 0.11394623\n",
            "Iteration 198, loss = 0.11553777\n",
            "Iteration 199, loss = 0.11702955\n",
            "Iteration 200, loss = 0.11452661\n",
            "Iteration 201, loss = 0.11126178\n",
            "Iteration 202, loss = 0.11063190\n",
            "Iteration 203, loss = 0.11206707\n",
            "Iteration 204, loss = 0.11090501\n",
            "Iteration 205, loss = 0.10907339\n",
            "Iteration 206, loss = 0.11164097\n",
            "Iteration 207, loss = 0.10998946\n",
            "Iteration 208, loss = 0.11071232\n",
            "Iteration 209, loss = 0.11204033\n",
            "Iteration 210, loss = 0.11104313\n",
            "Iteration 211, loss = 0.11104788\n",
            "Iteration 212, loss = 0.10962014\n",
            "Iteration 213, loss = 0.11178011\n",
            "Iteration 214, loss = 0.11254293\n",
            "Iteration 215, loss = 0.10900134\n",
            "Iteration 216, loss = 0.11073100\n",
            "Iteration 217, loss = 0.11049634\n",
            "Iteration 218, loss = 0.10820508\n",
            "Iteration 219, loss = 0.10901902\n",
            "Iteration 220, loss = 0.11049085\n",
            "Iteration 221, loss = 0.10726428\n",
            "Iteration 222, loss = 0.10714181\n",
            "Iteration 223, loss = 0.10616830\n",
            "Iteration 224, loss = 0.10594112\n",
            "Iteration 225, loss = 0.10440714\n",
            "Iteration 226, loss = 0.10642872\n",
            "Iteration 227, loss = 0.10815236\n",
            "Iteration 228, loss = 0.10727732\n",
            "Iteration 229, loss = 0.10656198\n",
            "Iteration 230, loss = 0.10548984\n",
            "Iteration 231, loss = 0.10439019\n",
            "Iteration 232, loss = 0.10695463\n",
            "Iteration 233, loss = 0.10915095\n",
            "Iteration 234, loss = 0.10847189\n",
            "Iteration 235, loss = 0.10953722\n",
            "Iteration 236, loss = 0.10897735\n",
            "Iteration 237, loss = 0.10486034\n",
            "Iteration 238, loss = 0.10320038\n",
            "Iteration 239, loss = 0.10368753\n",
            "Iteration 240, loss = 0.10366645\n",
            "Iteration 241, loss = 0.10318917\n",
            "Iteration 242, loss = 0.10454530\n",
            "Iteration 243, loss = 0.10523191\n",
            "Iteration 244, loss = 0.10353047\n",
            "Iteration 245, loss = 0.10504809\n",
            "Iteration 246, loss = 0.10465801\n",
            "Iteration 247, loss = 0.10233893\n",
            "Iteration 248, loss = 0.10149739\n",
            "Iteration 249, loss = 0.10034737\n",
            "Iteration 250, loss = 0.10013121\n",
            "Iteration 251, loss = 0.09922208\n",
            "Iteration 252, loss = 0.10136992\n",
            "Iteration 253, loss = 0.10247696\n",
            "Iteration 254, loss = 0.10043216\n",
            "Iteration 255, loss = 0.10116842\n",
            "Iteration 256, loss = 0.10018132\n",
            "Iteration 257, loss = 0.10176917\n",
            "Iteration 258, loss = 0.09858051\n",
            "Iteration 259, loss = 0.09830561\n",
            "Iteration 260, loss = 0.10007503\n",
            "Iteration 261, loss = 0.09888145\n",
            "Iteration 262, loss = 0.10119235\n",
            "Iteration 263, loss = 0.10301737\n",
            "Iteration 264, loss = 0.10061931\n",
            "Iteration 265, loss = 0.10284650\n",
            "Iteration 266, loss = 0.09990106\n",
            "Iteration 267, loss = 0.09850551\n",
            "Iteration 268, loss = 0.09836226\n",
            "Iteration 269, loss = 0.09796736\n",
            "Iteration 270, loss = 0.10213643\n",
            "Iteration 271, loss = 0.10138904\n",
            "Iteration 272, loss = 0.09896169\n",
            "Iteration 273, loss = 0.09815725\n",
            "Iteration 274, loss = 0.09779106\n",
            "Iteration 275, loss = 0.09918550\n",
            "Iteration 276, loss = 0.09932458\n",
            "Iteration 277, loss = 0.09860157\n",
            "Iteration 278, loss = 0.09820794\n",
            "Iteration 279, loss = 0.09565731\n",
            "Iteration 280, loss = 0.09569978\n",
            "Iteration 281, loss = 0.09437129\n",
            "Iteration 282, loss = 0.09615862\n",
            "Iteration 283, loss = 0.09636149\n",
            "Iteration 284, loss = 0.09852925\n",
            "Iteration 285, loss = 0.09728224\n",
            "Iteration 286, loss = 0.09719392\n",
            "Iteration 287, loss = 0.09427663\n",
            "Iteration 288, loss = 0.09867932\n",
            "Iteration 289, loss = 0.09599239\n",
            "Iteration 290, loss = 0.09637973\n",
            "Iteration 291, loss = 0.09838184\n",
            "Iteration 292, loss = 0.10019418\n",
            "Iteration 293, loss = 0.09530886\n",
            "Iteration 294, loss = 0.09273271\n",
            "Iteration 295, loss = 0.09620554\n",
            "Iteration 296, loss = 0.09688877\n",
            "Iteration 297, loss = 0.09305057\n",
            "Iteration 298, loss = 0.09228196\n",
            "Iteration 299, loss = 0.09352939\n",
            "Iteration 300, loss = 0.09479177\n",
            "Iteration 301, loss = 0.09338096\n",
            "Iteration 302, loss = 0.09374543\n",
            "Iteration 303, loss = 0.09413775\n",
            "Iteration 304, loss = 0.09627103\n",
            "Iteration 305, loss = 0.09317805\n",
            "Iteration 306, loss = 0.09466662\n",
            "Iteration 307, loss = 0.09241312\n",
            "Iteration 308, loss = 0.09085396\n",
            "Iteration 309, loss = 0.09008693\n",
            "Iteration 310, loss = 0.09378995\n",
            "Iteration 311, loss = 0.09148456\n",
            "Iteration 312, loss = 0.09263947\n",
            "Iteration 313, loss = 0.09256277\n",
            "Iteration 314, loss = 0.09160093\n",
            "Iteration 315, loss = 0.09320970\n",
            "Iteration 316, loss = 0.09185951\n",
            "Iteration 317, loss = 0.09013496\n",
            "Iteration 318, loss = 0.09342045\n",
            "Iteration 319, loss = 0.09266688\n",
            "Iteration 320, loss = 0.09160553\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8208595604703988"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO2lNJ8R0zm5",
        "outputId": "c666b316-f1a5-4a91-9fb4-2517f487f907"
      },
      "source": [
        "#hold\n",
        "mlp_hold = mlp.fit(X_df, y_df)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.37087089\n",
            "Iteration 2, loss = 0.31990380\n",
            "Iteration 3, loss = 0.30935038\n",
            "Iteration 4, loss = 0.30263992\n",
            "Iteration 5, loss = 0.29866038\n",
            "Iteration 6, loss = 0.29465254\n",
            "Iteration 7, loss = 0.29158229\n",
            "Iteration 8, loss = 0.29002726\n",
            "Iteration 9, loss = 0.28587846\n",
            "Iteration 10, loss = 0.28354505\n",
            "Iteration 11, loss = 0.28164010\n",
            "Iteration 12, loss = 0.27894465\n",
            "Iteration 13, loss = 0.27679635\n",
            "Iteration 14, loss = 0.27511571\n",
            "Iteration 15, loss = 0.27264200\n",
            "Iteration 16, loss = 0.26913299\n",
            "Iteration 17, loss = 0.26806864\n",
            "Iteration 18, loss = 0.26528629\n",
            "Iteration 19, loss = 0.26287101\n",
            "Iteration 20, loss = 0.26102884\n",
            "Iteration 21, loss = 0.25767552\n",
            "Iteration 22, loss = 0.25577936\n",
            "Iteration 23, loss = 0.25358864\n",
            "Iteration 24, loss = 0.25297871\n",
            "Iteration 25, loss = 0.25008240\n",
            "Iteration 26, loss = 0.24775644\n",
            "Iteration 27, loss = 0.24504887\n",
            "Iteration 28, loss = 0.24305110\n",
            "Iteration 29, loss = 0.24009771\n",
            "Iteration 30, loss = 0.23946210\n",
            "Iteration 31, loss = 0.23733711\n",
            "Iteration 32, loss = 0.23517955\n",
            "Iteration 33, loss = 0.23217004\n",
            "Iteration 34, loss = 0.23192353\n",
            "Iteration 35, loss = 0.22932416\n",
            "Iteration 36, loss = 0.22708487\n",
            "Iteration 37, loss = 0.22718152\n",
            "Iteration 38, loss = 0.22366937\n",
            "Iteration 39, loss = 0.22162366\n",
            "Iteration 40, loss = 0.22083318\n",
            "Iteration 41, loss = 0.22051042\n",
            "Iteration 42, loss = 0.21834643\n",
            "Iteration 43, loss = 0.21620333\n",
            "Iteration 44, loss = 0.21473119\n",
            "Iteration 45, loss = 0.21353372\n",
            "Iteration 46, loss = 0.21059731\n",
            "Iteration 47, loss = 0.21073429\n",
            "Iteration 48, loss = 0.21037601\n",
            "Iteration 49, loss = 0.20645050\n",
            "Iteration 50, loss = 0.20550376\n",
            "Iteration 51, loss = 0.20617597\n",
            "Iteration 52, loss = 0.20298523\n",
            "Iteration 53, loss = 0.20232492\n",
            "Iteration 54, loss = 0.20149384\n",
            "Iteration 55, loss = 0.19979148\n",
            "Iteration 56, loss = 0.19901599\n",
            "Iteration 57, loss = 0.19747495\n",
            "Iteration 58, loss = 0.19571791\n",
            "Iteration 59, loss = 0.19594660\n",
            "Iteration 60, loss = 0.19329443\n",
            "Iteration 61, loss = 0.19314647\n",
            "Iteration 62, loss = 0.19152488\n",
            "Iteration 63, loss = 0.19116032\n",
            "Iteration 64, loss = 0.19122446\n",
            "Iteration 65, loss = 0.18851919\n",
            "Iteration 66, loss = 0.18802148\n",
            "Iteration 67, loss = 0.18779839\n",
            "Iteration 68, loss = 0.18724431\n",
            "Iteration 69, loss = 0.18589697\n",
            "Iteration 70, loss = 0.18354874\n",
            "Iteration 71, loss = 0.18476664\n",
            "Iteration 72, loss = 0.18264633\n",
            "Iteration 73, loss = 0.18083915\n",
            "Iteration 74, loss = 0.18111660\n",
            "Iteration 75, loss = 0.17847099\n",
            "Iteration 76, loss = 0.17948362\n",
            "Iteration 77, loss = 0.17917778\n",
            "Iteration 78, loss = 0.17739121\n",
            "Iteration 79, loss = 0.17680543\n",
            "Iteration 80, loss = 0.17510712\n",
            "Iteration 81, loss = 0.17444074\n",
            "Iteration 82, loss = 0.17411152\n",
            "Iteration 83, loss = 0.17189033\n",
            "Iteration 84, loss = 0.17142940\n",
            "Iteration 85, loss = 0.17119753\n",
            "Iteration 86, loss = 0.17186725\n",
            "Iteration 87, loss = 0.17002781\n",
            "Iteration 88, loss = 0.17064009\n",
            "Iteration 89, loss = 0.16905387\n",
            "Iteration 90, loss = 0.16844778\n",
            "Iteration 91, loss = 0.16656067\n",
            "Iteration 92, loss = 0.16925776\n",
            "Iteration 93, loss = 0.16836438\n",
            "Iteration 94, loss = 0.16532992\n",
            "Iteration 95, loss = 0.16515573\n",
            "Iteration 96, loss = 0.16394034\n",
            "Iteration 97, loss = 0.16305053\n",
            "Iteration 98, loss = 0.16259136\n",
            "Iteration 99, loss = 0.16385470\n",
            "Iteration 100, loss = 0.16232244\n",
            "Iteration 101, loss = 0.16223007\n",
            "Iteration 102, loss = 0.16240959\n",
            "Iteration 103, loss = 0.16005244\n",
            "Iteration 104, loss = 0.15968920\n",
            "Iteration 105, loss = 0.15718371\n",
            "Iteration 106, loss = 0.16027103\n",
            "Iteration 107, loss = 0.15978255\n",
            "Iteration 108, loss = 0.15874273\n",
            "Iteration 109, loss = 0.15911234\n",
            "Iteration 110, loss = 0.15681142\n",
            "Iteration 111, loss = 0.15547904\n",
            "Iteration 112, loss = 0.15520688\n",
            "Iteration 113, loss = 0.15383210\n",
            "Iteration 114, loss = 0.15422778\n",
            "Iteration 115, loss = 0.15493331\n",
            "Iteration 116, loss = 0.15313138\n",
            "Iteration 117, loss = 0.15413598\n",
            "Iteration 118, loss = 0.15466890\n",
            "Iteration 119, loss = 0.15122919\n",
            "Iteration 120, loss = 0.15176578\n",
            "Iteration 121, loss = 0.15158000\n",
            "Iteration 122, loss = 0.15059514\n",
            "Iteration 123, loss = 0.14986099\n",
            "Iteration 124, loss = 0.15110862\n",
            "Iteration 125, loss = 0.14766823\n",
            "Iteration 126, loss = 0.14849732\n",
            "Iteration 127, loss = 0.14766013\n",
            "Iteration 128, loss = 0.14746856\n",
            "Iteration 129, loss = 0.14785862\n",
            "Iteration 130, loss = 0.14965358\n",
            "Iteration 131, loss = 0.14787381\n",
            "Iteration 132, loss = 0.15103227\n",
            "Iteration 133, loss = 0.14822296\n",
            "Iteration 134, loss = 0.14470294\n",
            "Iteration 135, loss = 0.14380589\n",
            "Iteration 136, loss = 0.14346488\n",
            "Iteration 137, loss = 0.14410422\n",
            "Iteration 138, loss = 0.14550443\n",
            "Iteration 139, loss = 0.14345515\n",
            "Iteration 140, loss = 0.14201979\n",
            "Iteration 141, loss = 0.14189416\n",
            "Iteration 142, loss = 0.14621217\n",
            "Iteration 143, loss = 0.14122422\n",
            "Iteration 144, loss = 0.14197321\n",
            "Iteration 145, loss = 0.14205119\n",
            "Iteration 146, loss = 0.13954185\n",
            "Iteration 147, loss = 0.13980158\n",
            "Iteration 148, loss = 0.13974990\n",
            "Iteration 149, loss = 0.14183430\n",
            "Iteration 150, loss = 0.14107165\n",
            "Iteration 151, loss = 0.13956788\n",
            "Iteration 152, loss = 0.13873170\n",
            "Iteration 153, loss = 0.13901428\n",
            "Iteration 154, loss = 0.13879928\n",
            "Iteration 155, loss = 0.13813296\n",
            "Iteration 156, loss = 0.13875455\n",
            "Iteration 157, loss = 0.13610864\n",
            "Iteration 158, loss = 0.13690057\n",
            "Iteration 159, loss = 0.13812754\n",
            "Iteration 160, loss = 0.13819481\n",
            "Iteration 161, loss = 0.13578826\n",
            "Iteration 162, loss = 0.13485628\n",
            "Iteration 163, loss = 0.13613570\n",
            "Iteration 164, loss = 0.13344920\n",
            "Iteration 165, loss = 0.13475381\n",
            "Iteration 166, loss = 0.13481578\n",
            "Iteration 167, loss = 0.13524231\n",
            "Iteration 168, loss = 0.13292460\n",
            "Iteration 169, loss = 0.13286205\n",
            "Iteration 170, loss = 0.13513906\n",
            "Iteration 171, loss = 0.13338968\n",
            "Iteration 172, loss = 0.13380508\n",
            "Iteration 173, loss = 0.13249353\n",
            "Iteration 174, loss = 0.13378568\n",
            "Iteration 175, loss = 0.13205737\n",
            "Iteration 176, loss = 0.13178749\n",
            "Iteration 177, loss = 0.13329154\n",
            "Iteration 178, loss = 0.13047057\n",
            "Iteration 179, loss = 0.13172765\n",
            "Iteration 180, loss = 0.13421913\n",
            "Iteration 181, loss = 0.13380264\n",
            "Iteration 182, loss = 0.13123412\n",
            "Iteration 183, loss = 0.13143587\n",
            "Iteration 184, loss = 0.13044027\n",
            "Iteration 185, loss = 0.12964447\n",
            "Iteration 186, loss = 0.12888274\n",
            "Iteration 187, loss = 0.13044365\n",
            "Iteration 188, loss = 0.12941376\n",
            "Iteration 189, loss = 0.12892231\n",
            "Iteration 190, loss = 0.12901749\n",
            "Iteration 191, loss = 0.12741773\n",
            "Iteration 192, loss = 0.12709433\n",
            "Iteration 193, loss = 0.12914409\n",
            "Iteration 194, loss = 0.12709437\n",
            "Iteration 195, loss = 0.12632586\n",
            "Iteration 196, loss = 0.12782554\n",
            "Iteration 197, loss = 0.12565020\n",
            "Iteration 198, loss = 0.12777340\n",
            "Iteration 199, loss = 0.12802872\n",
            "Iteration 200, loss = 0.12810745\n",
            "Iteration 201, loss = 0.12640482\n",
            "Iteration 202, loss = 0.12400845\n",
            "Iteration 203, loss = 0.12735430\n",
            "Iteration 204, loss = 0.12468775\n",
            "Iteration 205, loss = 0.12324209\n",
            "Iteration 206, loss = 0.12368493\n",
            "Iteration 207, loss = 0.12406347\n",
            "Iteration 208, loss = 0.12403337\n",
            "Iteration 209, loss = 0.12414516\n",
            "Iteration 210, loss = 0.12483964\n",
            "Iteration 211, loss = 0.12647753\n",
            "Iteration 212, loss = 0.12470796\n",
            "Iteration 213, loss = 0.12363340\n",
            "Iteration 214, loss = 0.12234729\n",
            "Iteration 215, loss = 0.12070869\n",
            "Iteration 216, loss = 0.12187294\n",
            "Iteration 217, loss = 0.12072113\n",
            "Iteration 218, loss = 0.12777788\n",
            "Iteration 219, loss = 0.12417580\n",
            "Iteration 220, loss = 0.12361166\n",
            "Iteration 221, loss = 0.12548035\n",
            "Iteration 222, loss = 0.12013418\n",
            "Iteration 223, loss = 0.12083055\n",
            "Iteration 224, loss = 0.12129436\n",
            "Iteration 225, loss = 0.11904399\n",
            "Iteration 226, loss = 0.12150707\n",
            "Iteration 227, loss = 0.12093047\n",
            "Iteration 228, loss = 0.11896831\n",
            "Iteration 229, loss = 0.11959909\n",
            "Iteration 230, loss = 0.12000327\n",
            "Iteration 231, loss = 0.11890876\n",
            "Iteration 232, loss = 0.11951474\n",
            "Iteration 233, loss = 0.11830379\n",
            "Iteration 234, loss = 0.11867002\n",
            "Iteration 235, loss = 0.12088799\n",
            "Iteration 236, loss = 0.12451589\n",
            "Iteration 237, loss = 0.11930717\n",
            "Iteration 238, loss = 0.12050293\n",
            "Iteration 239, loss = 0.12136904\n",
            "Iteration 240, loss = 0.11787167\n",
            "Iteration 241, loss = 0.11979233\n",
            "Iteration 242, loss = 0.11581546\n",
            "Iteration 243, loss = 0.11674762\n",
            "Iteration 244, loss = 0.11619119\n",
            "Iteration 245, loss = 0.11785244\n",
            "Iteration 246, loss = 0.11571403\n",
            "Iteration 247, loss = 0.11470270\n",
            "Iteration 248, loss = 0.11550540\n",
            "Iteration 249, loss = 0.11862961\n",
            "Iteration 250, loss = 0.11604008\n",
            "Iteration 251, loss = 0.11485925\n",
            "Iteration 252, loss = 0.11700885\n",
            "Iteration 253, loss = 0.11503063\n",
            "Iteration 254, loss = 0.11392731\n",
            "Iteration 255, loss = 0.11415677\n",
            "Iteration 256, loss = 0.11630320\n",
            "Iteration 257, loss = 0.11331484\n",
            "Iteration 258, loss = 0.11544048\n",
            "Iteration 259, loss = 0.11410126\n",
            "Iteration 260, loss = 0.11258303\n",
            "Iteration 261, loss = 0.11179900\n",
            "Iteration 262, loss = 0.11284938\n",
            "Iteration 263, loss = 0.11606125\n",
            "Iteration 264, loss = 0.11269736\n",
            "Iteration 265, loss = 0.11250094\n",
            "Iteration 266, loss = 0.11299186\n",
            "Iteration 267, loss = 0.11472778\n",
            "Iteration 268, loss = 0.11414108\n",
            "Iteration 269, loss = 0.11136901\n",
            "Iteration 270, loss = 0.11138143\n",
            "Iteration 271, loss = 0.11375843\n",
            "Iteration 272, loss = 0.11519453\n",
            "Iteration 273, loss = 0.11244220\n",
            "Iteration 274, loss = 0.11624403\n",
            "Iteration 275, loss = 0.11228391\n",
            "Iteration 276, loss = 0.11065721\n",
            "Iteration 277, loss = 0.11163061\n",
            "Iteration 278, loss = 0.11129107\n",
            "Iteration 279, loss = 0.10808378\n",
            "Iteration 280, loss = 0.11297446\n",
            "Iteration 281, loss = 0.11127208\n",
            "Iteration 282, loss = 0.10940527\n",
            "Iteration 283, loss = 0.10961977\n",
            "Iteration 284, loss = 0.11381814\n",
            "Iteration 285, loss = 0.11497613\n",
            "Iteration 286, loss = 0.11158521\n",
            "Iteration 287, loss = 0.11110386\n",
            "Iteration 288, loss = 0.11004427\n",
            "Iteration 289, loss = 0.11007223\n",
            "Iteration 290, loss = 0.11062683\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jBqkRsgBNfQ"
      },
      "source": [
        "#DataFrame\n",
        "headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "\n",
        "df = pd.read_csv('adult.test.csv', names=headers)\n",
        "\n",
        "#Tratamento de dados faltantes para o mais representatívo\n",
        "columns = df.columns\n",
        "for i in columns:\n",
        "    missing = df[i].isin([' ?']).sum()\n",
        "    df[i] = df[i].replace(' ?', np.NaN)\n",
        "df = df.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
        "\n",
        "#Divisão em Parâmetro e Classe\n",
        "X_df = df.iloc[:, 0:14].values\n",
        "y_df = df.iloc[:, 14].values\n",
        "\n",
        "#LabelEncoder\n",
        "def labelencoder(pd_serie):\n",
        "    labelencoder = LabelEncoder()\n",
        "    pd_serie = labelencoder.fit_transform(pd_serie)\n",
        "    return pd_serie\n",
        "\n",
        "X_df[:, 1] = labelencoder(X_df[:, 1])\n",
        "X_df[:, 3] = labelencoder(X_df[:, 3])\n",
        "X_df[:, 5] = labelencoder(X_df[:, 5])\n",
        "X_df[:, 6] = labelencoder(X_df[:, 6])\n",
        "X_df[:, 7] = labelencoder(X_df[:, 7])\n",
        "X_df[:, 8] = labelencoder(X_df[:, 8])\n",
        "X_df[:, 9] = labelencoder(X_df[:, 9])\n",
        "X_df[:, 13] = labelencoder(X_df[:, 13])\n",
        "y_df = labelencoder(y_df)\n",
        "\n",
        "#One Hot Encoder\n",
        "onehotencorder = ColumnTransformer(transformers=[(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder='passthrough')\n",
        "X_df = onehotencorder.fit_transform(X_df).toarray()\n",
        "\n",
        "#Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "X_df = scaler.fit_transform(X_df)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YAIgn-mBn91",
        "outputId": "e00f6a90-6853-4d93-9b75-25171f1d7239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#knn\n",
        "predict_knn_hold = knn_hold.predict(X_df)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-567ac7eec9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#knn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_knn_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_hold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    662\u001b[0m                 delayed_query(\n\u001b[1;32m    663\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             )\n\u001b[1;32m    666\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree.query\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: query data dimension must match training data dimension"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rce_fJxMB66b",
        "outputId": "aa529046-e41d-401e-a8a4-707fc493f74a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#random_forest\n",
        "predict_random_forest_hold = random_forest_hold.predict(X_df)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-74478a94b40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#knn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_random_forest_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest_hold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    389\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 105 and input n_features is 104 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKLVVHfuCAFT",
        "outputId": "0caf608f-c112-4605-8290-238065178778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "#MLP\n",
        "predict_mlp_hold = mlp_hold.predict(X_df)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-906752d8ebdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_mlp_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_hold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    969\u001b[0m         \"\"\"\n\u001b[1;32m    970\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    683\u001b[0m                                          layer_units[i + 1])))\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 104\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 105 is different from 104)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24RyJE46DdHd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}