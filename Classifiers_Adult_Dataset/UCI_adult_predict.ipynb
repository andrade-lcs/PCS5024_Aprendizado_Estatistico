{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python382jvsc74a57bd019dd5068fd4c406359a86f27249fc517a25f1ed5fa0eb2341423f3354c0b7639",
      "display_name": "Python 3.8.2 32-bit"
    },
    "colab": {
      "name": "UCI_adult_predict.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "thsFFBMxfMHz"
      },
      "source": [
        "#Bibliotécas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiAfKFXzfMH_"
      },
      "source": [
        "#DataFrame\n",
        "headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "\n",
        "df = pd.read_csv('adult.data.csv', names=headers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzLUMIkrfMIA"
      },
      "source": [
        "#Divisão em Parâmetro e Classe\n",
        "X_df = df.iloc[:, 0:14].values\n",
        "y_df = df.iloc[:, 14].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8__DG0IfMIB"
      },
      "source": [
        "#LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "X_df[:, 1] = labelencoder.fit_transform(X_df[:, 1])\n",
        "X_df[:, 3] = labelencoder.fit_transform(X_df[:, 3])\n",
        "X_df[:, 5] = labelencoder.fit_transform(X_df[:, 5])\n",
        "X_df[:, 6] = labelencoder.fit_transform(X_df[:, 6])\n",
        "X_df[:, 7] = labelencoder.fit_transform(X_df[:, 7])\n",
        "X_df[:, 8] = labelencoder.fit_transform(X_df[:, 8])\n",
        "X_df[:, 9] = labelencoder.fit_transform(X_df[:, 9])\n",
        "X_df[:, 13] = labelencoder.fit_transform(X_df[:, 13])\n",
        "y_df = labelencoder.fit_transform(y_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYQn9S19fMIC"
      },
      "source": [
        "#One Hot Encoder\n",
        "onehotencorder = ColumnTransformer(transformers=[(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder='passthrough')\n",
        "X_df = onehotencorder.fit_transform(X_df).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twXAB1iKfMIC"
      },
      "source": [
        "#Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "X_df = scaler.fit_transform(X_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "312HacmbfMID"
      },
      "source": [
        "#Divisão df de treinamento e teste 15%\n",
        "X_df_train, X_df_test, y_df_train, y_df_test =  train_test_split(X_df, y_df, test_size=0.15, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8E9ZX2KfMID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14cfd4ba-242c-47fa-f4a5-3f5b474794ff"
      },
      "source": [
        "print(X_df_train.shape, X_df_test.shape, y_df_train.shape, y_df_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27676, 108) (4885, 108) (27676,) (4885,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cp03L4BfMIF"
      },
      "source": [
        "#kNN\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSiD4blLfMIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3115e93-37c7-4d51-ef0b-06229f37af95"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_df_train, y_df_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sul2sYy8fMIG"
      },
      "source": [
        "predict_knn = knn.predict(X_df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vovqlq6egJt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344336de-ea98-4e17-db3a-2d43721326f2"
      },
      "source": [
        "accuracy_score(y_df_test, predict_knn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8290685772773797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiWW-LhtgqYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4159eab1-18e3-49e2-d539-bdd05815981c"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_knn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3437,  256],\n",
              "       [ 579,  613]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAWrWQMGg8S6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2df822-7cf6-4a8d-8335-30b16d0677a4"
      },
      "source": [
        "print(classification_report(y_df_test, predict_knn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89      3693\n",
            "           1       0.71      0.51      0.59      1192\n",
            "\n",
            "    accuracy                           0.83      4885\n",
            "   macro avg       0.78      0.72      0.74      4885\n",
            "weighted avg       0.82      0.83      0.82      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TPpC38rfMIH"
      },
      "source": [
        "#cross-validation\n",
        "from sklern.model_selection import cross_val_score\n",
        "result = cross_val_score(knn, X_df, y_df, cv = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYvd9aSofMIH"
      },
      "source": [
        "#random forest\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFY_Lg6mHBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68fdf13-7326-46db-abeb-8efeb6913208"
      },
      "source": [
        "random_forest = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)\n",
        "random_forest.fit(X_df_train, y_df_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfSNGu-7mlJ9"
      },
      "source": [
        "predict_rf = random_forest.predict(X_df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmHSc6sXtpFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ccc4b67-d9f4-481e-b7fa-dec8b36abf6b"
      },
      "source": [
        "accuracy_score(y_df_test, predict_rf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8507676560900717"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOz-SI0DtpqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b51ffe-2d29-41e7-e5dc-b4040bad39ff"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_rf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3420,  273],\n",
              "       [ 456,  736]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqmaUQ2ItsGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e48e5e8-81b8-4513-b946-875a32ec7f1b"
      },
      "source": [
        "print(classification_report(y_df_test, predict_rf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90      3693\n",
            "           1       0.73      0.62      0.67      1192\n",
            "\n",
            "    accuracy                           0.85      4885\n",
            "   macro avg       0.81      0.77      0.79      4885\n",
            "weighted avg       0.85      0.85      0.85      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk7yxJ3FtyNB"
      },
      "source": [
        "#MLP\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34mRORBC0up4"
      },
      "source": [
        "mlp_keras = Sequential()\n",
        "mlp_keras.add(Dense(units=55, activation='relu', input_dim=108))\n",
        "mlp_keras.add(Dense(units=55, activation='relu'))\n",
        "mlp_keras.add(Dense(units=1, activation='sigmoid'))\n",
        "mlp_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "redL5hNs0wqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c088c3e-00fd-43ab-adcc-5018c6fdee59"
      },
      "source": [
        "mlp_keras.fit(X_df_train, y_df_train, batch_size=10, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.3703 - accuracy: 0.8306\n",
            "Epoch 2/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.3084 - accuracy: 0.8573\n",
            "Epoch 3/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.3040 - accuracy: 0.8594\n",
            "Epoch 4/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2961 - accuracy: 0.8632\n",
            "Epoch 5/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2955 - accuracy: 0.8656\n",
            "Epoch 6/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2893 - accuracy: 0.8656\n",
            "Epoch 7/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2884 - accuracy: 0.8668\n",
            "Epoch 8/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2789 - accuracy: 0.8723\n",
            "Epoch 9/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2796 - accuracy: 0.8741\n",
            "Epoch 10/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2710 - accuracy: 0.8734\n",
            "Epoch 11/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2692 - accuracy: 0.8757\n",
            "Epoch 12/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2632 - accuracy: 0.8780\n",
            "Epoch 13/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2623 - accuracy: 0.8777\n",
            "Epoch 14/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2601 - accuracy: 0.8761\n",
            "Epoch 15/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2557 - accuracy: 0.8804\n",
            "Epoch 16/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2452 - accuracy: 0.8844\n",
            "Epoch 17/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2421 - accuracy: 0.8877\n",
            "Epoch 18/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2431 - accuracy: 0.8858\n",
            "Epoch 19/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2424 - accuracy: 0.8851\n",
            "Epoch 20/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2357 - accuracy: 0.8890\n",
            "Epoch 21/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2317 - accuracy: 0.8922\n",
            "Epoch 22/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2308 - accuracy: 0.8925\n",
            "Epoch 23/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2330 - accuracy: 0.8911\n",
            "Epoch 24/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2297 - accuracy: 0.8917\n",
            "Epoch 25/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2206 - accuracy: 0.8987\n",
            "Epoch 26/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2216 - accuracy: 0.8982\n",
            "Epoch 27/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2152 - accuracy: 0.9022\n",
            "Epoch 28/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2179 - accuracy: 0.8968\n",
            "Epoch 29/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2145 - accuracy: 0.9004\n",
            "Epoch 30/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2144 - accuracy: 0.8991\n",
            "Epoch 31/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2135 - accuracy: 0.9015\n",
            "Epoch 32/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2105 - accuracy: 0.9020\n",
            "Epoch 33/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2068 - accuracy: 0.9042\n",
            "Epoch 34/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2023 - accuracy: 0.9065\n",
            "Epoch 35/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2011 - accuracy: 0.9049\n",
            "Epoch 36/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.2046 - accuracy: 0.9034\n",
            "Epoch 37/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1963 - accuracy: 0.9105\n",
            "Epoch 38/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1987 - accuracy: 0.9045\n",
            "Epoch 39/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1978 - accuracy: 0.9081\n",
            "Epoch 40/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1983 - accuracy: 0.9097\n",
            "Epoch 41/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1879 - accuracy: 0.9158\n",
            "Epoch 42/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1926 - accuracy: 0.9107\n",
            "Epoch 43/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1942 - accuracy: 0.9098\n",
            "Epoch 44/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1880 - accuracy: 0.9147\n",
            "Epoch 45/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1896 - accuracy: 0.9138\n",
            "Epoch 46/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1870 - accuracy: 0.9111\n",
            "Epoch 47/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1829 - accuracy: 0.9150\n",
            "Epoch 48/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1816 - accuracy: 0.9166\n",
            "Epoch 49/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1800 - accuracy: 0.9169\n",
            "Epoch 50/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1777 - accuracy: 0.9177\n",
            "Epoch 51/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1833 - accuracy: 0.9141\n",
            "Epoch 52/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1772 - accuracy: 0.9161\n",
            "Epoch 53/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1792 - accuracy: 0.9170\n",
            "Epoch 54/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1743 - accuracy: 0.9189\n",
            "Epoch 55/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1756 - accuracy: 0.9189\n",
            "Epoch 56/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1713 - accuracy: 0.9210\n",
            "Epoch 57/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1752 - accuracy: 0.9174\n",
            "Epoch 58/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1742 - accuracy: 0.9180\n",
            "Epoch 59/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1688 - accuracy: 0.9197\n",
            "Epoch 60/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1697 - accuracy: 0.9210\n",
            "Epoch 61/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1743 - accuracy: 0.9181\n",
            "Epoch 62/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1702 - accuracy: 0.9200\n",
            "Epoch 63/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1701 - accuracy: 0.9190\n",
            "Epoch 64/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1667 - accuracy: 0.9212\n",
            "Epoch 65/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1649 - accuracy: 0.9242\n",
            "Epoch 66/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1653 - accuracy: 0.9239\n",
            "Epoch 67/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1680 - accuracy: 0.9220\n",
            "Epoch 68/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1670 - accuracy: 0.9235\n",
            "Epoch 69/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1659 - accuracy: 0.9226\n",
            "Epoch 70/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1631 - accuracy: 0.9230\n",
            "Epoch 71/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1641 - accuracy: 0.9219\n",
            "Epoch 72/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1642 - accuracy: 0.9248\n",
            "Epoch 73/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1635 - accuracy: 0.9233\n",
            "Epoch 74/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1640 - accuracy: 0.9234\n",
            "Epoch 75/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1599 - accuracy: 0.9262\n",
            "Epoch 76/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1575 - accuracy: 0.9265\n",
            "Epoch 77/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1641 - accuracy: 0.9237\n",
            "Epoch 78/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1636 - accuracy: 0.9238\n",
            "Epoch 79/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1569 - accuracy: 0.9289\n",
            "Epoch 80/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1594 - accuracy: 0.9250\n",
            "Epoch 81/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1594 - accuracy: 0.9258\n",
            "Epoch 82/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1527 - accuracy: 0.9297\n",
            "Epoch 83/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1621 - accuracy: 0.9246\n",
            "Epoch 84/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1627 - accuracy: 0.9245\n",
            "Epoch 85/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1578 - accuracy: 0.9264\n",
            "Epoch 86/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1561 - accuracy: 0.9277\n",
            "Epoch 87/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1532 - accuracy: 0.9286\n",
            "Epoch 88/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1565 - accuracy: 0.9260\n",
            "Epoch 89/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1560 - accuracy: 0.9265\n",
            "Epoch 90/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1513 - accuracy: 0.9294\n",
            "Epoch 91/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1559 - accuracy: 0.9278\n",
            "Epoch 92/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1492 - accuracy: 0.9309\n",
            "Epoch 93/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1533 - accuracy: 0.9269\n",
            "Epoch 94/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1583 - accuracy: 0.9265\n",
            "Epoch 95/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1461 - accuracy: 0.9328\n",
            "Epoch 96/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1518 - accuracy: 0.9300\n",
            "Epoch 97/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1521 - accuracy: 0.9305\n",
            "Epoch 98/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1489 - accuracy: 0.9307\n",
            "Epoch 99/100\n",
            "2768/2768 [==============================] - 6s 2ms/step - loss: 0.1518 - accuracy: 0.9285\n",
            "Epoch 100/100\n",
            "2768/2768 [==============================] - 5s 2ms/step - loss: 0.1507 - accuracy: 0.9295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f88f26a58d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSSLWtIg04HP"
      },
      "source": [
        "predict_mlp_keras = mlp_keras.predict(X_df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_80233Kd46qr"
      },
      "source": [
        "predict_mlp_keras = (predict_mlp_keras > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RsY_UwO4mVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486d3e51-4398-41ef-92b7-14d36b699b58"
      },
      "source": [
        "accuracy_score(y_df_test, predict_mlp_keras)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8184237461617195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQhFZaj24wXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9913724-cdb4-4471-cd2d-3161cde65576"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_mlp_keras)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3253,  440],\n",
              "       [ 447,  745]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5lzhM7b5Hth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d1b2ae-01fe-4580-a2c8-10c9556fdb54"
      },
      "source": [
        "print(classification_report(y_df_test, predict_mlp_keras ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88      3693\n",
            "           1       0.63      0.62      0.63      1192\n",
            "\n",
            "    accuracy                           0.82      4885\n",
            "   macro avg       0.75      0.75      0.75      4885\n",
            "weighted avg       0.82      0.82      0.82      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxd-waKZ5UFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d12d992-9a4d-4869-926e-9a4a229b5597"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(verbose=True, max_iter=1000, tol=0.000010, hidden_layer_sizes=(100, 100))\n",
        "mlp.fit(X_df_train, y_df_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.37552725\n",
            "Iteration 2, loss = 0.32181706\n",
            "Iteration 3, loss = 0.30995593\n",
            "Iteration 4, loss = 0.30173559\n",
            "Iteration 5, loss = 0.29670519\n",
            "Iteration 6, loss = 0.29233434\n",
            "Iteration 7, loss = 0.28840006\n",
            "Iteration 8, loss = 0.28529671\n",
            "Iteration 9, loss = 0.28273772\n",
            "Iteration 10, loss = 0.27971318\n",
            "Iteration 11, loss = 0.27620812\n",
            "Iteration 12, loss = 0.27424059\n",
            "Iteration 13, loss = 0.27142015\n",
            "Iteration 14, loss = 0.26943867\n",
            "Iteration 15, loss = 0.26610260\n",
            "Iteration 16, loss = 0.26464183\n",
            "Iteration 17, loss = 0.26078358\n",
            "Iteration 18, loss = 0.25974695\n",
            "Iteration 19, loss = 0.25698770\n",
            "Iteration 20, loss = 0.25382001\n",
            "Iteration 21, loss = 0.25205727\n",
            "Iteration 22, loss = 0.24866301\n",
            "Iteration 23, loss = 0.24665569\n",
            "Iteration 24, loss = 0.24369740\n",
            "Iteration 25, loss = 0.24259023\n",
            "Iteration 26, loss = 0.24023257\n",
            "Iteration 27, loss = 0.23674159\n",
            "Iteration 28, loss = 0.23614613\n",
            "Iteration 29, loss = 0.23387445\n",
            "Iteration 30, loss = 0.23239805\n",
            "Iteration 31, loss = 0.22969525\n",
            "Iteration 32, loss = 0.22904512\n",
            "Iteration 33, loss = 0.22509556\n",
            "Iteration 34, loss = 0.22389233\n",
            "Iteration 35, loss = 0.22069014\n",
            "Iteration 36, loss = 0.21879029\n",
            "Iteration 37, loss = 0.21661643\n",
            "Iteration 38, loss = 0.21563409\n",
            "Iteration 39, loss = 0.21474591\n",
            "Iteration 40, loss = 0.21220132\n",
            "Iteration 41, loss = 0.21172864\n",
            "Iteration 42, loss = 0.20843110\n",
            "Iteration 43, loss = 0.20661254\n",
            "Iteration 44, loss = 0.20603625\n",
            "Iteration 45, loss = 0.20512968\n",
            "Iteration 46, loss = 0.20399261\n",
            "Iteration 47, loss = 0.20161314\n",
            "Iteration 48, loss = 0.20291912\n",
            "Iteration 49, loss = 0.19862361\n",
            "Iteration 50, loss = 0.19764229\n",
            "Iteration 51, loss = 0.19684071\n",
            "Iteration 52, loss = 0.19498661\n",
            "Iteration 53, loss = 0.19289125\n",
            "Iteration 54, loss = 0.19243208\n",
            "Iteration 55, loss = 0.18987759\n",
            "Iteration 56, loss = 0.18825582\n",
            "Iteration 57, loss = 0.18854779\n",
            "Iteration 58, loss = 0.18832034\n",
            "Iteration 59, loss = 0.18690276\n",
            "Iteration 60, loss = 0.18382332\n",
            "Iteration 61, loss = 0.18264501\n",
            "Iteration 62, loss = 0.18247022\n",
            "Iteration 63, loss = 0.18377451\n",
            "Iteration 64, loss = 0.18021858\n",
            "Iteration 65, loss = 0.17859944\n",
            "Iteration 66, loss = 0.17806351\n",
            "Iteration 67, loss = 0.17740379\n",
            "Iteration 68, loss = 0.17522263\n",
            "Iteration 69, loss = 0.17541413\n",
            "Iteration 70, loss = 0.17350049\n",
            "Iteration 71, loss = 0.17264774\n",
            "Iteration 72, loss = 0.17117628\n",
            "Iteration 73, loss = 0.17306605\n",
            "Iteration 74, loss = 0.17037472\n",
            "Iteration 75, loss = 0.16806255\n",
            "Iteration 76, loss = 0.16749781\n",
            "Iteration 77, loss = 0.16725999\n",
            "Iteration 78, loss = 0.16573161\n",
            "Iteration 79, loss = 0.16561322\n",
            "Iteration 80, loss = 0.16615520\n",
            "Iteration 81, loss = 0.16582909\n",
            "Iteration 82, loss = 0.16308236\n",
            "Iteration 83, loss = 0.16194693\n",
            "Iteration 84, loss = 0.16169995\n",
            "Iteration 85, loss = 0.16030203\n",
            "Iteration 86, loss = 0.16021913\n",
            "Iteration 87, loss = 0.16217880\n",
            "Iteration 88, loss = 0.16001522\n",
            "Iteration 89, loss = 0.16056805\n",
            "Iteration 90, loss = 0.15656098\n",
            "Iteration 91, loss = 0.15759649\n",
            "Iteration 92, loss = 0.15501831\n",
            "Iteration 93, loss = 0.15482884\n",
            "Iteration 94, loss = 0.15312087\n",
            "Iteration 95, loss = 0.15473720\n",
            "Iteration 96, loss = 0.15541822\n",
            "Iteration 97, loss = 0.15183931\n",
            "Iteration 98, loss = 0.15311959\n",
            "Iteration 99, loss = 0.15583110\n",
            "Iteration 100, loss = 0.15259323\n",
            "Iteration 101, loss = 0.15043457\n",
            "Iteration 102, loss = 0.14943588\n",
            "Iteration 103, loss = 0.14835501\n",
            "Iteration 104, loss = 0.14763470\n",
            "Iteration 105, loss = 0.14863240\n",
            "Iteration 106, loss = 0.14791712\n",
            "Iteration 107, loss = 0.14655776\n",
            "Iteration 108, loss = 0.14796338\n",
            "Iteration 109, loss = 0.14439712\n",
            "Iteration 110, loss = 0.14328502\n",
            "Iteration 111, loss = 0.14502104\n",
            "Iteration 112, loss = 0.14355514\n",
            "Iteration 113, loss = 0.14670490\n",
            "Iteration 114, loss = 0.14686407\n",
            "Iteration 115, loss = 0.14268269\n",
            "Iteration 116, loss = 0.14158128\n",
            "Iteration 117, loss = 0.14103980\n",
            "Iteration 118, loss = 0.14186271\n",
            "Iteration 119, loss = 0.13957475\n",
            "Iteration 120, loss = 0.14152664\n",
            "Iteration 121, loss = 0.13919691\n",
            "Iteration 122, loss = 0.13844444\n",
            "Iteration 123, loss = 0.13658510\n",
            "Iteration 124, loss = 0.13698295\n",
            "Iteration 125, loss = 0.13720391\n",
            "Iteration 126, loss = 0.13706541\n",
            "Iteration 127, loss = 0.13578536\n",
            "Iteration 128, loss = 0.13705302\n",
            "Iteration 129, loss = 0.13467324\n",
            "Iteration 130, loss = 0.13572608\n",
            "Iteration 131, loss = 0.13561772\n",
            "Iteration 132, loss = 0.13422657\n",
            "Iteration 133, loss = 0.13215473\n",
            "Iteration 134, loss = 0.13452866\n",
            "Iteration 135, loss = 0.13347551\n",
            "Iteration 136, loss = 0.13305631\n",
            "Iteration 137, loss = 0.13179043\n",
            "Iteration 138, loss = 0.13127308\n",
            "Iteration 139, loss = 0.13188406\n",
            "Iteration 140, loss = 0.13024110\n",
            "Iteration 141, loss = 0.13093915\n",
            "Iteration 142, loss = 0.13007830\n",
            "Iteration 143, loss = 0.12972057\n",
            "Iteration 144, loss = 0.12762488\n",
            "Iteration 145, loss = 0.12958817\n",
            "Iteration 146, loss = 0.13038089\n",
            "Iteration 147, loss = 0.12890967\n",
            "Iteration 148, loss = 0.12633432\n",
            "Iteration 149, loss = 0.12874851\n",
            "Iteration 150, loss = 0.12819419\n",
            "Iteration 151, loss = 0.12652906\n",
            "Iteration 152, loss = 0.12715413\n",
            "Iteration 153, loss = 0.12831335\n",
            "Iteration 154, loss = 0.12459682\n",
            "Iteration 155, loss = 0.12545845\n",
            "Iteration 156, loss = 0.12750011\n",
            "Iteration 157, loss = 0.13117304\n",
            "Iteration 158, loss = 0.12892101\n",
            "Iteration 159, loss = 0.12353960\n",
            "Iteration 160, loss = 0.12411334\n",
            "Iteration 161, loss = 0.12243453\n",
            "Iteration 162, loss = 0.12177311\n",
            "Iteration 163, loss = 0.12221000\n",
            "Iteration 164, loss = 0.12260882\n",
            "Iteration 165, loss = 0.12507377\n",
            "Iteration 166, loss = 0.12198257\n",
            "Iteration 167, loss = 0.12214120\n",
            "Iteration 168, loss = 0.12161819\n",
            "Iteration 169, loss = 0.12115144\n",
            "Iteration 170, loss = 0.12053899\n",
            "Iteration 171, loss = 0.12098252\n",
            "Iteration 172, loss = 0.11993524\n",
            "Iteration 173, loss = 0.12377902\n",
            "Iteration 174, loss = 0.11839720\n",
            "Iteration 175, loss = 0.11892630\n",
            "Iteration 176, loss = 0.12077339\n",
            "Iteration 177, loss = 0.11749327\n",
            "Iteration 178, loss = 0.11667746\n",
            "Iteration 179, loss = 0.11813025\n",
            "Iteration 180, loss = 0.12049971\n",
            "Iteration 181, loss = 0.11940366\n",
            "Iteration 182, loss = 0.11740577\n",
            "Iteration 183, loss = 0.11829515\n",
            "Iteration 184, loss = 0.11708718\n",
            "Iteration 185, loss = 0.11804280\n",
            "Iteration 186, loss = 0.11689054\n",
            "Iteration 187, loss = 0.11690455\n",
            "Iteration 188, loss = 0.11671901\n",
            "Iteration 189, loss = 0.11406449\n",
            "Iteration 190, loss = 0.11453486\n",
            "Iteration 191, loss = 0.11625903\n",
            "Iteration 192, loss = 0.11450474\n",
            "Iteration 193, loss = 0.11513105\n",
            "Iteration 194, loss = 0.11430928\n",
            "Iteration 195, loss = 0.11398295\n",
            "Iteration 196, loss = 0.11216901\n",
            "Iteration 197, loss = 0.11507175\n",
            "Iteration 198, loss = 0.11852531\n",
            "Iteration 199, loss = 0.11520758\n",
            "Iteration 200, loss = 0.11422208\n",
            "Iteration 201, loss = 0.11381765\n",
            "Iteration 202, loss = 0.11452100\n",
            "Iteration 203, loss = 0.11448345\n",
            "Iteration 204, loss = 0.11321403\n",
            "Iteration 205, loss = 0.11003415\n",
            "Iteration 206, loss = 0.11014612\n",
            "Iteration 207, loss = 0.11016033\n",
            "Iteration 208, loss = 0.11038892\n",
            "Iteration 209, loss = 0.11091553\n",
            "Iteration 210, loss = 0.11078950\n",
            "Iteration 211, loss = 0.10905608\n",
            "Iteration 212, loss = 0.11013247\n",
            "Iteration 213, loss = 0.10934803\n",
            "Iteration 214, loss = 0.11021488\n",
            "Iteration 215, loss = 0.10979811\n",
            "Iteration 216, loss = 0.11026788\n",
            "Iteration 217, loss = 0.10903146\n",
            "Iteration 218, loss = 0.10853460\n",
            "Iteration 219, loss = 0.11051839\n",
            "Iteration 220, loss = 0.10794950\n",
            "Iteration 221, loss = 0.11077247\n",
            "Iteration 222, loss = 0.11042135\n",
            "Iteration 223, loss = 0.10993633\n",
            "Iteration 224, loss = 0.11093528\n",
            "Iteration 225, loss = 0.10995567\n",
            "Iteration 226, loss = 0.10937955\n",
            "Iteration 227, loss = 0.10732959\n",
            "Iteration 228, loss = 0.10448569\n",
            "Iteration 229, loss = 0.10642992\n",
            "Iteration 230, loss = 0.10801850\n",
            "Iteration 231, loss = 0.10609342\n",
            "Iteration 232, loss = 0.10530092\n",
            "Iteration 233, loss = 0.10663497\n",
            "Iteration 234, loss = 0.10574602\n",
            "Iteration 235, loss = 0.10784593\n",
            "Iteration 236, loss = 0.10505917\n",
            "Iteration 237, loss = 0.10428653\n",
            "Iteration 238, loss = 0.10418175\n",
            "Iteration 239, loss = 0.10401647\n",
            "Iteration 240, loss = 0.10449826\n",
            "Iteration 241, loss = 0.10297329\n",
            "Iteration 242, loss = 0.10339163\n",
            "Iteration 243, loss = 0.10338324\n",
            "Iteration 244, loss = 0.10695718\n",
            "Iteration 245, loss = 0.10164961\n",
            "Iteration 246, loss = 0.10124968\n",
            "Iteration 247, loss = 0.10595664\n",
            "Iteration 248, loss = 0.10425242\n",
            "Iteration 249, loss = 0.10286924\n",
            "Iteration 250, loss = 0.10145051\n",
            "Iteration 251, loss = 0.10286154\n",
            "Iteration 252, loss = 0.10552243\n",
            "Iteration 253, loss = 0.10392037\n",
            "Iteration 254, loss = 0.10279993\n",
            "Iteration 255, loss = 0.10109744\n",
            "Iteration 256, loss = 0.10182952\n",
            "Iteration 257, loss = 0.10336473\n",
            "Iteration 258, loss = 0.10088143\n",
            "Iteration 259, loss = 0.10197261\n",
            "Iteration 260, loss = 0.10184948\n",
            "Iteration 261, loss = 0.10126717\n",
            "Iteration 262, loss = 0.10040844\n",
            "Iteration 263, loss = 0.10044171\n",
            "Iteration 264, loss = 0.10297200\n",
            "Iteration 265, loss = 0.10874490\n",
            "Iteration 266, loss = 0.09916863\n",
            "Iteration 267, loss = 0.09885697\n",
            "Iteration 268, loss = 0.09932694\n",
            "Iteration 269, loss = 0.10107206\n",
            "Iteration 270, loss = 0.10417055\n",
            "Iteration 271, loss = 0.09939895\n",
            "Iteration 272, loss = 0.10052375\n",
            "Iteration 273, loss = 0.09897341\n",
            "Iteration 274, loss = 0.09989046\n",
            "Iteration 275, loss = 0.09844222\n",
            "Iteration 276, loss = 0.09765784\n",
            "Iteration 277, loss = 0.10045836\n",
            "Iteration 278, loss = 0.10031722\n",
            "Iteration 279, loss = 0.10037484\n",
            "Iteration 280, loss = 0.09971599\n",
            "Iteration 281, loss = 0.09734790\n",
            "Iteration 282, loss = 0.09824228\n",
            "Iteration 283, loss = 0.09597917\n",
            "Iteration 284, loss = 0.09588290\n",
            "Iteration 285, loss = 0.09666784\n",
            "Iteration 286, loss = 0.09663493\n",
            "Iteration 287, loss = 0.09936267\n",
            "Iteration 288, loss = 0.09931469\n",
            "Iteration 289, loss = 0.09934720\n",
            "Iteration 290, loss = 0.09948593\n",
            "Iteration 291, loss = 0.09656565\n",
            "Iteration 292, loss = 0.09558684\n",
            "Iteration 293, loss = 0.09872764\n",
            "Iteration 294, loss = 0.09877632\n",
            "Iteration 295, loss = 0.09745756\n",
            "Iteration 296, loss = 0.09490645\n",
            "Iteration 297, loss = 0.09721288\n",
            "Iteration 298, loss = 0.09441047\n",
            "Iteration 299, loss = 0.09498531\n",
            "Iteration 300, loss = 0.09796099\n",
            "Iteration 301, loss = 0.09519381\n",
            "Iteration 302, loss = 0.09344542\n",
            "Iteration 303, loss = 0.09465690\n",
            "Iteration 304, loss = 0.09527937\n",
            "Iteration 305, loss = 0.09330370\n",
            "Iteration 306, loss = 0.09669080\n",
            "Iteration 307, loss = 0.09348979\n",
            "Iteration 308, loss = 0.09590892\n",
            "Iteration 309, loss = 0.09193673\n",
            "Iteration 310, loss = 0.09348763\n",
            "Iteration 311, loss = 0.09339650\n",
            "Iteration 312, loss = 0.09427082\n",
            "Iteration 313, loss = 0.09574129\n",
            "Iteration 314, loss = 0.09613909\n",
            "Iteration 315, loss = 0.09190580\n",
            "Iteration 316, loss = 0.09364176\n",
            "Iteration 317, loss = 0.09470028\n",
            "Iteration 318, loss = 0.09480548\n",
            "Iteration 319, loss = 0.09400617\n",
            "Iteration 320, loss = 0.09316464\n",
            "Iteration 321, loss = 0.09218906\n",
            "Iteration 322, loss = 0.09048721\n",
            "Iteration 323, loss = 0.09121598\n",
            "Iteration 324, loss = 0.09355416\n",
            "Iteration 325, loss = 0.09455597\n",
            "Iteration 326, loss = 0.09107955\n",
            "Iteration 327, loss = 0.09074823\n",
            "Iteration 328, loss = 0.09388917\n",
            "Iteration 329, loss = 0.09510028\n",
            "Iteration 330, loss = 0.09260396\n",
            "Iteration 331, loss = 0.09097020\n",
            "Iteration 332, loss = 0.09546999\n",
            "Iteration 333, loss = 0.09346715\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECLqYWNY5w3s"
      },
      "source": [
        "predict_mlp = mlp.predict(X_df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kpdAtSk6doS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df66194a-79d4-4889-ccf2-73cd33ebf99c"
      },
      "source": [
        "accuracy_score(y_df_test, predict_mlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8073694984646879"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAWSlA476mF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1192b60a-eaaa-474d-d02d-9eab8e899808"
      },
      "source": [
        "confusion_matrix(y_df_test, predict_mlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3245,  448],\n",
              "       [ 493,  699]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG4t-kps6q4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31bcbba-6272-4c12-f471-58f8bfe9ce45"
      },
      "source": [
        "print(classification_report(y_df_test, predict_mlp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87      3693\n",
            "           1       0.61      0.59      0.60      1192\n",
            "\n",
            "    accuracy                           0.81      4885\n",
            "   macro avg       0.74      0.73      0.74      4885\n",
            "weighted avg       0.80      0.81      0.81      4885\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85RO4hLp6tTq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}